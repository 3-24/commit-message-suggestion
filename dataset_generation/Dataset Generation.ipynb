{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "dataset_gen",
      "language": "python",
      "name": "dataset_gen"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "name": "Dataset Generation.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a5ade31"
      },
      "source": [
        "## 1. Collect CodeSearchNet Repositories"
      ],
      "id": "8a5ade31"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06bb109"
      },
      "source": [
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "pd.set_option('max_colwidth',300)\n",
        "from pprint import pprint"
      ],
      "id": "c06bb109",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c50cf9b",
        "outputId": "d9378109-40df-450d-dbcf-aac869b53438"
      },
      "source": [
        "!wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
        "!mkdir CodeSearchNet\n",
        "!unzip python.zip -d CodeSearchNet"
      ],
      "id": "4c50cf9b",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-30 15:58:09--  https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.142.16\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.142.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 940909997 (897M) [application/zip]\n",
            "Saving to: ‘python.zip’\n",
            "\n",
            "python.zip          100%[===================>] 897.32M  81.5MB/s    in 11s     \n",
            "\n",
            "2021-11-30 15:58:21 (79.4 MB/s) - ‘python.zip’ saved [940909997/940909997]\n",
            "\n",
            "Archive:  python.zip\n",
            "   creating: CodeSearchNet/python/\n",
            "   creating: CodeSearchNet/python/final/\n",
            "   creating: CodeSearchNet/python/final/jsonl/\n",
            "   creating: CodeSearchNet/python/final/jsonl/train/\n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_9.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_12.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_10.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_0.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_6.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_2.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_4.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_8.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_11.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_5.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_13.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_3.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_1.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python/final/jsonl/train/python_train_7.jsonl.gz  \n",
            "   creating: CodeSearchNet/python/final/jsonl/test/\n",
            "  inflating: CodeSearchNet/python/final/jsonl/test/python_test_0.jsonl.gz  \n",
            "   creating: CodeSearchNet/python/final/jsonl/valid/\n",
            "  inflating: CodeSearchNet/python/final/jsonl/valid/python_valid_0.jsonl.gz  \n",
            "  inflating: CodeSearchNet/python_dedupe_definitions_v2.pkl  \n",
            "  inflating: CodeSearchNet/python_licenses.pkl  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2847085c"
      },
      "source": [
        "python_files = sorted(Path('CodeSearchNet/python').glob('**/*.gz'))"
      ],
      "id": "2847085c",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e15382",
        "outputId": "77a0f648-3755-4feb-ac23-67bccc2006a5"
      },
      "source": [
        "print(python_files)"
      ],
      "id": "a2e15382",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PosixPath('CodeSearchNet/python/final/jsonl/test/python_test_0.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_0.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_1.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_10.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_11.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_12.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_13.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_2.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_3.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_4.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_5.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_6.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_7.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_8.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_9.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/valid/python_valid_0.jsonl.gz')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aa7dbe1"
      },
      "source": [
        "columns_long_list = ['repo', 'path', 'url', 'code', \n",
        "                     'code_tokens', 'docstring', 'docstring_tokens', \n",
        "                     'language', 'partition']\n",
        "\n",
        "def jsonl_list_to_dataframe(file_list, columns=columns_long_list):\n",
        "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
        "    return pd.concat([pd.read_json(f, \n",
        "                                   orient='records', \n",
        "                                   compression='gzip',\n",
        "                                   lines=True)[columns] \n",
        "                      for f in file_list], sort=False)"
      ],
      "id": "8aa7dbe1",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be6c1c77"
      },
      "source": [
        "columns_repo = ['repo']\n",
        "\n",
        "pydf = jsonl_list_to_dataframe(python_files, columns=columns_repo)"
      ],
      "id": "be6c1c77",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e19c0d36"
      },
      "source": [
        "pydf = pydf.drop_duplicates().reset_index(drop=True)"
      ],
      "id": "e19c0d36",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "ea71c7ad",
        "outputId": "61b01d1a-61e3-4dcb-ef11-a70d3989d8d9"
      },
      "source": [
        "print(pydf.shape)\n",
        "pydf.head(13590)"
      ],
      "id": "ea71c7ad",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13590, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>soimort/you-get</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apache/airflow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pytorch/vision</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asciimoo/searx</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tensorflow/probability</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13585</th>\n",
              "      <td>praekelt/python-ambient</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13586</th>\n",
              "      <td>zenreach/py-era</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13587</th>\n",
              "      <td>TakesxiSximada/custom_settings</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13588</th>\n",
              "      <td>openpermissions/bass</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13589</th>\n",
              "      <td>xnuinside/clifier</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13590 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 repo\n",
              "0                     soimort/you-get\n",
              "1                      apache/airflow\n",
              "2                      pytorch/vision\n",
              "3                      asciimoo/searx\n",
              "4              tensorflow/probability\n",
              "...                               ...\n",
              "13585         praekelt/python-ambient\n",
              "13586                 zenreach/py-era\n",
              "13587  TakesxiSximada/custom_settings\n",
              "13588            openpermissions/bass\n",
              "13589               xnuinside/clifier\n",
              "\n",
              "[13590 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b07e592d"
      },
      "source": [
        "pydf.to_pickle(\"repos.pkl\")"
      ],
      "id": "b07e592d",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d0559c4"
      },
      "source": [
        "## 2. Collect diff and commits"
      ],
      "id": "1d0559c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ee24339",
        "outputId": "b1d5d2db-c160-423b-f1aa-9a795415c03d"
      },
      "source": [
        "!mkdir repos\n",
        "!pip install pydriller\n",
        "!pip install pandas\n",
        "!pip install spacy"
      ],
      "id": "8ee24339",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydriller\n",
            "  Downloading PyDriller-2.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting gitpython\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from pydriller) (2018.9)\n",
            "Collecting lizard\n",
            "  Downloading lizard-1.17.9-py2.py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->pydriller) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, lizard, gitpython, pydriller\n",
            "Successfully installed gitdb-4.0.9 gitpython-3.1.24 lizard-1.17.9 pydriller-2.0 smmap-5.0.0\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72e76292"
      },
      "source": [
        "from pydriller import *\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "from functools import reduce\n",
        "# spacy.cli.download(\"en_core_web_sm\")"
      ],
      "id": "72e76292",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "845d5eb7",
        "outputId": "601db1e1-5628-4362-a67a-c925fd7e61fc"
      },
      "source": [
        "repodf = pd.read_pickle(\"repos.pkl\")\n",
        "print(repodf.shape)\n",
        "spacy_tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "diff_tokenizer = nltk.tokenize.WordPunctTokenizer()"
      ],
      "id": "845d5eb7",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13590, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "449680fc"
      },
      "source": [
        "import re\n",
        "def basic_filter(message):\n",
        "    return message.split(\"\\n\", 1)[0].strip()\n",
        "\n",
        "# Remove [label] in front of commit if exists\n",
        "def label_filter(message):\n",
        "    if (message.startswith('[')):\n",
        "        end_bracket_index = message.find(']')\n",
        "        if (end_bracket_index == -1):\n",
        "            return None\n",
        "        return message[:end_bracket_index+1]\n",
        "    return message\n",
        "\n",
        "def camel_case_split(str):\n",
        "    return re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', str)\n",
        "\n",
        "\n",
        "def case_splitter(token):\n",
        "    return list(map(lambda x: x.lower(), camel_case_split(token[0].upper() + token[1:])))\n",
        "    \n",
        "\n",
        "def parse_repo_commits(repo_name, commit_limit=50):\n",
        "    data = []\n",
        "    commit_count = 0\n",
        "    for commit in Repository(\n",
        "        f\"https://github.com/{repo_name}\",\n",
        "        only_modifications_with_file_types=[\".py\"],\n",
        "        only_no_merge=True,\n",
        "        order='reverse'\n",
        "    ).traverse_commits():\n",
        "        if (commit_count >= commit_limit): break\n",
        "        line = basic_filter(commit.msg)\n",
        "        line = label_filter(line)\n",
        "        \n",
        "        if (line is None):\n",
        "            print(f\"[DEBUG] Label filter return None for repo {repo} and hash {commit.hash}\")\n",
        "        \n",
        "        line = line.replace('_', ' ')\n",
        "        \n",
        "        # Only alphabet and blank characters\n",
        "        if (not line.isascii() or not all([c.isalpha() or c.isspace() for c in line])):\n",
        "            continue\n",
        "        \n",
        "        '''\n",
        "        if (not line.isascii()):                     # Ignore non-English\n",
        "            continue\n",
        "        \n",
        "        if ('@' in line and not 'decorat' in line):  # Ignore Github mentions\n",
        "            continue\n",
        "        \n",
        "        if ('#' in line):                            # Ignore Github issue\n",
        "            continue\n",
        "        '''\n",
        "        \n",
        "        tokens = spacy_tokenizer(line)\n",
        "        \n",
        "        # VERB filter\n",
        "        if (tokens[0].pos_ != 'VERB'):\n",
        "            continue\n",
        "        \n",
        "        tokens = reduce(lambda a,b: a+b, map(case_splitter, [token.text for token in tokens]), [])\n",
        "        \n",
        "        if (len(tokens) < 3 or len(tokens) > 30):\n",
        "            continue\n",
        "        \n",
        "        # Check if changed files are python\n",
        "        file_failed = False\n",
        "        \n",
        "        for mf in commit.modified_files:\n",
        "            if (not mf.filename.endswith(\".py\")):\n",
        "                file_failed = True\n",
        "                break\n",
        "        \n",
        "        if (file_failed):\n",
        "            continue\n",
        "        \n",
        "        def diff_processing(mf):\n",
        "            print(mf.diff)\n",
        "            diff = '\\n'.join(map(lambda x: x[1], filter(lambda y: y[0] % 2 == 0, enumerate(mf.diff.split(\"@@\")))))\n",
        "            diff = diff.replace('\\n+', '\\n<add>').replace('\\n-', '\\n<del>')\n",
        "            #TODO\n",
        "            #replace_symbol_in_string = ex)url\n",
        "            replace_number = re.compile(r\"\"\"\n",
        "            (?P<prefix>[^a-zA-Z_])  #prefix is not alphabet\n",
        "            (?P<number>\n",
        "              0x[0-9A-Fa-f]+        #hexadecimal number\n",
        "              |[0-9]+               #decimal number\n",
        "            )\n",
        "            \"\"\",re.VERBOSE)\n",
        "            diff_number_filtered = replace_number.sub(\"\\g<prefix><number>\",diff)\n",
        "            print(diff_number_filtered)\n",
        "            token_regex = r\"\"\"(?x)\n",
        "             <(?:add|del|number)>   #Filtered eariler\n",
        "            |[-+*/^&~|=%!]=?        #Symbols which can join with equal\n",
        "            |[<>]{1,2}              #neq and bit shift symbols\n",
        "            |#+                     #Comment symbol\n",
        "            |[@?$]                  #Other symbols\n",
        "            |[a-zA-Z0-9]+           #General text\n",
        "            \"\"\"\n",
        "            test = r\"\"\"<(?:add|del|number)>|[-+*/^&~|=%!]=?|[<>]{1,2}|#+|[@?$]|[a-zA-Z0-9]+\"\"\"\n",
        "            #'\"`\\,.;:()[]{}_ not included\n",
        "            token = nltk.tokenize.regexp_tokenize(diff_number_filtered,test)\n",
        "            print(token)\n",
        "            input()\n",
        "            return diff\n",
        "        \n",
        "        print(tokens)\n",
        "        diff = ''.join(map(diff_processing, commit.modified_files))\n",
        "        \n",
        "        data.append([repo_name, commit.hash, ' '.join(tokens), diff])\n",
        "        commit_count += 1\n",
        "    \n",
        "    return pd.DataFrame(data, columns=[\"repo\", \"hash\", \"commit_messsage\", \"diff\"])"
      ],
      "id": "449680fc",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2854419",
        "outputId": "62ea3d78-8c51-465d-90b5-9d3238183ceb"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "df = parse_repo_commits(\"soimort/you-get\")\n",
        "end = time.time()\n",
        "print(end - start)\n",
        "df.head(3)"
      ],
      "id": "b2854419",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['add', 'hdr', 'support', 'for', 'bilibili']\n",
            "@@ -12,6 +12,8 @@ class Bilibili(VideoExtractor):\n",
            " \n",
            "     # Bilibili media encoding options, in descending quality order.\n",
            "     stream_types = [\n",
            "+        {'id': 'hdflv2', 'quality': 125, 'audio_quality': 30280,\n",
            "+         'container': 'FLV', 'video_resolution': '3840p', 'desc': '真彩 HDR'},\n",
            "         {'id': 'hdflv2_4k', 'quality': 120, 'audio_quality': 30280,\n",
            "          'container': 'FLV', 'video_resolution': '2160p', 'desc': '超清 4K'},\n",
            "         {'id': 'flv_p60', 'quality': 116, 'audio_quality': 30280,\n",
            "\n",
            "\n",
            " class Bilibili(VideoExtractor):\n",
            " \n",
            "     # Bilibili media encoding options, in descending quality order.\n",
            "     stream_types = [\n",
            "<add>        {'id': 'hdflv2', 'quality': <number>, 'audio_quality': <number>,\n",
            "<add>         'container': 'FLV', 'video_resolution': '<number>p', 'desc': '真彩 HDR'},\n",
            "         {'id': 'hdflv2_4k', 'quality': <number>, 'audio_quality': <number>,\n",
            "          'container': 'FLV', 'video_resolution': '<number>p', 'desc': '超清 <number>K'},\n",
            "         {'id': 'flv_p6<number>', 'quality': <number>, 'audio_quality': <number>,\n",
            "\n",
            "['class', 'Bilibili', 'VideoExtractor', '#', 'Bilibili', 'media', 'encoding', 'options', 'in', 'descending', 'quality', 'order', 'stream', 'types', '=', '<add>', 'id', 'hdflv2', 'quality', '<number>', 'audio', 'quality', '<number>', '<add>', 'container', 'FLV', 'video', 'resolution', '<number>', 'p', 'desc', 'HDR', 'id', 'hdflv2', '4k', 'quality', '<number>', 'audio', 'quality', '<number>', 'container', 'FLV', 'video', 'resolution', '<number>', 'p', 'desc', '<number>', 'K', 'id', 'flv', 'p6', '<number>', 'quality', '<number>', 'audio', 'quality', '<number>']\n",
            "\n",
            "['add', 'fake', 'header']\n",
            "@@ -123,10 +123,10 @@ def netease_song_download(song, output_dir='.', info_only=False, playlist_prefix\n",
            "                             output_dir=output_dir, info_only=info_only)\n",
            " \n",
            " def netease_download_common(title, url_best, output_dir, info_only):\n",
            "-    songtype, ext, size = url_info(url_best)\n",
            "+    songtype, ext, size = url_info(url_best, faker=True)\n",
            "     print_info(site_info, title, songtype, size)\n",
            "     if not info_only:\n",
            "-        download_urls([url_best], title, ext, size, output_dir)\n",
            "+        download_urls([url_best], title, ext, size, output_dir, faker=True)\n",
            " \n",
            " \n",
            " def netease_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\n",
            "\n",
            "\n",
            " def netease_song_download(song, output_dir='.', info_only=False, playlist_prefix\n",
            "                             output_dir=output_dir, info_only=info_only)\n",
            " \n",
            " def netease_download_common(title, url_best, output_dir, info_only):\n",
            "<del>    songtype, ext, size = url_info(url_best)\n",
            "<add>    songtype, ext, size = url_info(url_best, faker=True)\n",
            "     print_info(site_info, title, songtype, size)\n",
            "     if not info_only:\n",
            "<del>        download_urls([url_best], title, ext, size, output_dir)\n",
            "<add>        download_urls([url_best], title, ext, size, output_dir, faker=True)\n",
            " \n",
            " \n",
            " def netease_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\n",
            "\n",
            "['def', 'netease', 'song', 'download', 'song', 'output', 'dir', '=', 'info', 'only', '=', 'False', 'playlist', 'prefix', 'output', 'dir', '=', 'output', 'dir', 'info', 'only', '=', 'info', 'only', 'def', 'netease', 'download', 'common', 'title', 'url', 'best', 'output', 'dir', 'info', 'only', '<del>', 'songtype', 'ext', 'size', '=', 'url', 'info', 'url', 'best', '<add>', 'songtype', 'ext', 'size', '=', 'url', 'info', 'url', 'best', 'faker', '=', 'True', 'print', 'info', 'site', 'info', 'title', 'songtype', 'size', 'if', 'not', 'info', 'only', '<del>', 'download', 'urls', 'url', 'best', 'title', 'ext', 'size', 'output', 'dir', '<add>', 'download', 'urls', 'url', 'best', 'title', 'ext', 'size', 'output', 'dir', 'faker', '=', 'True', 'def', 'netease', 'download', 'url', 'output', 'dir', '=', 'merge', '=', 'True', 'info', 'only', '=', 'False', '*', '*', 'kwargs']\n",
            "\n",
            "['add', 'format', 'selection', 'for', 'ac', 'fun']\n",
            "@@ -1,168 +1,213 @@\n",
            " #!/usr/bin/env python\n",
            " \n",
            "-__all__ = ['acfun_download']\n",
            "-\n",
            " from ..common import *\n",
            "+from ..extractor import VideoExtractor\n",
            "+\n",
            "+class AcFun(VideoExtractor):\n",
            "+    name = \"AcFun\"\n",
            "+\n",
            "+    stream_types = [\n",
            "+        {'id': '2160P', 'qualityType': '2160p'},\n",
            "+        {'id': '1080P60', 'qualityType': '1080p60'},\n",
            "+        {'id': '720P60', 'qualityType': '720p60'},\n",
            "+        {'id': '1080P+', 'qualityType': '1080p+'},\n",
            "+        {'id': '1080P', 'qualityType': '1080p'},\n",
            "+        {'id': '720P', 'qualityType': '720p'},\n",
            "+        {'id': '540P', 'qualityType': '540p'},\n",
            "+        {'id': '360P', 'qualityType': '360p'}\n",
            "+    ]    \n",
            "+\n",
            "+    def prepare(self, **kwargs):\n",
            "+        assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', self.url)\n",
            "+\n",
            "+        if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', self.url):\n",
            "+            html = get_content(self.url, headers=fake_headers)\n",
            "+            json_text = match1(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "+            json_data = json.loads(json_text)\n",
            "+            vid = json_data.get('currentVideoInfo').get('id')\n",
            "+            up = json_data.get('user').get('name')\n",
            "+            self.title = json_data.get('title')\n",
            "+            video_list = json_data.get('videoList')\n",
            "+            if len(video_list) > 1:\n",
            "+                self.title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][0]\n",
            "+            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "+\n",
            "+        elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", self.url):\n",
            "+            html = get_content(self.url, headers=fake_headers)\n",
            "+            tag_script = match1(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "+            json_text = tag_script[tag_script.find('{') : tag_script.find('};') + 1]\n",
            "+            json_data = json.loads(json_text)\n",
            "+            self.title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "+            vid = str(json_data['videoId'])\n",
            "+            up = \"acfun\"\n",
            "+            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            " \n",
            "-from .le import letvcloud_download_by_vu\n",
            "-from .qq import qq_download_by_vid\n",
            "-from .sina import sina_download_by_vid\n",
            "-from .tudou import tudou_download_by_iid\n",
            "-from .youku import youku_download_by_vid\n",
            "-\n",
            "-import json\n",
            "-import re\n",
            "-import base64\n",
            "-import time\n",
            "-\n",
            "-def get_srt_json(id):\n",
            "-    url = 'http://danmu.aixifan.com/V2/%s' % id\n",
            "-    return get_content(url)\n",
            "-\n",
            "-def youku_acfun_proxy(vid, sign, ref):\n",
            "-    endpoint = 'http://player.acfun.cn/flash_data?vid={}&ct=85&ev=3&sign={}&time={}'\n",
            "-    url = endpoint.format(vid, sign, str(int(time.time() * 1000)))\n",
            "-    json_data = json.loads(get_content(url, headers=dict(referer=ref)))['data']\n",
            "-    enc_text = base64.b64decode(json_data)\n",
            "-    dec_text = rc4(b'8bdc7e1a', enc_text).decode('utf8')\n",
            "-    youku_json = json.loads(dec_text)\n",
            "-\n",
            "-    yk_streams = {}\n",
            "-    for stream in youku_json['stream']:\n",
            "-        tp = stream['stream_type']\n",
            "-        yk_streams[tp] = [], stream['total_size']\n",
            "-        if stream.get('segs'):\n",
            "-            for seg in stream['segs']:\n",
            "-                yk_streams[tp][0].append(seg['url'])\n",
            "-        else:\n",
            "-            yk_streams[tp] = stream['m3u8'], stream['total_size']\n",
            "-\n",
            "-    return yk_streams\n",
            "-\n",
            "-def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "-    \"\"\"str, str, str, bool, bool ->None\n",
            "-\n",
            "-    Download Acfun video by vid.\n",
            "-\n",
            "-    Call Acfun API, decide which site to use, and pass the job to its\n",
            "-    extractor.\n",
            "-    \"\"\"\n",
            "-\n",
            "-    #first call the main parasing API\n",
            "-    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid, headers=fake_headers))\n",
            "-\n",
            "-    sourceType = info['sourceType']\n",
            "-\n",
            "-    #decide sourceId to know which extractor to use\n",
            "-    if 'sourceId' in info: sourceId = info['sourceId']\n",
            "-    # danmakuId = info['danmakuId']\n",
            "-\n",
            "-    #call extractor decided by sourceId\n",
            "-    if sourceType == 'sina':\n",
            "-        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "-    elif sourceType == 'youku':\n",
            "-        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n",
            "-    elif sourceType == 'tudou':\n",
            "-        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "-    elif sourceType == 'qq':\n",
            "-        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "-    elif sourceType == 'letv':\n",
            "-        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "-    elif sourceType == 'zhuzhan':\n",
            "-        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this\n",
            "-#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player\n",
            "-#old code removed\n",
            "-        url = 'http://www.acfun.cn/v/ac' + vid\n",
            "-        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n",
            "-        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n",
            "-        for t in seq:\n",
            "-            if yk_streams.get(t):\n",
            "-                preferred = yk_streams[t]\n",
            "-                break\n",
            "-#total_size in the json could be incorrect(F.I. 0)\n",
            "-        size = 0\n",
            "-        for url in preferred[0]:\n",
            "-            _, _, seg_size = url_info(url)\n",
            "-            size += seg_size\n",
            "-#fallback to flvhd is not quite possible\n",
            "-        if re.search(r'fid=[0-9A-Z\\-]*.flv', preferred[0][0]):\n",
            "-            ext = 'flv'\n",
            "         else:\n",
            "-            ext = 'mp4'\n",
            "-        print_info(site_info, title, ext, size)\n",
            "-        if not info_only:\n",
            "-            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)\n",
            "-    else:\n",
            "-        raise NotImplementedError(sourceType)\n",
            "-\n",
            "-    if not info_only and not dry_run:\n",
            "-        if not kwargs['caption']:\n",
            "-            print('Skipping danmaku.')\n",
            "-            return\n",
            "-        try:\n",
            "-            title = get_filename(title)\n",
            "-            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n",
            "-            cmt = get_srt_json(vid)\n",
            "-            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:\n",
            "-                x.write(cmt)\n",
            "-        except:\n",
            "-            pass\n",
            "-\n",
            "-def acfun_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "-    assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', url)\n",
            "-\n",
            "-    def getM3u8UrlFromCurrentVideoInfo(currentVideoInfo):\n",
            "-        if 'playInfos' in currentVideoInfo:\n",
            "-            return currentVideoInfo['playInfos'][0]['playUrls'][0]\n",
            "-        elif 'ksPlayJson' in currentVideoInfo:\n",
            "-            ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "+            raise NotImplemented            \n",
            "+\n",
            "+        if 'ksPlayJson' in currentVideoInfo:\n",
            "+            durationMillis = currentVideoInfo['durationMillis']\n",
            "+            ksPlayJson = ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "             representation = ksPlayJson.get('adaptationSet')[0].get('representation')\n",
            "-            reps = []\n",
            "-            for one in representation:\n",
            "-                reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "-            return max(reps)[1]\n",
            "-\n",
            "-\n",
            "-    if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', url):\n",
            "-        html = get_content(url, headers=fake_headers)\n",
            "-        json_text = match1(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "-        json_data = json.loads(json_text)\n",
            "-        vid = json_data.get('currentVideoInfo').get('id')\n",
            "-        up = json_data.get('user').get('name')\n",
            "-        title = json_data.get('title')\n",
            "-        video_list = json_data.get('videoList')\n",
            "-        if len(video_list) > 1:\n",
            "-            title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][0]\n",
            "-        currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "-        m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "-    elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", url):\n",
            "-        html = get_content(url, headers=fake_headers)\n",
            "-        tag_script = match1(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "-        json_text = tag_script[tag_script.find('{') : tag_script.find('};') + 1]\n",
            "-        json_data = json.loads(json_text)\n",
            "-        title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "-        vid = str(json_data['videoId'])\n",
            "-        up = \"acfun\"\n",
            "-\n",
            "-        currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "-        m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "-\n",
            "-    else:\n",
            "-        raise NotImplemented\n",
            "-\n",
            "-    assert title and m3u8_url\n",
            "-    title = unescape_html(title)\n",
            "-    title = escape_file_path(title)\n",
            "-    p_title = r1('active\">([^<]+)', html)\n",
            "-    title = '%s (%s)' % (title, up)\n",
            "-    if p_title:\n",
            "-        title = '%s - %s' % (title, p_title)\n",
            "-\n",
            "-    print_info(site_info, title, 'm3u8', float('inf'))\n",
            "-    if not info_only:\n",
            "-        download_url_ffmpeg(m3u8_url, title, 'mp4', output_dir=output_dir, merge=merge)\n",
            "+            stream_list = representation\n",
            "+\n",
            "+        for stream in stream_list:\n",
            "+            m3u8_url = stream[\"url\"]\n",
            "+            size = durationMillis * stream[\"avgBitrate\"] / 8\n",
            "+            # size = float('inf')\n",
            "+            container = 'mp4'\n",
            "+            stream_id = stream[\"qualityLabel\"]\n",
            "+            quality = stream[\"qualityType\"]\n",
            "+            \n",
            "+            stream_data = dict(src=m3u8_url, size=size, container=container, quality=quality)\n",
            "+            self.streams[stream_id] = stream_data\n",
            "+\n",
            "+        assert self.title and m3u8_url\n",
            "+        self.title = unescape_html(self.title)\n",
            "+        self.title = escape_file_path(self.title)\n",
            "+        p_title = r1('active\">([^<]+)', html)\n",
            "+        self.title = '%s (%s)' % (self.title, up)\n",
            "+        if p_title:\n",
            "+            self.title = '%s - %s' % (self.title, p_title)       \n",
            "+\n",
            "+\n",
            "+    def download(self, **kwargs):\n",
            "+        if 'json_output' in kwargs and kwargs['json_output']:\n",
            "+            json_output.output(self)\n",
            "+        elif 'info_only' in kwargs and kwargs['info_only']:\n",
            "+            if 'stream_id' in kwargs and kwargs['stream_id']:\n",
            "+                # Display the stream\n",
            "+                stream_id = kwargs['stream_id']\n",
            "+                if 'index' not in kwargs:\n",
            "+                    self.p(stream_id)\n",
            "+                else:\n",
            "+                    self.p_i(stream_id)\n",
            "+            else:\n",
            "+                # Display all available streams\n",
            "+                if 'index' not in kwargs:\n",
            "+                    self.p([])\n",
            "+                else:\n",
            "+                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n",
            "+                    self.p_i(stream_id)\n",
            "+\n",
            "+        else:\n",
            "+            if 'stream_id' in kwargs and kwargs['stream_id']:\n",
            "+                # Download the stream\n",
            "+                stream_id = kwargs['stream_id']\n",
            "+            else:\n",
            "+                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n",
            "+\n",
            "+            if 'index' not in kwargs:\n",
            "+                self.p(stream_id)\n",
            "+            else:\n",
            "+                self.p_i(stream_id)\n",
            "+            if stream_id in self.streams:\n",
            "+                url = self.streams[stream_id]['src']\n",
            "+                ext = self.streams[stream_id]['container']\n",
            "+                total_size = self.streams[stream_id]['size']\n",
            "+\n",
            "+\n",
            "+            if ext == 'm3u8' or ext == 'm4a':\n",
            "+                ext = 'mp4'\n",
            "+\n",
            "+            if not url:\n",
            "+                log.wtf('[Failed] Cannot extract video source.')\n",
            "+            # For legacy main()\n",
            "+            headers = {}\n",
            "+            if self.ua is not None:\n",
            "+                headers['User-Agent'] = self.ua\n",
            "+            if self.referer is not None:\n",
            "+                headers['Referer'] = self.referer\n",
            "+\n",
            "+            download_url_ffmpeg(url, self.title, ext, output_dir=kwargs['output_dir'], merge=kwargs['merge'])                           \n",
            "+\n",
            "+            if 'caption' not in kwargs or not kwargs['caption']:\n",
            "+                print('Skipping captions or danmaku.')\n",
            "+                return\n",
            "+\n",
            "+            for lang in self.caption_tracks:\n",
            "+                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n",
            "+                print('Saving %s ... ' % filename, end=\"\", flush=True)\n",
            "+                srt = self.caption_tracks[lang]\n",
            "+                with open(os.path.join(kwargs['output_dir'], filename),\n",
            "+                          'w', encoding='utf-8') as x:\n",
            "+                    x.write(srt)\n",
            "+                print('Done.')\n",
            "+\n",
            "+            if self.danmaku is not None and not dry_run:\n",
            "+                filename = '{}.cmt.xml'.format(get_filename(self.title))\n",
            "+                print('Downloading {} ...\\n'.format(filename))\n",
            "+                with open(os.path.join(kwargs['output_dir'], filename), 'w', encoding='utf8') as fp:\n",
            "+                    fp.write(self.danmaku)\n",
            "+\n",
            "+            if self.lyrics is not None and not dry_run:\n",
            "+                filename = '{}.lrc'.format(get_filename(self.title))\n",
            "+                print('Downloading {} ...\\n'.format(filename))\n",
            "+                with open(os.path.join(kwargs['output_dir'], filename), 'w', encoding='utf8') as fp:\n",
            "+                    fp.write(self.lyrics)\n",
            "+\n",
            "+            # For main_dev()\n",
            "+            #download_urls(urls, self.title, self.streams[stream_id]['container'], self.streams[stream_id]['size'])\n",
            "+        keep_obj = kwargs.get('keep_obj', False)\n",
            "+        if not keep_obj:\n",
            "+            self.__init__()\n",
            "+\n",
            "+\n",
            "+    def acfun_download(self, url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "+        assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', url)\n",
            "+\n",
            "+        def getM3u8UrlFromCurrentVideoInfo(currentVideoInfo):\n",
            "+            if 'playInfos' in currentVideoInfo:\n",
            "+                return currentVideoInfo['playInfos'][0]['playUrls'][0]\n",
            "+            elif 'ksPlayJson' in currentVideoInfo:\n",
            "+                ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "+                representation = ksPlayJson.get('adaptationSet')[0].get('representation')\n",
            "+                reps = []\n",
            "+                for one in representation:\n",
            "+                    reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "+                return max(reps)[1]\n",
            "+\n",
            "+\n",
            "+        if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', url):\n",
            "+            html = get_content(url, headers=fake_headers)\n",
            "+            json_text = match1(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "+            json_data = json.loads(json_text)\n",
            "+            vid = json_data.get('currentVideoInfo').get('id')\n",
            "+            up = json_data.get('user').get('name')\n",
            "+            title = json_data.get('title')\n",
            "+            video_list = json_data.get('videoList')\n",
            "+            if len(video_list) > 1:\n",
            "+                title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][0]\n",
            "+            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "+            m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "+        elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", url):\n",
            "+            html = get_content(url, headers=fake_headers)\n",
            "+            tag_script = match1(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "+            json_text = tag_script[tag_script.find('{') : tag_script.find('};') + 1]\n",
            "+            json_data = json.loads(json_text)\n",
            "+            title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "+            vid = str(json_data['videoId'])\n",
            "+            up = \"acfun\"\n",
            "+\n",
            "+            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "+            m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "+\n",
            "+        else:\n",
            "+            raise NotImplemented\n",
            " \n",
            "+        assert title and m3u8_url\n",
            "+        title = unescape_html(title)\n",
            "+        title = escape_file_path(title)\n",
            "+        p_title = r1('active\">([^<]+)', html)\n",
            "+        title = '%s (%s)' % (title, up)\n",
            "+        if p_title:\n",
            "+            title = '%s - %s' % (title, p_title)\n",
            "+\n",
            "+        print_info(site_info, title, 'm3u8', float('inf'))\n",
            "+        if not info_only:\n",
            "+            download_url_ffmpeg(m3u8_url, title, 'mp4', output_dir=output_dir, merge=merge)\n",
            " \n",
            "+site = AcFun()\n",
            " site_info = \"AcFun.cn\"\n",
            "-download = acfun_download\n",
            "+download = site.download_by_url\n",
            " download_playlist = playlist_not_supported('acfun')\n",
            "\n",
            "\n",
            "\n",
            " #!/usr/bin/env python\n",
            " \n",
            "<del>__all__ = ['acfun_download']\n",
            "<del>\n",
            " from ..common import *\n",
            "<add>from ..extractor import VideoExtractor\n",
            "<add>\n",
            "<add>class AcFun(VideoExtractor):\n",
            "<add>    name = \"AcFun\"\n",
            "<add>\n",
            "<add>    stream_types = [\n",
            "<add>        {'id': '<number>P', 'qualityType': '<number>p'},\n",
            "<add>        {'id': '<number>P6<number>', 'qualityType': '<number>p6<number>'},\n",
            "<add>        {'id': '<number>P6<number>', 'qualityType': '<number>p6<number>'},\n",
            "<add>        {'id': '<number>P+', 'qualityType': '<number>p+'},\n",
            "<add>        {'id': '<number>P', 'qualityType': '<number>p'},\n",
            "<add>        {'id': '<number>P', 'qualityType': '<number>p'},\n",
            "<add>        {'id': '<number>P', 'qualityType': '<number>p'},\n",
            "<add>        {'id': '<number>P', 'qualityType': '<number>p'}\n",
            "<add>    ]    \n",
            "<add>\n",
            "<add>    def prepare(self, **kwargs):\n",
            "<add>        assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', self.url)\n",
            "<add>\n",
            "<add>        if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', self.url):\n",
            "<add>            html = get_content(self.url, headers=fake_headers)\n",
            "<add>            json_text = match1(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "<add>            json_data = json.loads(json_text)\n",
            "<add>            vid = json_data.get('currentVideoInfo').get('id')\n",
            "<add>            up = json_data.get('user').get('name')\n",
            "<add>            self.title = json_data.get('title')\n",
            "<add>            video_list = json_data.get('videoList')\n",
            "<add>            if len(video_list) > <number>:\n",
            "<add>                self.title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][<number>]\n",
            "<add>            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<add>\n",
            "<add>        elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", self.url):\n",
            "<add>            html = get_content(self.url, headers=fake_headers)\n",
            "<add>            tag_script = match1(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "<add>            json_text = tag_script[tag_script.find('{') : tag_script.find('};') + <number>]\n",
            "<add>            json_data = json.loads(json_text)\n",
            "<add>            self.title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "<add>            vid = str(json_data['videoId'])\n",
            "<add>            up = \"acfun\"\n",
            "<add>            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            " \n",
            "<del>from .le import letvcloud_download_by_vu\n",
            "<del>from .qq import qq_download_by_vid\n",
            "<del>from .sina import sina_download_by_vid\n",
            "<del>from .tudou import tudou_download_by_iid\n",
            "<del>from .youku import youku_download_by_vid\n",
            "<del>\n",
            "<del>import json\n",
            "<del>import re\n",
            "<del>import base6<number>\n",
            "<del>import time\n",
            "<del>\n",
            "<del>def get_srt_json(id):\n",
            "<del>    url = 'http://danmu.aixifan.com/V2/%s' % id\n",
            "<del>    return get_content(url)\n",
            "<del>\n",
            "<del>def youku_acfun_proxy(vid, sign, ref):\n",
            "<del>    endpoint = 'http://player.acfun.cn/flash_data?vid={}&ct=<number>&ev=<number>&sign={}&time={}'\n",
            "<del>    url = endpoint.format(vid, sign, str(int(time.time() * <number>)))\n",
            "<del>    json_data = json.loads(get_content(url, headers=dict(referer=ref)))['data']\n",
            "<del>    enc_text = base6<number>.b6<number>decode(json_data)\n",
            "<del>    dec_text = rc4(b'<number>bdc7e1a', enc_text).decode('utf8')\n",
            "<del>    youku_json = json.loads(dec_text)\n",
            "<del>\n",
            "<del>    yk_streams = {}\n",
            "<del>    for stream in youku_json['stream']:\n",
            "<del>        tp = stream['stream_type']\n",
            "<del>        yk_streams[tp] = [], stream['total_size']\n",
            "<del>        if stream.get('segs'):\n",
            "<del>            for seg in stream['segs']:\n",
            "<del>                yk_streams[tp][<number>].append(seg['url'])\n",
            "<del>        else:\n",
            "<del>            yk_streams[tp] = stream['m3u8'], stream['total_size']\n",
            "<del>\n",
            "<del>    return yk_streams\n",
            "<del>\n",
            "<del>def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "<del>    \"\"\"str, str, str, bool, bool ->None\n",
            "<del>\n",
            "<del>    Download Acfun video by vid.\n",
            "<del>\n",
            "<del>    Call Acfun API, decide which site to use, and pass the job to its\n",
            "<del>    extractor.\n",
            "<del>    \"\"\"\n",
            "<del>\n",
            "<del>    #first call the main parasing API\n",
            "<del>    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid, headers=fake_headers))\n",
            "<del>\n",
            "<del>    sourceType = info['sourceType']\n",
            "<del>\n",
            "<del>    #decide sourceId to know which extractor to use\n",
            "<del>    if 'sourceId' in info: sourceId = info['sourceId']\n",
            "<del>    # danmakuId = info['danmakuId']\n",
            "<del>\n",
            "<del>    #call extractor decided by sourceId\n",
            "<del>    if sourceType == 'sina':\n",
            "<del>        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "<del>    elif sourceType == 'youku':\n",
            "<del>        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n",
            "<del>    elif sourceType == 'tudou':\n",
            "<del>        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "<del>    elif sourceType == 'qq':\n",
            "<del>        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "<del>    elif sourceType == 'letv':\n",
            "<del>        letvcloud_download_by_vu(sourceId, '<number>d8c0<number>', title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "<del>    elif sourceType == 'zhuzhan':\n",
            "<del>        #As in Jul.<number>.<number>, Acfun is using embsig to anti hotlink so we need to pass this\n",
            "<del>#In Mar. <number> there is a dedicated ``acfun_proxy'' in youku cloud player\n",
            "<del>#old code removed\n",
            "<del>        url = 'http://www.acfun.cn/v/ac' + vid\n",
            "<del>        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n",
            "<del>        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n",
            "<del>        for t in seq:\n",
            "<del>            if yk_streams.get(t):\n",
            "<del>                preferred = yk_streams[t]\n",
            "<del>                break\n",
            "<del>#total_size in the json could be incorrect(F.I. <number>)\n",
            "<del>        size = <number>\n",
            "<del>        for url in preferred[<number>]:\n",
            "<del>            _, _, seg_size = url_info(url)\n",
            "<del>            size += seg_size\n",
            "<del>#fallback to flvhd is not quite possible\n",
            "<del>        if re.search(r'fid=[<number>-<number>A-Z\\-]*.flv', preferred[<number>][<number>]):\n",
            "<del>            ext = 'flv'\n",
            "         else:\n",
            "<del>            ext = 'mp4'\n",
            "<del>        print_info(site_info, title, ext, size)\n",
            "<del>        if not info_only:\n",
            "<del>            download_urls(preferred[<number>], title, ext, size, output_dir=output_dir, merge=merge)\n",
            "<del>    else:\n",
            "<del>        raise NotImplementedError(sourceType)\n",
            "<del>\n",
            "<del>    if not info_only and not dry_run:\n",
            "<del>        if not kwargs['caption']:\n",
            "<del>            print('Skipping danmaku.')\n",
            "<del>            return\n",
            "<del>        try:\n",
            "<del>            title = get_filename(title)\n",
            "<del>            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n",
            "<del>            cmt = get_srt_json(vid)\n",
            "<del>            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-<number>') as x:\n",
            "<del>                x.write(cmt)\n",
            "<del>        except:\n",
            "<del>            pass\n",
            "<del>\n",
            "<del>def acfun_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "<del>    assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', url)\n",
            "<del>\n",
            "<del>    def getM3u8UrlFromCurrentVideoInfo(currentVideoInfo):\n",
            "<del>        if 'playInfos' in currentVideoInfo:\n",
            "<del>            return currentVideoInfo['playInfos'][<number>]['playUrls'][<number>]\n",
            "<del>        elif 'ksPlayJson' in currentVideoInfo:\n",
            "<del>            ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "<add>            raise NotImplemented            \n",
            "<add>\n",
            "<add>        if 'ksPlayJson' in currentVideoInfo:\n",
            "<add>            durationMillis = currentVideoInfo['durationMillis']\n",
            "<add>            ksPlayJson = ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "             representation = ksPlayJson.get('adaptationSet')[<number>].get('representation')\n",
            "<del>            reps = []\n",
            "<del>            for one in representation:\n",
            "<del>                reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "<del>            return max(reps)[<number>]\n",
            "<del>\n",
            "<del>\n",
            "<del>    if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', url):\n",
            "<del>        html = get_content(url, headers=fake_headers)\n",
            "<del>        json_text = match1(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "<del>        json_data = json.loads(json_text)\n",
            "<del>        vid = json_data.get('currentVideoInfo').get('id')\n",
            "<del>        up = json_data.get('user').get('name')\n",
            "<del>        title = json_data.get('title')\n",
            "<del>        video_list = json_data.get('videoList')\n",
            "<del>        if len(video_list) > <number>:\n",
            "<del>            title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][<number>]\n",
            "<del>        currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<del>        m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "<del>    elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", url):\n",
            "<del>        html = get_content(url, headers=fake_headers)\n",
            "<del>        tag_script = match1(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "<del>        json_text = tag_script[tag_script.find('{') : tag_script.find('};') + <number>]\n",
            "<del>        json_data = json.loads(json_text)\n",
            "<del>        title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "<del>        vid = str(json_data['videoId'])\n",
            "<del>        up = \"acfun\"\n",
            "<del>\n",
            "<del>        currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<del>        m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "<del>\n",
            "<del>    else:\n",
            "<del>        raise NotImplemented\n",
            "<del>\n",
            "<del>    assert title and m3u8_url\n",
            "<del>    title = unescape_html(title)\n",
            "<del>    title = escape_file_path(title)\n",
            "<del>    p_title = r1('active\">([^<]+)', html)\n",
            "<del>    title = '%s (%s)' % (title, up)\n",
            "<del>    if p_title:\n",
            "<del>        title = '%s - %s' % (title, p_title)\n",
            "<del>\n",
            "<del>    print_info(site_info, title, 'm3u8', float('inf'))\n",
            "<del>    if not info_only:\n",
            "<del>        download_url_ffmpeg(m3u8_url, title, 'mp4', output_dir=output_dir, merge=merge)\n",
            "<add>            stream_list = representation\n",
            "<add>\n",
            "<add>        for stream in stream_list:\n",
            "<add>            m3u8_url = stream[\"url\"]\n",
            "<add>            size = durationMillis * stream[\"avgBitrate\"] / <number>\n",
            "<add>            # size = float('inf')\n",
            "<add>            container = 'mp4'\n",
            "<add>            stream_id = stream[\"qualityLabel\"]\n",
            "<add>            quality = stream[\"qualityType\"]\n",
            "<add>            \n",
            "<add>            stream_data = dict(src=m3u8_url, size=size, container=container, quality=quality)\n",
            "<add>            self.streams[stream_id] = stream_data\n",
            "<add>\n",
            "<add>        assert self.title and m3u8_url\n",
            "<add>        self.title = unescape_html(self.title)\n",
            "<add>        self.title = escape_file_path(self.title)\n",
            "<add>        p_title = r1('active\">([^<]+)', html)\n",
            "<add>        self.title = '%s (%s)' % (self.title, up)\n",
            "<add>        if p_title:\n",
            "<add>            self.title = '%s - %s' % (self.title, p_title)       \n",
            "<add>\n",
            "<add>\n",
            "<add>    def download(self, **kwargs):\n",
            "<add>        if 'json_output' in kwargs and kwargs['json_output']:\n",
            "<add>            json_output.output(self)\n",
            "<add>        elif 'info_only' in kwargs and kwargs['info_only']:\n",
            "<add>            if 'stream_id' in kwargs and kwargs['stream_id']:\n",
            "<add>                # Display the stream\n",
            "<add>                stream_id = kwargs['stream_id']\n",
            "<add>                if 'index' not in kwargs:\n",
            "<add>                    self.p(stream_id)\n",
            "<add>                else:\n",
            "<add>                    self.p_i(stream_id)\n",
            "<add>            else:\n",
            "<add>                # Display all available streams\n",
            "<add>                if 'index' not in kwargs:\n",
            "<add>                    self.p([])\n",
            "<add>                else:\n",
            "<add>                    stream_id = self.streams_sorted[<number>]['id'] if 'id' in self.streams_sorted[<number>] else self.streams_sorted[<number>]['itag']\n",
            "<add>                    self.p_i(stream_id)\n",
            "<add>\n",
            "<add>        else:\n",
            "<add>            if 'stream_id' in kwargs and kwargs['stream_id']:\n",
            "<add>                # Download the stream\n",
            "<add>                stream_id = kwargs['stream_id']\n",
            "<add>            else:\n",
            "<add>                stream_id = self.streams_sorted[<number>]['id'] if 'id' in self.streams_sorted[<number>] else self.streams_sorted[<number>]['itag']\n",
            "<add>\n",
            "<add>            if 'index' not in kwargs:\n",
            "<add>                self.p(stream_id)\n",
            "<add>            else:\n",
            "<add>                self.p_i(stream_id)\n",
            "<add>            if stream_id in self.streams:\n",
            "<add>                url = self.streams[stream_id]['src']\n",
            "<add>                ext = self.streams[stream_id]['container']\n",
            "<add>                total_size = self.streams[stream_id]['size']\n",
            "<add>\n",
            "<add>\n",
            "<add>            if ext == 'm3u8' or ext == 'm4a':\n",
            "<add>                ext = 'mp4'\n",
            "<add>\n",
            "<add>            if not url:\n",
            "<add>                log.wtf('[Failed] Cannot extract video source.')\n",
            "<add>            # For legacy main()\n",
            "<add>            headers = {}\n",
            "<add>            if self.ua is not None:\n",
            "<add>                headers['User-Agent'] = self.ua\n",
            "<add>            if self.referer is not None:\n",
            "<add>                headers['Referer'] = self.referer\n",
            "<add>\n",
            "<add>            download_url_ffmpeg(url, self.title, ext, output_dir=kwargs['output_dir'], merge=kwargs['merge'])                           \n",
            "<add>\n",
            "<add>            if 'caption' not in kwargs or not kwargs['caption']:\n",
            "<add>                print('Skipping captions or danmaku.')\n",
            "<add>                return\n",
            "<add>\n",
            "<add>            for lang in self.caption_tracks:\n",
            "<add>                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n",
            "<add>                print('Saving %s ... ' % filename, end=\"\", flush=True)\n",
            "<add>                srt = self.caption_tracks[lang]\n",
            "<add>                with open(os.path.join(kwargs['output_dir'], filename),\n",
            "<add>                          'w', encoding='utf-<number>') as x:\n",
            "<add>                    x.write(srt)\n",
            "<add>                print('Done.')\n",
            "<add>\n",
            "<add>            if self.danmaku is not None and not dry_run:\n",
            "<add>                filename = '{}.cmt.xml'.format(get_filename(self.title))\n",
            "<add>                print('Downloading {} ...\\n'.format(filename))\n",
            "<add>                with open(os.path.join(kwargs['output_dir'], filename), 'w', encoding='utf8') as fp:\n",
            "<add>                    fp.write(self.danmaku)\n",
            "<add>\n",
            "<add>            if self.lyrics is not None and not dry_run:\n",
            "<add>                filename = '{}.lrc'.format(get_filename(self.title))\n",
            "<add>                print('Downloading {} ...\\n'.format(filename))\n",
            "<add>                with open(os.path.join(kwargs['output_dir'], filename), 'w', encoding='utf8') as fp:\n",
            "<add>                    fp.write(self.lyrics)\n",
            "<add>\n",
            "<add>            # For main_dev()\n",
            "<add>            #download_urls(urls, self.title, self.streams[stream_id]['container'], self.streams[stream_id]['size'])\n",
            "<add>        keep_obj = kwargs.get('keep_obj', False)\n",
            "<add>        if not keep_obj:\n",
            "<add>            self.__init__()\n",
            "<add>\n",
            "<add>\n",
            "<add>    def acfun_download(self, url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "<add>        assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', url)\n",
            "<add>\n",
            "<add>        def getM3u8UrlFromCurrentVideoInfo(currentVideoInfo):\n",
            "<add>            if 'playInfos' in currentVideoInfo:\n",
            "<add>                return currentVideoInfo['playInfos'][<number>]['playUrls'][<number>]\n",
            "<add>            elif 'ksPlayJson' in currentVideoInfo:\n",
            "<add>                ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "<add>                representation = ksPlayJson.get('adaptationSet')[<number>].get('representation')\n",
            "<add>                reps = []\n",
            "<add>                for one in representation:\n",
            "<add>                    reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "<add>                return max(reps)[<number>]\n",
            "<add>\n",
            "<add>\n",
            "<add>        if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', url):\n",
            "<add>            html = get_content(url, headers=fake_headers)\n",
            "<add>            json_text = match1(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "<add>            json_data = json.loads(json_text)\n",
            "<add>            vid = json_data.get('currentVideoInfo').get('id')\n",
            "<add>            up = json_data.get('user').get('name')\n",
            "<add>            title = json_data.get('title')\n",
            "<add>            video_list = json_data.get('videoList')\n",
            "<add>            if len(video_list) > <number>:\n",
            "<add>                title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][<number>]\n",
            "<add>            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<add>            m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "<add>        elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", url):\n",
            "<add>            html = get_content(url, headers=fake_headers)\n",
            "<add>            tag_script = match1(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "<add>            json_text = tag_script[tag_script.find('{') : tag_script.find('};') + <number>]\n",
            "<add>            json_data = json.loads(json_text)\n",
            "<add>            title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "<add>            vid = str(json_data['videoId'])\n",
            "<add>            up = \"acfun\"\n",
            "<add>\n",
            "<add>            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<add>            m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "<add>\n",
            "<add>        else:\n",
            "<add>            raise NotImplemented\n",
            " \n",
            "<add>        assert title and m3u8_url\n",
            "<add>        title = unescape_html(title)\n",
            "<add>        title = escape_file_path(title)\n",
            "<add>        p_title = r1('active\">([^<]+)', html)\n",
            "<add>        title = '%s (%s)' % (title, up)\n",
            "<add>        if p_title:\n",
            "<add>            title = '%s - %s' % (title, p_title)\n",
            "<add>\n",
            "<add>        print_info(site_info, title, 'm3u8', float('inf'))\n",
            "<add>        if not info_only:\n",
            "<add>            download_url_ffmpeg(m3u8_url, title, 'mp4', output_dir=output_dir, merge=merge)\n",
            " \n",
            "<add>site = AcFun()\n",
            " site_info = \"AcFun.cn\"\n",
            "<del>download = acfun_download\n",
            "<add>download = site.download_by_url\n",
            " download_playlist = playlist_not_supported('acfun')\n",
            "\n",
            "['#', '!', '/', 'usr', '/', 'bin', '/', 'env', 'python', '<del>', 'all', '=', 'acfun', 'download', '<del>', 'from', 'common', 'import', '*', '<add>', 'from', 'extractor', 'import', 'VideoExtractor', '<add>', '<add>', 'class', 'AcFun', 'VideoExtractor', '<add>', 'name', '=', 'AcFun', '<add>', '<add>', 'stream', 'types', '=', '<add>', 'id', '<number>', 'P', 'qualityType', '<number>', 'p', '<add>', 'id', '<number>', 'P6', '<number>', 'qualityType', '<number>', 'p6', '<number>', '<add>', 'id', '<number>', 'P6', '<number>', 'qualityType', '<number>', 'p6', '<number>', '<add>', 'id', '<number>', 'P', '+', 'qualityType', '<number>', 'p', '+', '<add>', 'id', '<number>', 'P', 'qualityType', '<number>', 'p', '<add>', 'id', '<number>', 'P', 'qualityType', '<number>', 'p', '<add>', 'id', '<number>', 'P', 'qualityType', '<number>', 'p', '<add>', 'id', '<number>', 'P', 'qualityType', '<number>', 'p', '<add>', '<add>', '<add>', 'def', 'prepare', 'self', '*', '*', 'kwargs', '<add>', 'assert', 're', 'match', 'r', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'D', '|', 'bangumi', '/', 'D', 'D', 'd', '+', 'self', 'url', '<add>', '<add>', 'if', 're', 'match', 'r', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'D', '/', 'D', 'D', 'd', '+', 'self', 'url', '<add>', 'html', '=', 'get', 'content', 'self', 'url', 'headers', '=', 'fake', 'headers', '<add>', 'json', 'text', '=', 'match1', 'html', 'r', '?', 's', 'videoInfo', 's', '*=', 's', '*', '*', '?', '<add>', 'json', 'data', '=', 'json', 'loads', 'json', 'text', '<add>', 'vid', '=', 'json', 'data', 'get', 'currentVideoInfo', 'get', 'id', '<add>', 'up', '=', 'json', 'data', 'get', 'user', 'get', 'name', '<add>', 'self', 'title', '=', 'json', 'data', 'get', 'title', '<add>', 'video', 'list', '=', 'json', 'data', 'get', 'videoList', '<add>', 'if', 'len', 'video', 'list', '>', '<number>', '<add>', 'self', 'title', '+=', '-', '+', 'p', 'get', 'title', 'for', 'p', 'in', 'video', 'list', 'if', 'p', 'get', 'id', '==', 'vid', '<number>', '<add>', 'currentVideoInfo', '=', 'json', 'data', 'get', 'currentVideoInfo', '<add>', '<add>', 'elif', 're', 'match', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'bangumi', '/', 'aa', 'd', '+', 'self', 'url', '<add>', 'html', '=', 'get', 'content', 'self', 'url', 'headers', '=', 'fake', 'headers', '<add>', 'tag', 'script', '=', 'match1', 'html', 'r', '<', 'script', '>', 's', '*', 'window', 'pageInfo', '^', '<', '+', '<', '/', 'script', '>', '<add>', 'json', 'text', '=', 'tag', 'script', 'tag', 'script', 'find', 'tag', 'script', 'find', '+', '<number>', '<add>', 'json', 'data', '=', 'json', 'loads', 'json', 'text', '<add>', 'self', 'title', '=', 'json', 'data', 'bangumiTitle', '+', '+', 'json', 'data', 'episodeName', '+', '+', 'json', 'data', 'title', '<add>', 'vid', '=', 'str', 'json', 'data', 'videoId', '<add>', 'up', '=', 'acfun', '<add>', 'currentVideoInfo', '=', 'json', 'data', 'get', 'currentVideoInfo', '<del>', 'from', 'le', 'import', 'letvcloud', 'download', 'by', 'vu', '<del>', 'from', 'qq', 'import', 'qq', 'download', 'by', 'vid', '<del>', 'from', 'sina', 'import', 'sina', 'download', 'by', 'vid', '<del>', 'from', 'tudou', 'import', 'tudou', 'download', 'by', 'iid', '<del>', 'from', 'youku', 'import', 'youku', 'download', 'by', 'vid', '<del>', '<del>', 'import', 'json', '<del>', 'import', 're', '<del>', 'import', 'base6', '<number>', '<del>', 'import', 'time', '<del>', '<del>', 'def', 'get', 'srt', 'json', 'id', '<del>', 'url', '=', 'http', '/', '/', 'danmu', 'aixifan', 'com', '/', 'V2', '/', '%', 's', '%', 'id', '<del>', 'return', 'get', 'content', 'url', '<del>', '<del>', 'def', 'youku', 'acfun', 'proxy', 'vid', 'sign', 'ref', '<del>', 'endpoint', '=', 'http', '/', '/', 'player', 'acfun', 'cn', '/', 'flash', 'data', '?', 'vid', '=', '&', 'ct', '=', '<number>', '&', 'ev', '=', '<number>', '&', 'sign', '=', '&', 'time', '=', '<del>', 'url', '=', 'endpoint', 'format', 'vid', 'sign', 'str', 'int', 'time', 'time', '*', '<number>', '<del>', 'json', 'data', '=', 'json', 'loads', 'get', 'content', 'url', 'headers', '=', 'dict', 'referer', '=', 'ref', 'data', '<del>', 'enc', 'text', '=', 'base6', '<number>', 'b6', '<number>', 'decode', 'json', 'data', '<del>', 'dec', 'text', '=', 'rc4', 'b', '<number>', 'bdc7e1a', 'enc', 'text', 'decode', 'utf8', '<del>', 'youku', 'json', '=', 'json', 'loads', 'dec', 'text', '<del>', '<del>', 'yk', 'streams', '=', '<del>', 'for', 'stream', 'in', 'youku', 'json', 'stream', '<del>', 'tp', '=', 'stream', 'stream', 'type', '<del>', 'yk', 'streams', 'tp', '=', 'stream', 'total', 'size', '<del>', 'if', 'stream', 'get', 'segs', '<del>', 'for', 'seg', 'in', 'stream', 'segs', '<del>', 'yk', 'streams', 'tp', '<number>', 'append', 'seg', 'url', '<del>', 'else', '<del>', 'yk', 'streams', 'tp', '=', 'stream', 'm3u8', 'stream', 'total', 'size', '<del>', '<del>', 'return', 'yk', 'streams', '<del>', '<del>', 'def', 'acfun', 'download', 'by', 'vid', 'vid', 'title', 'output', 'dir', '=', 'merge', '=', 'True', 'info', 'only', '=', 'False', '*', '*', 'kwargs', '<del>', 'str', 'str', 'str', 'bool', 'bool', '-', '>', 'None', '<del>', '<del>', 'Download', 'Acfun', 'video', 'by', 'vid', '<del>', '<del>', 'Call', 'Acfun', 'API', 'decide', 'which', 'site', 'to', 'use', 'and', 'pass', 'the', 'job', 'to', 'its', '<del>', 'extractor', '<del>', '<del>', '<del>', '#', 'first', 'call', 'the', 'main', 'parasing', 'API', '<del>', 'info', '=', 'json', 'loads', 'get', 'content', 'http', '/', '/', 'www', 'acfun', 'cn', '/', 'video', '/', 'getVideo', 'aspx', '?', 'id', '=', '+', 'vid', 'headers', '=', 'fake', 'headers', '<del>', '<del>', 'sourceType', '=', 'info', 'sourceType', '<del>', '<del>', '#', 'decide', 'sourceId', 'to', 'know', 'which', 'extractor', 'to', 'use', '<del>', 'if', 'sourceId', 'in', 'info', 'sourceId', '=', 'info', 'sourceId', '<del>', '#', 'danmakuId', '=', 'info', 'danmakuId', '<del>', '<del>', '#', 'call', 'extractor', 'decided', 'by', 'sourceId', '<del>', 'if', 'sourceType', '==', 'sina', '<del>', 'sina', 'download', 'by', 'vid', 'sourceId', 'title', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'info', 'only', '=', 'info', 'only', '<del>', 'elif', 'sourceType', '==', 'youku', '<del>', 'youku', 'download', 'by', 'vid', 'sourceId', 'title', '=', 'title', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'info', 'only', '=', 'info', 'only', '*', '*', 'kwargs', '<del>', 'elif', 'sourceType', '==', 'tudou', '<del>', 'tudou', 'download', 'by', 'iid', 'sourceId', 'title', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'info', 'only', '=', 'info', 'only', '<del>', 'elif', 'sourceType', '==', 'qq', '<del>', 'qq', 'download', 'by', 'vid', 'sourceId', 'title', 'True', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'info', 'only', '=', 'info', 'only', '<del>', 'elif', 'sourceType', '==', 'letv', '<del>', 'letvcloud', 'download', 'by', 'vu', 'sourceId', '<number>', 'd8c0', '<number>', 'title', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'info', 'only', '=', 'info', 'only', '<del>', 'elif', 'sourceType', '==', 'zhuzhan', '<del>', '#', 'As', 'in', 'Jul', '<number>', '<number>', 'Acfun', 'is', 'using', 'embsig', 'to', 'anti', 'hotlink', 'so', 'we', 'need', 'to', 'pass', 'this', '<del>', '#', 'In', 'Mar', '<number>', 'there', 'is', 'a', 'dedicated', 'acfun', 'proxy', 'in', 'youku', 'cloud', 'player', '<del>', '#', 'old', 'code', 'removed', '<del>', 'url', '=', 'http', '/', '/', 'www', 'acfun', 'cn', '/', 'v', '/', 'ac', '+', 'vid', '<del>', 'yk', 'streams', '=', 'youku', 'acfun', 'proxy', 'info', 'sourceId', 'info', 'encode', 'url', '<del>', 'seq', '=', 'mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd', '<del>', 'for', 't', 'in', 'seq', '<del>', 'if', 'yk', 'streams', 'get', 't', '<del>', 'preferred', '=', 'yk', 'streams', 't', '<del>', 'break', '<del>', '#', 'total', 'size', 'in', 'the', 'json', 'could', 'be', 'incorrect', 'F', 'I', '<number>', '<del>', 'size', '=', '<number>', '<del>', 'for', 'url', 'in', 'preferred', '<number>', '<del>', 'seg', 'size', '=', 'url', 'info', 'url', '<del>', 'size', '+=', 'seg', 'size', '<del>', '#', 'fallback', 'to', 'flvhd', 'is', 'not', 'quite', 'possible', '<del>', 'if', 're', 'search', 'r', 'fid', '=', '<number>', '-', '<number>', 'A', '-', 'Z', '-', '*', 'flv', 'preferred', '<number>', '<number>', '<del>', 'ext', '=', 'flv', 'else', '<del>', 'ext', '=', 'mp4', '<del>', 'print', 'info', 'site', 'info', 'title', 'ext', 'size', '<del>', 'if', 'not', 'info', 'only', '<del>', 'download', 'urls', 'preferred', '<number>', 'title', 'ext', 'size', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', '<del>', 'else', '<del>', 'raise', 'NotImplementedError', 'sourceType', '<del>', '<del>', 'if', 'not', 'info', 'only', 'and', 'not', 'dry', 'run', '<del>', 'if', 'not', 'kwargs', 'caption', '<del>', 'print', 'Skipping', 'danmaku', '<del>', 'return', '<del>', 'try', '<del>', 'title', '=', 'get', 'filename', 'title', '<del>', 'print', 'Downloading', '%', 's', 'n', '%', 'title', '+', 'cmt', 'json', '<del>', 'cmt', '=', 'get', 'srt', 'json', 'vid', '<del>', 'with', 'open', 'os', 'path', 'join', 'output', 'dir', 'title', '+', 'cmt', 'json', 'w', 'encoding', '=', 'utf', '-', '<number>', 'as', 'x', '<del>', 'x', 'write', 'cmt', '<del>', 'except', '<del>', 'pass', '<del>', '<del>', 'def', 'acfun', 'download', 'url', 'output', 'dir', '=', 'merge', '=', 'True', 'info', 'only', '=', 'False', '*', '*', 'kwargs', '<del>', 'assert', 're', 'match', 'r', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'D', '|', 'bangumi', '/', 'D', 'D', 'd', '+', 'url', '<del>', '<del>', 'def', 'getM3u8UrlFromCurrentVideoInfo', 'currentVideoInfo', '<del>', 'if', 'playInfos', 'in', 'currentVideoInfo', '<del>', 'return', 'currentVideoInfo', 'playInfos', '<number>', 'playUrls', '<number>', '<del>', 'elif', 'ksPlayJson', 'in', 'currentVideoInfo', '<del>', 'ksPlayJson', '=', 'json', 'loads', 'currentVideoInfo', 'ksPlayJson', '<add>', 'raise', 'NotImplemented', '<add>', '<add>', 'if', 'ksPlayJson', 'in', 'currentVideoInfo', '<add>', 'durationMillis', '=', 'currentVideoInfo', 'durationMillis', '<add>', 'ksPlayJson', '=', 'ksPlayJson', '=', 'json', 'loads', 'currentVideoInfo', 'ksPlayJson', 'representation', '=', 'ksPlayJson', 'get', 'adaptationSet', '<number>', 'get', 'representation', '<del>', 'reps', '=', '<del>', 'for', 'one', 'in', 'representation', '<del>', 'reps', 'append', 'one', 'width', '*', 'one', 'height', 'one', 'url', 'one', 'backupUrl', '<del>', 'return', 'max', 'reps', '<number>', '<del>', '<del>', '<del>', 'if', 're', 'match', 'r', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'D', '/', 'D', 'D', 'd', '+', 'url', '<del>', 'html', '=', 'get', 'content', 'url', 'headers', '=', 'fake', 'headers', '<del>', 'json', 'text', '=', 'match1', 'html', 'r', '?', 's', 'videoInfo', 's', '*=', 's', '*', '*', '?', '<del>', 'json', 'data', '=', 'json', 'loads', 'json', 'text', '<del>', 'vid', '=', 'json', 'data', 'get', 'currentVideoInfo', 'get', 'id', '<del>', 'up', '=', 'json', 'data', 'get', 'user', 'get', 'name', '<del>', 'title', '=', 'json', 'data', 'get', 'title', '<del>', 'video', 'list', '=', 'json', 'data', 'get', 'videoList', '<del>', 'if', 'len', 'video', 'list', '>', '<number>', '<del>', 'title', '+=', '-', '+', 'p', 'get', 'title', 'for', 'p', 'in', 'video', 'list', 'if', 'p', 'get', 'id', '==', 'vid', '<number>', '<del>', 'currentVideoInfo', '=', 'json', 'data', 'get', 'currentVideoInfo', '<del>', 'm3u8', 'url', '=', 'getM3u8UrlFromCurrentVideoInfo', 'currentVideoInfo', '<del>', 'elif', 're', 'match', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'bangumi', '/', 'aa', 'd', '+', 'url', '<del>', 'html', '=', 'get', 'content', 'url', 'headers', '=', 'fake', 'headers', '<del>', 'tag', 'script', '=', 'match1', 'html', 'r', '<', 'script', '>', 's', '*', 'window', 'pageInfo', '^', '<', '+', '<', '/', 'script', '>', '<del>', 'json', 'text', '=', 'tag', 'script', 'tag', 'script', 'find', 'tag', 'script', 'find', '+', '<number>', '<del>', 'json', 'data', '=', 'json', 'loads', 'json', 'text', '<del>', 'title', '=', 'json', 'data', 'bangumiTitle', '+', '+', 'json', 'data', 'episodeName', '+', '+', 'json', 'data', 'title', '<del>', 'vid', '=', 'str', 'json', 'data', 'videoId', '<del>', 'up', '=', 'acfun', '<del>', '<del>', 'currentVideoInfo', '=', 'json', 'data', 'get', 'currentVideoInfo', '<del>', 'm3u8', 'url', '=', 'getM3u8UrlFromCurrentVideoInfo', 'currentVideoInfo', '<del>', '<del>', 'else', '<del>', 'raise', 'NotImplemented', '<del>', '<del>', 'assert', 'title', 'and', 'm3u8', 'url', '<del>', 'title', '=', 'unescape', 'html', 'title', '<del>', 'title', '=', 'escape', 'file', 'path', 'title', '<del>', 'p', 'title', '=', 'r1', 'active', '>', '^', '<', '+', 'html', '<del>', 'title', '=', '%', 's', '%', 's', '%', 'title', 'up', '<del>', 'if', 'p', 'title', '<del>', 'title', '=', '%', 's', '-', '%', 's', '%', 'title', 'p', 'title', '<del>', '<del>', 'print', 'info', 'site', 'info', 'title', 'm3u8', 'float', 'inf', '<del>', 'if', 'not', 'info', 'only', '<del>', 'download', 'url', 'ffmpeg', 'm3u8', 'url', 'title', 'mp4', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', '<add>', 'stream', 'list', '=', 'representation', '<add>', '<add>', 'for', 'stream', 'in', 'stream', 'list', '<add>', 'm3u8', 'url', '=', 'stream', 'url', '<add>', 'size', '=', 'durationMillis', '*', 'stream', 'avgBitrate', '/', '<number>', '<add>', '#', 'size', '=', 'float', 'inf', '<add>', 'container', '=', 'mp4', '<add>', 'stream', 'id', '=', 'stream', 'qualityLabel', '<add>', 'quality', '=', 'stream', 'qualityType', '<add>', '<add>', 'stream', 'data', '=', 'dict', 'src', '=', 'm3u8', 'url', 'size', '=', 'size', 'container', '=', 'container', 'quality', '=', 'quality', '<add>', 'self', 'streams', 'stream', 'id', '=', 'stream', 'data', '<add>', '<add>', 'assert', 'self', 'title', 'and', 'm3u8', 'url', '<add>', 'self', 'title', '=', 'unescape', 'html', 'self', 'title', '<add>', 'self', 'title', '=', 'escape', 'file', 'path', 'self', 'title', '<add>', 'p', 'title', '=', 'r1', 'active', '>', '^', '<', '+', 'html', '<add>', 'self', 'title', '=', '%', 's', '%', 's', '%', 'self', 'title', 'up', '<add>', 'if', 'p', 'title', '<add>', 'self', 'title', '=', '%', 's', '-', '%', 's', '%', 'self', 'title', 'p', 'title', '<add>', '<add>', '<add>', 'def', 'download', 'self', '*', '*', 'kwargs', '<add>', 'if', 'json', 'output', 'in', 'kwargs', 'and', 'kwargs', 'json', 'output', '<add>', 'json', 'output', 'output', 'self', '<add>', 'elif', 'info', 'only', 'in', 'kwargs', 'and', 'kwargs', 'info', 'only', '<add>', 'if', 'stream', 'id', 'in', 'kwargs', 'and', 'kwargs', 'stream', 'id', '<add>', '#', 'Display', 'the', 'stream', '<add>', 'stream', 'id', '=', 'kwargs', 'stream', 'id', '<add>', 'if', 'index', 'not', 'in', 'kwargs', '<add>', 'self', 'p', 'stream', 'id', '<add>', 'else', '<add>', 'self', 'p', 'i', 'stream', 'id', '<add>', 'else', '<add>', '#', 'Display', 'all', 'available', 'streams', '<add>', 'if', 'index', 'not', 'in', 'kwargs', '<add>', 'self', 'p', '<add>', 'else', '<add>', 'stream', 'id', '=', 'self', 'streams', 'sorted', '<number>', 'id', 'if', 'id', 'in', 'self', 'streams', 'sorted', '<number>', 'else', 'self', 'streams', 'sorted', '<number>', 'itag', '<add>', 'self', 'p', 'i', 'stream', 'id', '<add>', '<add>', 'else', '<add>', 'if', 'stream', 'id', 'in', 'kwargs', 'and', 'kwargs', 'stream', 'id', '<add>', '#', 'Download', 'the', 'stream', '<add>', 'stream', 'id', '=', 'kwargs', 'stream', 'id', '<add>', 'else', '<add>', 'stream', 'id', '=', 'self', 'streams', 'sorted', '<number>', 'id', 'if', 'id', 'in', 'self', 'streams', 'sorted', '<number>', 'else', 'self', 'streams', 'sorted', '<number>', 'itag', '<add>', '<add>', 'if', 'index', 'not', 'in', 'kwargs', '<add>', 'self', 'p', 'stream', 'id', '<add>', 'else', '<add>', 'self', 'p', 'i', 'stream', 'id', '<add>', 'if', 'stream', 'id', 'in', 'self', 'streams', '<add>', 'url', '=', 'self', 'streams', 'stream', 'id', 'src', '<add>', 'ext', '=', 'self', 'streams', 'stream', 'id', 'container', '<add>', 'total', 'size', '=', 'self', 'streams', 'stream', 'id', 'size', '<add>', '<add>', '<add>', 'if', 'ext', '==', 'm3u8', 'or', 'ext', '==', 'm4a', '<add>', 'ext', '=', 'mp4', '<add>', '<add>', 'if', 'not', 'url', '<add>', 'log', 'wtf', 'Failed', 'Cannot', 'extract', 'video', 'source', '<add>', '#', 'For', 'legacy', 'main', '<add>', 'headers', '=', '<add>', 'if', 'self', 'ua', 'is', 'not', 'None', '<add>', 'headers', 'User', '-', 'Agent', '=', 'self', 'ua', '<add>', 'if', 'self', 'referer', 'is', 'not', 'None', '<add>', 'headers', 'Referer', '=', 'self', 'referer', '<add>', '<add>', 'download', 'url', 'ffmpeg', 'url', 'self', 'title', 'ext', 'output', 'dir', '=', 'kwargs', 'output', 'dir', 'merge', '=', 'kwargs', 'merge', '<add>', '<add>', 'if', 'caption', 'not', 'in', 'kwargs', 'or', 'not', 'kwargs', 'caption', '<add>', 'print', 'Skipping', 'captions', 'or', 'danmaku', '<add>', 'return', '<add>', '<add>', 'for', 'lang', 'in', 'self', 'caption', 'tracks', '<add>', 'filename', '=', '%', 's', '%', 's', 'srt', '%', 'get', 'filename', 'self', 'title', 'lang', '<add>', 'print', 'Saving', '%', 's', '%', 'filename', 'end', '=', 'flush', '=', 'True', '<add>', 'srt', '=', 'self', 'caption', 'tracks', 'lang', '<add>', 'with', 'open', 'os', 'path', 'join', 'kwargs', 'output', 'dir', 'filename', '<add>', 'w', 'encoding', '=', 'utf', '-', '<number>', 'as', 'x', '<add>', 'x', 'write', 'srt', '<add>', 'print', 'Done', '<add>', '<add>', 'if', 'self', 'danmaku', 'is', 'not', 'None', 'and', 'not', 'dry', 'run', '<add>', 'filename', '=', 'cmt', 'xml', 'format', 'get', 'filename', 'self', 'title', '<add>', 'print', 'Downloading', 'n', 'format', 'filename', '<add>', 'with', 'open', 'os', 'path', 'join', 'kwargs', 'output', 'dir', 'filename', 'w', 'encoding', '=', 'utf8', 'as', 'fp', '<add>', 'fp', 'write', 'self', 'danmaku', '<add>', '<add>', 'if', 'self', 'lyrics', 'is', 'not', 'None', 'and', 'not', 'dry', 'run', '<add>', 'filename', '=', 'lrc', 'format', 'get', 'filename', 'self', 'title', '<add>', 'print', 'Downloading', 'n', 'format', 'filename', '<add>', 'with', 'open', 'os', 'path', 'join', 'kwargs', 'output', 'dir', 'filename', 'w', 'encoding', '=', 'utf8', 'as', 'fp', '<add>', 'fp', 'write', 'self', 'lyrics', '<add>', '<add>', '#', 'For', 'main', 'dev', '<add>', '#', 'download', 'urls', 'urls', 'self', 'title', 'self', 'streams', 'stream', 'id', 'container', 'self', 'streams', 'stream', 'id', 'size', '<add>', 'keep', 'obj', '=', 'kwargs', 'get', 'keep', 'obj', 'False', '<add>', 'if', 'not', 'keep', 'obj', '<add>', 'self', 'init', '<add>', '<add>', '<add>', 'def', 'acfun', 'download', 'self', 'url', 'output', 'dir', '=', 'merge', '=', 'True', 'info', 'only', '=', 'False', '*', '*', 'kwargs', '<add>', 'assert', 're', 'match', 'r', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'D', '|', 'bangumi', '/', 'D', 'D', 'd', '+', 'url', '<add>', '<add>', 'def', 'getM3u8UrlFromCurrentVideoInfo', 'currentVideoInfo', '<add>', 'if', 'playInfos', 'in', 'currentVideoInfo', '<add>', 'return', 'currentVideoInfo', 'playInfos', '<number>', 'playUrls', '<number>', '<add>', 'elif', 'ksPlayJson', 'in', 'currentVideoInfo', '<add>', 'ksPlayJson', '=', 'json', 'loads', 'currentVideoInfo', 'ksPlayJson', '<add>', 'representation', '=', 'ksPlayJson', 'get', 'adaptationSet', '<number>', 'get', 'representation', '<add>', 'reps', '=', '<add>', 'for', 'one', 'in', 'representation', '<add>', 'reps', 'append', 'one', 'width', '*', 'one', 'height', 'one', 'url', 'one', 'backupUrl', '<add>', 'return', 'max', 'reps', '<number>', '<add>', '<add>', '<add>', 'if', 're', 'match', 'r', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'D', '/', 'D', 'D', 'd', '+', 'url', '<add>', 'html', '=', 'get', 'content', 'url', 'headers', '=', 'fake', 'headers', '<add>', 'json', 'text', '=', 'match1', 'html', 'r', '?', 's', 'videoInfo', 's', '*=', 's', '*', '*', '?', '<add>', 'json', 'data', '=', 'json', 'loads', 'json', 'text', '<add>', 'vid', '=', 'json', 'data', 'get', 'currentVideoInfo', 'get', 'id', '<add>', 'up', '=', 'json', 'data', 'get', 'user', 'get', 'name', '<add>', 'title', '=', 'json', 'data', 'get', 'title', '<add>', 'video', 'list', '=', 'json', 'data', 'get', 'videoList', '<add>', 'if', 'len', 'video', 'list', '>', '<number>', '<add>', 'title', '+=', '-', '+', 'p', 'get', 'title', 'for', 'p', 'in', 'video', 'list', 'if', 'p', 'get', 'id', '==', 'vid', '<number>', '<add>', 'currentVideoInfo', '=', 'json', 'data', 'get', 'currentVideoInfo', '<add>', 'm3u8', 'url', '=', 'getM3u8UrlFromCurrentVideoInfo', 'currentVideoInfo', '<add>', 'elif', 're', 'match', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'bangumi', '/', 'aa', 'd', '+', 'url', '<add>', 'html', '=', 'get', 'content', 'url', 'headers', '=', 'fake', 'headers', '<add>', 'tag', 'script', '=', 'match1', 'html', 'r', '<', 'script', '>', 's', '*', 'window', 'pageInfo', '^', '<', '+', '<', '/', 'script', '>', '<add>', 'json', 'text', '=', 'tag', 'script', 'tag', 'script', 'find', 'tag', 'script', 'find', '+', '<number>', '<add>', 'json', 'data', '=', 'json', 'loads', 'json', 'text', '<add>', 'title', '=', 'json', 'data', 'bangumiTitle', '+', '+', 'json', 'data', 'episodeName', '+', '+', 'json', 'data', 'title', '<add>', 'vid', '=', 'str', 'json', 'data', 'videoId', '<add>', 'up', '=', 'acfun', '<add>', '<add>', 'currentVideoInfo', '=', 'json', 'data', 'get', 'currentVideoInfo', '<add>', 'm3u8', 'url', '=', 'getM3u8UrlFromCurrentVideoInfo', 'currentVideoInfo', '<add>', '<add>', 'else', '<add>', 'raise', 'NotImplemented', '<add>', 'assert', 'title', 'and', 'm3u8', 'url', '<add>', 'title', '=', 'unescape', 'html', 'title', '<add>', 'title', '=', 'escape', 'file', 'path', 'title', '<add>', 'p', 'title', '=', 'r1', 'active', '>', '^', '<', '+', 'html', '<add>', 'title', '=', '%', 's', '%', 's', '%', 'title', 'up', '<add>', 'if', 'p', 'title', '<add>', 'title', '=', '%', 's', '-', '%', 's', '%', 'title', 'p', 'title', '<add>', '<add>', 'print', 'info', 'site', 'info', 'title', 'm3u8', 'float', 'inf', '<add>', 'if', 'not', 'info', 'only', '<add>', 'download', 'url', 'ffmpeg', 'm3u8', 'url', 'title', 'mp4', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', '<add>', 'site', '=', 'AcFun', 'site', 'info', '=', 'AcFun', 'cn', '<del>', 'download', '=', 'acfun', 'download', '<add>', 'download', '=', 'site', 'download', 'by', 'url', 'download', 'playlist', '=', 'playlist', 'not', 'supported', 'acfun']\n",
            "\n",
            "['fixed', 'tiktok', 'extraction']\n",
            "@@ -15,16 +15,16 @@ def tiktok_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "     uniqueId = videoData['authorInfos'].get('uniqueId')\n",
            "     nickName = videoData['authorInfos'].get('nickName')\n",
            " \n",
            "-    for i, url in enumerate(urls):\n",
            "+    for i, videoUrl in enumerate(urls):\n",
            "         title = '%s [%s]' % (nickName or uniqueId, videoId)\n",
            "         if len(urls) > 1:\n",
            "             title = '%s [%s]' % (title, i)\n",
            " \n",
            "-        mime, ext, size = url_info(url)\n",
            "+        mime, ext, size = url_info(videoUrl, headers={'Referer': url})\n",
            " \n",
            "         print_info(site_info, title, mime, size)\n",
            "         if not info_only:\n",
            "-            download_urls([url], title, ext, size, output_dir=output_dir, merge=merge)\n",
            "+            download_urls([videoUrl], title, ext, size, output_dir=output_dir, merge=merge, headers={'Referer': url})\n",
            " \n",
            " site_info = \"TikTok.com\"\n",
            " download = tiktok_download\n",
            "\n",
            "\n",
            " def tiktok_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "     uniqueId = videoData['authorInfos'].get('uniqueId')\n",
            "     nickName = videoData['authorInfos'].get('nickName')\n",
            " \n",
            "<del>    for i, url in enumerate(urls):\n",
            "<add>    for i, videoUrl in enumerate(urls):\n",
            "         title = '%s [%s]' % (nickName or uniqueId, videoId)\n",
            "         if len(urls) > <number>:\n",
            "             title = '%s [%s]' % (title, i)\n",
            " \n",
            "<del>        mime, ext, size = url_info(url)\n",
            "<add>        mime, ext, size = url_info(videoUrl, headers={'Referer': url})\n",
            " \n",
            "         print_info(site_info, title, mime, size)\n",
            "         if not info_only:\n",
            "<del>            download_urls([url], title, ext, size, output_dir=output_dir, merge=merge)\n",
            "<add>            download_urls([videoUrl], title, ext, size, output_dir=output_dir, merge=merge, headers={'Referer': url})\n",
            " \n",
            " site_info = \"TikTok.com\"\n",
            " download = tiktok_download\n",
            "\n",
            "['def', 'tiktok', 'download', 'url', 'output', 'dir', '=', 'merge', '=', 'True', 'info', 'only', '=', 'False', '*', '*', 'kwargs', 'uniqueId', '=', 'videoData', 'authorInfos', 'get', 'uniqueId', 'nickName', '=', 'videoData', 'authorInfos', 'get', 'nickName', '<del>', 'for', 'i', 'url', 'in', 'enumerate', 'urls', '<add>', 'for', 'i', 'videoUrl', 'in', 'enumerate', 'urls', 'title', '=', '%', 's', '%', 's', '%', 'nickName', 'or', 'uniqueId', 'videoId', 'if', 'len', 'urls', '>', '<number>', 'title', '=', '%', 's', '%', 's', '%', 'title', 'i', '<del>', 'mime', 'ext', 'size', '=', 'url', 'info', 'url', '<add>', 'mime', 'ext', 'size', '=', 'url', 'info', 'videoUrl', 'headers', '=', 'Referer', 'url', 'print', 'info', 'site', 'info', 'title', 'mime', 'size', 'if', 'not', 'info', 'only', '<del>', 'download', 'urls', 'url', 'title', 'ext', 'size', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', '<add>', 'download', 'urls', 'videoUrl', 'title', 'ext', 'size', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'headers', '=', 'Referer', 'url', 'site', 'info', '=', 'TikTok', 'com', 'download', '=', 'tiktok', 'download']\n",
            "\n",
            "['fix', 'iqiyi', 'playlist', 'extrator']\n",
            "@@ -119,10 +119,10 @@ class Iqiyi(VideoExtractor):\n",
            "         self.url = url\n",
            " \n",
            "         video_page = get_content(url)\n",
            "-        videos = set(re.findall(r'<a href=\"(http://www\\.iqiyi\\.com/v_[^\"]+)\"', video_page))\n",
            "+        videos = set(re.findall(r'<a href=\"(?=https?:)?(//www\\.iqiyi\\.com/v_[^\"]+)\"', video_page))\n",
            " \n",
            "         for video in videos:\n",
            "-            self.__class__().download_by_url(video, **kwargs)\n",
            "+            self.__class__().download_by_url('https:' + video, **kwargs)\n",
            " \n",
            "     def prepare(self, **kwargs):\n",
            "         assert self.url or self.vid\n",
            "@@ -153,7 +153,7 @@ class Iqiyi(VideoExtractor):\n",
            "             except Exception as e:\n",
            "                 log.i(\"vd: {} is not handled\".format(stream['vd']))\n",
            "                 log.i(\"info is {}\".format(stream))\n",
            "-    \n",
            "+\n",
            " \n",
            "     def download(self, **kwargs):\n",
            "         \"\"\"Override the original one\n",
            "@@ -201,7 +201,7 @@ class Iqiyi(VideoExtractor):\n",
            "             if not urls:\n",
            "                 log.wtf('[Failed] Cannot extract video source.')\n",
            "             # For legacy main()\n",
            "-            \n",
            "+\n",
            "             #Here's the change!!\n",
            "             download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n",
            " \n",
            "@@ -215,7 +215,7 @@ class Iqiyi(VideoExtractor):\n",
            "                 with open(os.path.join(kwargs['output_dir'], filename),\n",
            "                           'w', encoding='utf-8') as x:\n",
            "                     x.write(srt)\n",
            "-                print('Done.')    \n",
            "+                print('Done.')\n",
            " \n",
            " '''\n",
            "         if info[\"code\"] != \"A000000\":\n",
            "\n",
            "\n",
            " class Iqiyi(VideoExtractor):\n",
            "         self.url = url\n",
            " \n",
            "         video_page = get_content(url)\n",
            "<del>        videos = set(re.findall(r'<a href=\"(http://www\\.iqiyi\\.com/v_[^\"]+)\"', video_page))\n",
            "<add>        videos = set(re.findall(r'<a href=\"(?=https?:)?(//www\\.iqiyi\\.com/v_[^\"]+)\"', video_page))\n",
            " \n",
            "         for video in videos:\n",
            "<del>            self.__class__().download_by_url(video, **kwargs)\n",
            "<add>            self.__class__().download_by_url('https:' + video, **kwargs)\n",
            " \n",
            "     def prepare(self, **kwargs):\n",
            "         assert self.url or self.vid\n",
            "\n",
            " class Iqiyi(VideoExtractor):\n",
            "             except Exception as e:\n",
            "                 log.i(\"vd: {} is not handled\".format(stream['vd']))\n",
            "                 log.i(\"info is {}\".format(stream))\n",
            "<del>    \n",
            "<add>\n",
            " \n",
            "     def download(self, **kwargs):\n",
            "         \"\"\"Override the original one\n",
            "\n",
            " class Iqiyi(VideoExtractor):\n",
            "             if not urls:\n",
            "                 log.wtf('[Failed] Cannot extract video source.')\n",
            "             # For legacy main()\n",
            "<del>            \n",
            "<add>\n",
            "             #Here's the change!!\n",
            "             download_url_ffmpeg(urls[<number>], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n",
            " \n",
            "\n",
            " class Iqiyi(VideoExtractor):\n",
            "                 with open(os.path.join(kwargs['output_dir'], filename),\n",
            "                           'w', encoding='utf-<number>') as x:\n",
            "                     x.write(srt)\n",
            "<del>                print('Done.')    \n",
            "<add>                print('Done.')\n",
            " \n",
            " '''\n",
            "         if info[\"code\"] != \"A0<number>\":\n",
            "\n",
            "['class', 'Iqiyi', 'VideoExtractor', 'self', 'url', '=', 'url', 'video', 'page', '=', 'get', 'content', 'url', '<del>', 'videos', '=', 'set', 're', 'findall', 'r', '<', 'a', 'href', '=', 'http', '/', '/', 'www', 'iqiyi', 'com', '/', 'v', '^', '+', 'video', 'page', '<add>', 'videos', '=', 'set', 're', 'findall', 'r', '<', 'a', 'href', '=', '?', '=', 'https', '?', '?', '/', '/', 'www', 'iqiyi', 'com', '/', 'v', '^', '+', 'video', 'page', 'for', 'video', 'in', 'videos', '<del>', 'self', 'class', 'download', 'by', 'url', 'video', '*', '*', 'kwargs', '<add>', 'self', 'class', 'download', 'by', 'url', 'https', '+', 'video', '*', '*', 'kwargs', 'def', 'prepare', 'self', '*', '*', 'kwargs', 'assert', 'self', 'url', 'or', 'self', 'vid', 'class', 'Iqiyi', 'VideoExtractor', 'except', 'Exception', 'as', 'e', 'log', 'i', 'vd', 'is', 'not', 'handled', 'format', 'stream', 'vd', 'log', 'i', 'info', 'is', 'format', 'stream', '<del>', '<add>', 'def', 'download', 'self', '*', '*', 'kwargs', 'Override', 'the', 'original', 'one', 'class', 'Iqiyi', 'VideoExtractor', 'if', 'not', 'urls', 'log', 'wtf', 'Failed', 'Cannot', 'extract', 'video', 'source', '#', 'For', 'legacy', 'main', '<del>', '<add>', '#', 'Here', 's', 'the', 'change', '!', '!', 'download', 'url', 'ffmpeg', 'urls', '<number>', 'self', 'title', 'mp4', 'output', 'dir', '=', 'kwargs', 'output', 'dir', 'merge', '=', 'kwargs', 'merge', 'stream', '=', 'False', 'class', 'Iqiyi', 'VideoExtractor', 'with', 'open', 'os', 'path', 'join', 'kwargs', 'output', 'dir', 'filename', 'w', 'encoding', '=', 'utf', '-', '<number>', 'as', 'x', 'x', 'write', 'srt', '<del>', 'print', 'Done', '<add>', 'print', 'Done', 'if', 'info', 'code', '!=', 'A0', '<number>']\n",
            "\n",
            "['fix', 'acfun', 'download', 'fail']\n",
            "@@ -111,6 +111,18 @@ def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=Fals\n",
            " def acfun_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "     assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', url)\n",
            " \n",
            "+    def getM3u8UrlFromCurrentVideoInfo(currentVideoInfo):\n",
            "+        if 'playInfos' in currentVideoInfo:\n",
            "+            return currentVideoInfo['playInfos'][0]['playUrls'][0]\n",
            "+        elif 'ksPlayJson' in currentVideoInfo:\n",
            "+            ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "+            representation = ksPlayJson.get('adaptationSet')[0].get('representation')\n",
            "+            reps = []\n",
            "+            for one in representation:\n",
            "+                reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "+            return max(reps)[1]\n",
            "+\n",
            "+\n",
            "     if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', url):\n",
            "         html = get_content(url, headers=fake_headers)\n",
            "         json_text = match1(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "@@ -122,37 +134,18 @@ def acfun_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "         if len(video_list) > 1:\n",
            "             title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][0]\n",
            "         currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "-        if 'playInfos' in currentVideoInfo:\n",
            "-            m3u8_url = currentVideoInfo['playInfos'][0]['playUrls'][0]\n",
            "-        elif 'ksPlayJson' in currentVideoInfo:\n",
            "-            ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "-            representation = ksPlayJson.get('adaptationSet').get('representation')\n",
            "-            reps = []\n",
            "-            for one in representation:\n",
            "-                reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "-            m3u8_url = max(reps)[1]\n",
            "-\n",
            "+        m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "     elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", url):\n",
            "         html = get_content(url, headers=fake_headers)\n",
            "-        tag_script = match1(html, r'<script>window\\.pageInfo([^<]+)</script>')\n",
            "+        tag_script = match1(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "         json_text = tag_script[tag_script.find('{') : tag_script.find('};') + 1]\n",
            "         json_data = json.loads(json_text)\n",
            "         title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "         vid = str(json_data['videoId'])\n",
            "         up = \"acfun\"\n",
            " \n",
            "-        play_info = get_content(\"https://www.acfun.cn/rest/pc-direct/play/playInfo/m3u8Auto?videoId=\" + vid, headers=fake_headers)\n",
            "-        play_url = json.loads(play_info)['playInfo']['streams'][0]['playUrls'][0]\n",
            "-        m3u8_all_qualities_file = get_content(play_url)\n",
            "-        m3u8_all_qualities_lines = m3u8_all_qualities_file.split('#EXT-X-STREAM-INF:')[1:]\n",
            "-        highest_quality_line = m3u8_all_qualities_lines[0]\n",
            "-        for line in m3u8_all_qualities_lines:\n",
            "-            bandwith = int(match1(line, r'BANDWIDTH=(\\d+)'))\n",
            "-            if bandwith > int(match1(highest_quality_line, r'BANDWIDTH=(\\d+)')):\n",
            "-                highest_quality_line = line\n",
            "-        #TODO: 应由用户指定清晰度\n",
            "-        m3u8_url = match1(highest_quality_line, r'\\n([^#\\n]+)$')\n",
            "-        m3u8_url = play_url[:play_url.rfind(\"/\")+1] + m3u8_url\n",
            "+        currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "+        m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            " \n",
            "     else:\n",
            "         raise NotImplemented\n",
            "\n",
            "\n",
            " def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=Fals\n",
            " def acfun_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "     assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', url)\n",
            " \n",
            "<add>    def getM3u8UrlFromCurrentVideoInfo(currentVideoInfo):\n",
            "<add>        if 'playInfos' in currentVideoInfo:\n",
            "<add>            return currentVideoInfo['playInfos'][<number>]['playUrls'][<number>]\n",
            "<add>        elif 'ksPlayJson' in currentVideoInfo:\n",
            "<add>            ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "<add>            representation = ksPlayJson.get('adaptationSet')[<number>].get('representation')\n",
            "<add>            reps = []\n",
            "<add>            for one in representation:\n",
            "<add>                reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "<add>            return max(reps)[<number>]\n",
            "<add>\n",
            "<add>\n",
            "     if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', url):\n",
            "         html = get_content(url, headers=fake_headers)\n",
            "         json_text = match1(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "\n",
            " def acfun_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "         if len(video_list) > <number>:\n",
            "             title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][<number>]\n",
            "         currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<del>        if 'playInfos' in currentVideoInfo:\n",
            "<del>            m3u8_url = currentVideoInfo['playInfos'][<number>]['playUrls'][<number>]\n",
            "<del>        elif 'ksPlayJson' in currentVideoInfo:\n",
            "<del>            ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "<del>            representation = ksPlayJson.get('adaptationSet').get('representation')\n",
            "<del>            reps = []\n",
            "<del>            for one in representation:\n",
            "<del>                reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "<del>            m3u8_url = max(reps)[<number>]\n",
            "<del>\n",
            "<add>        m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "     elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", url):\n",
            "         html = get_content(url, headers=fake_headers)\n",
            "<del>        tag_script = match1(html, r'<script>window\\.pageInfo([^<]+)</script>')\n",
            "<add>        tag_script = match1(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "         json_text = tag_script[tag_script.find('{') : tag_script.find('};') + <number>]\n",
            "         json_data = json.loads(json_text)\n",
            "         title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "         vid = str(json_data['videoId'])\n",
            "         up = \"acfun\"\n",
            " \n",
            "<del>        play_info = get_content(\"https://www.acfun.cn/rest/pc-direct/play/playInfo/m3u8Auto?videoId=\" + vid, headers=fake_headers)\n",
            "<del>        play_url = json.loads(play_info)['playInfo']['streams'][<number>]['playUrls'][<number>]\n",
            "<del>        m3u8_all_qualities_file = get_content(play_url)\n",
            "<del>        m3u8_all_qualities_lines = m3u8_all_qualities_file.split('#EXT-X-STREAM-INF:')[<number>:]\n",
            "<del>        highest_quality_line = m3u8_all_qualities_lines[<number>]\n",
            "<del>        for line in m3u8_all_qualities_lines:\n",
            "<del>            bandwith = int(match1(line, r'BANDWIDTH=(\\d+)'))\n",
            "<del>            if bandwith > int(match1(highest_quality_line, r'BANDWIDTH=(\\d+)')):\n",
            "<del>                highest_quality_line = line\n",
            "<del>        #TODO: 应由用户指定清晰度\n",
            "<del>        m3u8_url = match1(highest_quality_line, r'\\n([^#\\n]+)$')\n",
            "<del>        m3u8_url = play_url[:play_url.rfind(\"/\")+<number>] + m3u8_url\n",
            "<add>        currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<add>        m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            " \n",
            "     else:\n",
            "         raise NotImplemented\n",
            "\n",
            "['def', 'acfun', 'download', 'by', 'vid', 'vid', 'title', 'output', 'dir', '=', 'merge', '=', 'True', 'info', 'only', '=', 'Fals', 'def', 'acfun', 'download', 'url', 'output', 'dir', '=', 'merge', '=', 'True', 'info', 'only', '=', 'False', '*', '*', 'kwargs', 'assert', 're', 'match', 'r', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'D', '|', 'bangumi', '/', 'D', 'D', 'd', '+', 'url', '<add>', 'def', 'getM3u8UrlFromCurrentVideoInfo', 'currentVideoInfo', '<add>', 'if', 'playInfos', 'in', 'currentVideoInfo', '<add>', 'return', 'currentVideoInfo', 'playInfos', '<number>', 'playUrls', '<number>', '<add>', 'elif', 'ksPlayJson', 'in', 'currentVideoInfo', '<add>', 'ksPlayJson', '=', 'json', 'loads', 'currentVideoInfo', 'ksPlayJson', '<add>', 'representation', '=', 'ksPlayJson', 'get', 'adaptationSet', '<number>', 'get', 'representation', '<add>', 'reps', '=', '<add>', 'for', 'one', 'in', 'representation', '<add>', 'reps', 'append', 'one', 'width', '*', 'one', 'height', 'one', 'url', 'one', 'backupUrl', '<add>', 'return', 'max', 'reps', '<number>', '<add>', '<add>', 'if', 're', 'match', 'r', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'D', '/', 'D', 'D', 'd', '+', 'url', 'html', '=', 'get', 'content', 'url', 'headers', '=', 'fake', 'headers', 'json', 'text', '=', 'match1', 'html', 'r', '?', 's', 'videoInfo', 's', '*=', 's', '*', '*', '?', 'def', 'acfun', 'download', 'url', 'output', 'dir', '=', 'merge', '=', 'True', 'info', 'only', '=', 'False', '*', '*', 'kwargs', 'if', 'len', 'video', 'list', '>', '<number>', 'title', '+=', '-', '+', 'p', 'get', 'title', 'for', 'p', 'in', 'video', 'list', 'if', 'p', 'get', 'id', '==', 'vid', '<number>', 'currentVideoInfo', '=', 'json', 'data', 'get', 'currentVideoInfo', '<del>', 'if', 'playInfos', 'in', 'currentVideoInfo', '<del>', 'm3u8', 'url', '=', 'currentVideoInfo', 'playInfos', '<number>', 'playUrls', '<number>', '<del>', 'elif', 'ksPlayJson', 'in', 'currentVideoInfo', '<del>', 'ksPlayJson', '=', 'json', 'loads', 'currentVideoInfo', 'ksPlayJson', '<del>', 'representation', '=', 'ksPlayJson', 'get', 'adaptationSet', 'get', 'representation', '<del>', 'reps', '=', '<del>', 'for', 'one', 'in', 'representation', '<del>', 'reps', 'append', 'one', 'width', '*', 'one', 'height', 'one', 'url', 'one', 'backupUrl', '<del>', 'm3u8', 'url', '=', 'max', 'reps', '<number>', '<del>', '<add>', 'm3u8', 'url', '=', 'getM3u8UrlFromCurrentVideoInfo', 'currentVideoInfo', 'elif', 're', 'match', 'https', '?', '/', '/', '^', '*', '*', 'acfun', '^', '+', '/', 'bangumi', '/', 'aa', 'd', '+', 'url', 'html', '=', 'get', 'content', 'url', 'headers', '=', 'fake', 'headers', '<del>', 'tag', 'script', '=', 'match1', 'html', 'r', '<', 'script', '>', 'window', 'pageInfo', '^', '<', '+', '<', '/', 'script', '>', '<add>', 'tag', 'script', '=', 'match1', 'html', 'r', '<', 'script', '>', 's', '*', 'window', 'pageInfo', '^', '<', '+', '<', '/', 'script', '>', 'json', 'text', '=', 'tag', 'script', 'tag', 'script', 'find', 'tag', 'script', 'find', '+', '<number>', 'json', 'data', '=', 'json', 'loads', 'json', 'text', 'title', '=', 'json', 'data', 'bangumiTitle', '+', '+', 'json', 'data', 'episodeName', '+', '+', 'json', 'data', 'title', 'vid', '=', 'str', 'json', 'data', 'videoId', 'up', '=', 'acfun', '<del>', 'play', 'info', '=', 'get', 'content', 'https', '/', '/', 'www', 'acfun', 'cn', '/', 'rest', '/', 'pc', '-', 'direct', '/', 'play', '/', 'playInfo', '/', 'm3u8Auto', '?', 'videoId', '=', '+', 'vid', 'headers', '=', 'fake', 'headers', '<del>', 'play', 'url', '=', 'json', 'loads', 'play', 'info', 'playInfo', 'streams', '<number>', 'playUrls', '<number>', '<del>', 'm3u8', 'all', 'qualities', 'file', '=', 'get', 'content', 'play', 'url', '<del>', 'm3u8', 'all', 'qualities', 'lines', '=', 'm3u8', 'all', 'qualities', 'file', 'split', '#', 'EXT', '-', 'X', '-', 'STREAM', '-', 'INF', '<number>', '<del>', 'highest', 'quality', 'line', '=', 'm3u8', 'all', 'qualities', 'lines', '<number>', '<del>', 'for', 'line', 'in', 'm3u8', 'all', 'qualities', 'lines', '<del>', 'bandwith', '=', 'int', 'match1', 'line', 'r', 'BANDWIDTH', '=', 'd', '+', '<del>', 'if', 'bandwith', '>', 'int', 'match1', 'highest', 'quality', 'line', 'r', 'BANDWIDTH', '=', 'd', '+', '<del>', 'highest', 'quality', 'line', '=', 'line', '<del>', '#', 'TODO', '<del>', 'm3u8', 'url', '=', 'match1', 'highest', 'quality', 'line', 'r', 'n', '^', '#', 'n', '+', '$', '<del>', 'm3u8', 'url', '=', 'play', 'url', 'play', 'url', 'rfind', '/', '+', '<number>', '+', 'm3u8', 'url', '<add>', 'currentVideoInfo', '=', 'json', 'data', 'get', 'currentVideoInfo', '<add>', 'm3u8', 'url', '=', 'getM3u8UrlFromCurrentVideoInfo', 'currentVideoInfo', 'else', 'raise', 'NotImplemented']\n",
            "\n",
            "@@ -39,6 +39,7 @@ class YouGetTests(unittest.TestCase):\n",
            " \n",
            "     def test_acfun(self):\n",
            "         acfun.download('https://www.acfun.cn/v/ac11701912', info_only=True)\n",
            "+        acfun.download('https://www.acfun.cn/bangumi/aa6002986', info_only=True)\n",
            " \n",
            "     def test_bilibil(self):\n",
            "         bilibili.download(\n",
            "\n",
            "\n",
            " class YouGetTests(unittest.TestCase):\n",
            " \n",
            "     def test_acfun(self):\n",
            "         acfun.download('https://www.acfun.cn/v/ac1<number>', info_only=True)\n",
            "<add>        acfun.download('https://www.acfun.cn/bangumi/aa6<number>', info_only=True)\n",
            " \n",
            "     def test_bilibil(self):\n",
            "         bilibili.download(\n",
            "\n",
            "['class', 'YouGetTests', 'unittest', 'TestCase', 'def', 'test', 'acfun', 'self', 'acfun', 'download', 'https', '/', '/', 'www', 'acfun', 'cn', '/', 'v', '/', 'ac1', '<number>', 'info', 'only', '=', 'True', '<add>', 'acfun', 'download', 'https', '/', '/', 'www', 'acfun', 'cn', '/', 'bangumi', '/', 'aa6', '<number>', 'info', 'only', '=', 'True', 'def', 'test', 'bilibil', 'self', 'bilibili', 'download']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0f65faf9",
        "outputId": "61f12f24-accb-492b-f0df-04b1f113a9ac"
      },
      "source": [
        "import multiprocessing\n",
        "\n",
        "data = []\n",
        "\n",
        "def f(repo):\n",
        "    df = parse_repo_commits(repo)\n",
        "    df.to_pickle(f\"./repos/{repo.replace('/', '+')}.pkl\")\n",
        "    print(repo, \"Done\")\n",
        "    return df\n",
        "\n",
        "pool = multiprocessing.Pool()\n",
        "outputs = pool.map(f, repodf['repo'][:3])\n",
        "pd.concat(outputs).to_pickle(\"data.pkl\")"
      ],
      "id": "0f65faf9",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['add', 'hdr', 'support', 'for', 'bilibili']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <finalize object at 0x7f0ad01eb3a0; dead>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/weakref.py\", line 572, in __call__\n",
            "    return info.func(*info.args, **(info.kwargs or {}))\n",
            "  File \"/usr/lib/python3.7/tempfile.py\", line 936, in _cleanup\n",
            "    _rmtree(name)\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 485, in rmtree\n",
            "    onerror(os.lstat, path, sys.exc_info())\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 483, in rmtree\n",
            "    orig_st = os.lstat(path)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpsz94zojv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['add', 'fake', 'header']\n",
            "['add', 'format', 'selection', 'for', 'ac', 'fun']\n",
            "['fixed', 'tiktok', 'extraction']\n",
            "['fix', 'iqiyi', 'playlist', 'extrator']\n",
            "['fix', 'acfun', 'download', 'fail']\n",
            "['fix', 'resuming', 'when', 'downloading', 'in', 'chunked', 'mode']\n",
            "['fix', 'wrong', 'range', 'usage']\n",
            "['fix', 'bilibili', 'favlist', 'download']\n",
            "['add', 'support', 'for', 'bvid', 'in', 'playlist', 'mode', 'of', 'bilibili']\n",
            "['purge', 'dead', 'sites']\n",
            "['add', 'support', 'for', 'bvid', 'of', 'bilibili']\n",
            "['use', 'urllib', 'instead', 'of', 'requests']\n",
            "['fix', 'issue', 'on', 'itag']\n",
            "['modify', 'encoding', 'with', 'open', 'cookies', 'file']\n",
            "['fix', 'wrong', 'video', 'title', 'for', 'ixigua']\n",
            "['pick', 'best', 'video', 'quality', 'for', 'ixigua']\n",
            "['replace', 'broken', 'api', 'to', 'get', 'correct', 'video', 'title']\n",
            "['fix', 'toutiao', 'errors']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <finalize object at 0x7f0ad01eb3a0; dead>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/weakref.py\", line 572, in __call__\n",
            "    return info.func(*info.args, **(info.kwargs or {}))\n",
            "  File \"/usr/lib/python3.7/tempfile.py\", line 936, in _cleanup\n",
            "    _rmtree(name)\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 485, in rmtree\n",
            "    onerror(os.lstat, path, sys.exc_info())\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 483, in rmtree\n",
            "    orig_st = os.lstat(path)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpsz94zojv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['purge', 'dead', 'sites']\n",
            "['check', 'if', 'the', 'player', 'exist', 'or', 'not']\n",
            "['fix', 'ixigua', 'downloading', 'failure']\n",
            "['fix', 'acfun', 'flv', 'support']\n",
            "['remove', 'retry', 'for', 'testing', 'bilibili']\n",
            "['reduce', 'logging', 'message']\n",
            "['added', 'an', 'auto', 'rename', 'option', 'and', 'fixed', 'the', 'force', 'option']\n",
            "['fix', 'download', 'url', 'ffmpeg', 'extension']\n",
            "['update', 'the', 'test']\n",
            "['fix', 'miaopai', 'download', 'failed']\n",
            "['fix', 'bar', 'display', 'under', 'windows', 'terminal']\n",
            "['fix', 'load', 'cookies', 'local', 'name', 'error']\n",
            "['fix', 'wrong', 'local', 'name']\n",
            "['comment', 'the', 'wip', 'code', 'to', 'silent', 'lint']\n",
            "['use', 'argparse', 'instead', 'of', 'getopt']\n",
            "['fix', 'parsing', 'irregular', 'episode', 'index']\n",
            "['add', 'support', 'for', 'send', 'the', 'password', 'from', 'cli']\n",
            "['fix', 'apikey', 'matching', 'error', 'in', 'gallery', 'case']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Process ForkPoolWorker-1:\n",
            "Process ForkPoolWorker-2:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
            "    return list(map(*args))\n",
            "  File \"<ipython-input-15-010b44758e56>\", line 6, in f\n",
            "    df = parse_repo_commits(repo)\n",
            "  File \"<ipython-input-13-37e54b8452af>\", line 28, in parse_repo_commits\n",
            "    order='reverse'\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/repository.py\", line 233, in traverse_commits\n",
            "    for commit in job.result():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/repository.py\", line 240, in _iter_commits\n",
            "    if self._conf.is_commit_filtered(commit):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
            "    return list(map(*args))\n",
            "  File \"<ipython-input-15-010b44758e56>\", line 6, in f\n",
            "    df = parse_repo_commits(repo)\n",
            "  File \"<ipython-input-13-37e54b8452af>\", line 28, in parse_repo_commits\n",
            "    order='reverse'\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/repository.py\", line 233, in traverse_commits\n",
            "    for commit in job.result():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/repository.py\", line 240, in _iter_commits\n",
            "    if self._conf.is_commit_filtered(commit):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/utils/conf.py\", line 267, in is_commit_filtered\n",
            "    if not self._has_modification_with_file_type(commit):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/utils/conf.py\", line 267, in is_commit_filtered\n",
            "    if not self._has_modification_with_file_type(commit):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/utils/conf.py\", line 282, in _has_modification_with_file_type\n",
            "    for mod in commit.modified_files:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/domain/commit.py\", line 668, in modified_files\n",
            "    self._modifications = self._get_modifications()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/domain/commit.py\", line 684, in _get_modifications\n",
            "    self._c_object, create_patch=True, **options\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/git/diff.py\", line 175, in diff\n",
            "    index = diff_method(self.repo, proc)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/git/diff.py\", line 454, in _index_from_patch_format\n",
            "    handle_process_output(proc, text_list.append, None, finalize_process, decode_streams=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/git/cmd.py\", line 149, in handle_process_output\n",
            "    t.start()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 857, in start\n",
            "    self._started.wait()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/utils/conf.py\", line 282, in _has_modification_with_file_type\n",
            "    for mod in commit.modified_files:\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 552, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/domain/commit.py\", line 668, in modified_files\n",
            "    self._modifications = self._get_modifications()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pydriller/domain/commit.py\", line 684, in _get_modifications\n",
            "    self._c_object, create_patch=True, **options\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 296, in wait\n",
            "    waiter.acquire()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/git/diff.py\", line 175, in diff\n",
            "    index = diff_method(self.repo, proc)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/git/diff.py\", line 454, in _index_from_patch_format\n",
            "    handle_process_output(proc, text_list.append, None, finalize_process, decode_streams=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/git/cmd.py\", line 149, in handle_process_output\n",
            "    t.start()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 857, in start\n",
            "    self._started.wait()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 552, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 296, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-010b44758e56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepodf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'repo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04bfe004"
      },
      "source": [
        "df = pd.read_pickle(\"data.pkl\")\n",
        "df.head(3)"
      ],
      "id": "04bfe004",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbdfa1a5"
      },
      "source": [
        ""
      ],
      "id": "fbdfa1a5",
      "execution_count": null,
      "outputs": []
    }
  ]
}