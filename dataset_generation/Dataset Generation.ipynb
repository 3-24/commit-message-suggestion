{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "dataset_gen",
      "language": "python",
      "name": "dataset_gen"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "name": "Dataset Generation.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a5ade31"
      },
      "source": [
        "## 1. Collect CodeSearchNet Repositories"
      ],
      "id": "8a5ade31"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c06bb109"
      },
      "source": [
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "pd.set_option('max_colwidth',300)\n",
        "from pprint import pprint"
      ],
      "id": "c06bb109",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c50cf9b",
        "outputId": "ef4e0e49-ad5f-4ead-e31d-ae49d867d37a"
      },
      "source": [
        "!wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
        "!mkdir CodeSearchNet\n",
        "!unzip python.zip -d CodeSearchNet"
      ],
      "id": "4c50cf9b",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-01 16:33:31--  https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.141.54\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.141.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 940909997 (897M) [application/zip]\n",
            "Saving to: ‘python.zip.2’\n",
            "\n",
            "python.zip.2        100%[===================>] 897.32M  44.0MB/s    in 17s     \n",
            "\n",
            "2021-12-01 16:33:48 (54.2 MB/s) - ‘python.zip.2’ saved [940909997/940909997]\n",
            "\n",
            "mkdir: cannot create directory ‘CodeSearchNet’: File exists\n",
            "Archive:  python.zip\n",
            "replace CodeSearchNet/python/final/jsonl/train/python_train_9.jsonl.gz? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace CodeSearchNet/python/final/jsonl/train/python_train_12.jsonl.gz? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2847085c"
      },
      "source": [
        "python_files = sorted(Path('CodeSearchNet/python').glob('**/*.gz'))"
      ],
      "id": "2847085c",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e15382",
        "outputId": "96a0999b-c8ac-4fcc-b253-a02e6690314a"
      },
      "source": [
        "print(python_files)"
      ],
      "id": "a2e15382",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PosixPath('CodeSearchNet/python/final/jsonl/test/python_test_0.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_0.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_1.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_10.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_11.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_12.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_13.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_2.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_3.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_4.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_5.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_6.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_7.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_8.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/train/python_train_9.jsonl.gz'), PosixPath('CodeSearchNet/python/final/jsonl/valid/python_valid_0.jsonl.gz')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aa7dbe1"
      },
      "source": [
        "columns_long_list = ['repo', 'path', 'url', 'code', \n",
        "                     'code_tokens', 'docstring', 'docstring_tokens', \n",
        "                     'language', 'partition']\n",
        "\n",
        "def jsonl_list_to_dataframe(file_list, columns=columns_long_list):\n",
        "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
        "    return pd.concat([pd.read_json(f, \n",
        "                                   orient='records', \n",
        "                                   compression='gzip',\n",
        "                                   lines=True)[columns] \n",
        "                      for f in file_list], sort=False)"
      ],
      "id": "8aa7dbe1",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be6c1c77"
      },
      "source": [
        "columns_repo = ['repo']\n",
        "\n",
        "pydf = jsonl_list_to_dataframe(python_files, columns=columns_repo)"
      ],
      "id": "be6c1c77",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e19c0d36"
      },
      "source": [
        "pydf = pydf.drop_duplicates().reset_index(drop=True)"
      ],
      "id": "e19c0d36",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea71c7ad"
      },
      "source": [
        "print(pydf.shape)\n",
        "pydf.head(13590)"
      ],
      "id": "ea71c7ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b07e592d"
      },
      "source": [
        "pydf.to_pickle(\"repos.pkl\")"
      ],
      "id": "b07e592d",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d0559c4"
      },
      "source": [
        "## 2. Collect diff and commits"
      ],
      "id": "1d0559c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ee24339",
        "outputId": "32cb7aae-0f58-448a-da67-ac6d3a26abfd"
      },
      "source": [
        "!mkdir repos\n",
        "!pip install pydriller\n",
        "!pip install pandas\n",
        "!pip install spacy"
      ],
      "id": "8ee24339",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘repos’: File exists\n",
            "Requirement already satisfied: pydriller in /usr/local/lib/python3.7/dist-packages (2.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from pydriller) (2018.9)\n",
            "Requirement already satisfied: lizard in /usr/local/lib/python3.7/dist-packages (from pydriller) (1.17.9)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from pydriller) (3.1.24)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->pydriller) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->pydriller) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->pydriller) (5.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.10.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72e76292"
      },
      "source": [
        "from pydriller import *\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "from functools import reduce\n",
        "# spacy.cli.download(\"en_core_web_sm\")"
      ],
      "id": "72e76292",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "845d5eb7",
        "outputId": "86578a62-fea4-4bf1-a0b3-dc98a311dff1"
      },
      "source": [
        "repodf = pd.read_pickle(\"repos.pkl\")\n",
        "print(repodf.shape)\n",
        "spacy_tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "diff_tokenizer = nltk.tokenize.WordPunctTokenizer()"
      ],
      "id": "845d5eb7",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13590, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "449680fc"
      },
      "source": [
        "import re\n",
        "def basic_filter(message):\n",
        "    return message.split(\"\\n\", 1)[0].strip()\n",
        "\n",
        "# Remove [label] in front of commit if exists\n",
        "def label_filter(message):\n",
        "    if (message.startswith('[')):\n",
        "        end_bracket_index = message.find(']')\n",
        "        if (end_bracket_index == -1):\n",
        "            return None\n",
        "        return message[:end_bracket_index+1]\n",
        "    return message\n",
        "\n",
        "def camel_case_split(str):\n",
        "    return re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', str)\n",
        "\n",
        "\n",
        "def case_splitter(token):\n",
        "    return list(map(lambda x: x.lower(), camel_case_split(token[0].upper() + token[1:])))\n",
        "    \n",
        "\n",
        "def parse_repo_commits(repo_name, commit_limit=50):\n",
        "    data = []\n",
        "    commit_count = 0\n",
        "    for commit in Repository(\n",
        "        f\"https://github.com/{repo_name}\",\n",
        "        only_modifications_with_file_types=[\".py\"],\n",
        "        only_no_merge=True,\n",
        "        order='reverse'\n",
        "    ).traverse_commits():\n",
        "        if (commit_count >= commit_limit): break\n",
        "        line = basic_filter(commit.msg)\n",
        "        line = label_filter(line)\n",
        "        \n",
        "        if (line is None):\n",
        "            print(f\"[DEBUG] Label filter return None for repo {repo} and hash {commit.hash}\")\n",
        "        \n",
        "        line = line.replace('_', ' ')\n",
        "        \n",
        "        # Only alphabet and blank characters\n",
        "        if (not line.isascii() or not all([c.isalpha() or c.isspace() for c in line])):\n",
        "            continue\n",
        "        \n",
        "        '''\n",
        "        if (not line.isascii()):                     # Ignore non-English\n",
        "            continue\n",
        "        \n",
        "        if ('@' in line and not 'decorat' in line):  # Ignore Github mentions\n",
        "            continue\n",
        "        \n",
        "        if ('#' in line):                            # Ignore Github issue\n",
        "            continue\n",
        "        '''\n",
        "        \n",
        "        tokens = spacy_tokenizer(line)\n",
        "        \n",
        "        # VERB filter\n",
        "        if (tokens[0].pos_ != 'VERB'):\n",
        "            continue\n",
        "        \n",
        "        tokens = reduce(lambda a,b: a+b, map(case_splitter, [token.text for token in tokens]), [])\n",
        "        \n",
        "        if (len(tokens) < 3 or len(tokens) > 30):\n",
        "            continue\n",
        "        \n",
        "        # Check if changed files are python\n",
        "        file_failed = False\n",
        "        \n",
        "        for mf in commit.modified_files:\n",
        "            if (not mf.filename.endswith(\".py\")):\n",
        "                file_failed = True\n",
        "                break\n",
        "        \n",
        "        if (file_failed):\n",
        "            continue\n",
        "        def diff_processing(mf):\n",
        "            print(mf.diff)\n",
        "            diff = '\\n'.join(map(lambda x: x[1], filter(lambda y: y[0] % 2 == 0, enumerate(mf.diff.split(\"@@\")))))\n",
        "            diff = diff.replace('\\n+', '\\n<add>').replace('\\n-', '\\n<del>')\n",
        "            #TODO\n",
        "            #replace_symbol_in_string = ex)url\n",
        "            replace_number = re.compile(r\"\"\"\n",
        "            (?:\n",
        "              0x[0-9A-Fa-f]+        #hexadecimal number\n",
        "              |[0-9]+               #decimal number\n",
        "            )\n",
        "            #(?:\\.[0-9]+)?           #fraction(optional)\n",
        "            \"\"\",re.VERBOSE)\n",
        "            diff = replace_number.sub(\"<number>\",diff)\n",
        "            diff = re.sub(r\"(?:\\n[ \\t\\r\\f\\v]*)+\",\"\\n\",diff) #Join continuous row change\n",
        "            print(diff)\n",
        "            token_regex = r\"\"\"(?x)\n",
        "             <(?:add|del|number)>   #Filtered eariler\n",
        "            #|[-+*/^&~|=%!]=?        #Symbols which can join with equal\n",
        "            #|[<>]{1,2}              #neq and bit shift symbols\n",
        "            #|#+                     #Comment symbol\n",
        "            #|[@?$]                  #Other symbols\n",
        "            |(?:[#][\\s]*)+           #Quotation\n",
        "            |(?:[-+*/^&~|=%!<>@?$][\\s]*)+     #Sequence of symbols\n",
        "            |[\\n]                    #Change row\n",
        "            |[a-zA-Z]+               #General text\n",
        "            \"\"\"\n",
        "            #'\"`,.;:()[]{}_ not included\n",
        "            token_initial = nltk.tokenize.regexp_tokenize(diff,token_regex)\n",
        "            token_camel_case_split = reduce(lambda a,b:a+b,map(lambda a: case_splitter(a) if 97<=ord(a[0].lower())<122 else [a], token_initial),[])\n",
        "            print(token_camel_case_split)\n",
        "            input()\n",
        "            return diff\n",
        "        \n",
        "        print(tokens)\n",
        "        diff = ''.join(map(diff_processing, commit.modified_files))\n",
        "        \n",
        "        data.append([repo_name, commit.hash, ' '.join(tokens), diff])\n",
        "        commit_count += 1\n",
        "\n",
        "    return pd.DataFrame(data, columns=[\"repo\", \"hash\", \"commit_messsage\", \"diff\"])"
      ],
      "id": "449680fc",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b2854419",
        "outputId": "0a84ae42-57e7-4c1e-8c91-9a59e14c6071"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "df = parse_repo_commits(\"soimort/you-get\")\n",
        "end = time.time()\n",
        "print(end - start)\n",
        "df.head(3)"
      ],
      "id": "b2854419",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['add', 'hdr', 'support', 'for', 'bilibili']\n",
            "@@ -12,6 +12,8 @@ class Bilibili(VideoExtractor):\n",
            " \n",
            "     # Bilibili media encoding options, in descending quality order.\n",
            "     stream_types = [\n",
            "+        {'id': 'hdflv2', 'quality': 125, 'audio_quality': 30280,\n",
            "+         'container': 'FLV', 'video_resolution': '3840p', 'desc': '真彩 HDR'},\n",
            "         {'id': 'hdflv2_4k', 'quality': 120, 'audio_quality': 30280,\n",
            "          'container': 'FLV', 'video_resolution': '2160p', 'desc': '超清 4K'},\n",
            "         {'id': 'flv_p60', 'quality': 116, 'audio_quality': 30280,\n",
            "\n",
            "\n",
            "class Bilibili(VideoExtractor):\n",
            "# Bilibili media encoding options, in descending quality order.\n",
            "stream_types = [\n",
            "<add>        {'id': 'hdflv<number>', 'quality': <number>, 'audio_quality': <number>,\n",
            "<add>         'container': 'FLV', 'video_resolution': '<number>p', 'desc': '真彩 HDR'},\n",
            "{'id': 'hdflv<number>_<number>k', 'quality': <number>, 'audio_quality': <number>,\n",
            "'container': 'FLV', 'video_resolution': '<number>p', 'desc': '超清 <number>K'},\n",
            "{'id': 'flv_p<number>', 'quality': <number>, 'audio_quality': <number>,\n",
            "\n",
            "['\\n', 'class', 'bilibili', 'video', 'extractor', '\\n', '# ', 'bilibili', 'media', 'encoding', 'options', 'in', 'descending', 'quality', 'order', '\\n', 'stream', 'types', '= ', '\\n', '<add>', 'id', 'hdflv', '<number>', 'quality', '<number>', 'audio', 'quality', '<number>', '\\n', '<add>', 'container', 'flv', 'video', 'resolution', '<number>', 'p', 'desc', 'hdr', '\\n', 'id', 'hdflv', '<number>', '<number>', 'k', 'quality', '<number>', 'audio', 'quality', '<number>', '\\n', 'container', 'flv', 'video', 'resolution', '<number>', 'p', 'desc', '<number>', 'k', '\\n', 'id', 'flv', 'p', '<number>', 'quality', '<number>', 'audio', 'quality', '<number>', '\\n']\n",
            "\n",
            "['add', 'fake', 'header']\n",
            "@@ -123,10 +123,10 @@ def netease_song_download(song, output_dir='.', info_only=False, playlist_prefix\n",
            "                             output_dir=output_dir, info_only=info_only)\n",
            " \n",
            " def netease_download_common(title, url_best, output_dir, info_only):\n",
            "-    songtype, ext, size = url_info(url_best)\n",
            "+    songtype, ext, size = url_info(url_best, faker=True)\n",
            "     print_info(site_info, title, songtype, size)\n",
            "     if not info_only:\n",
            "-        download_urls([url_best], title, ext, size, output_dir)\n",
            "+        download_urls([url_best], title, ext, size, output_dir, faker=True)\n",
            " \n",
            " \n",
            " def netease_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\n",
            "\n",
            "\n",
            "def netease_song_download(song, output_dir='.', info_only=False, playlist_prefix\n",
            "output_dir=output_dir, info_only=info_only)\n",
            "def netease_download_common(title, url_best, output_dir, info_only):\n",
            "<del>    songtype, ext, size = url_info(url_best)\n",
            "<add>    songtype, ext, size = url_info(url_best, faker=True)\n",
            "print_info(site_info, title, songtype, size)\n",
            "if not info_only:\n",
            "<del>        download_urls([url_best], title, ext, size, output_dir)\n",
            "<add>        download_urls([url_best], title, ext, size, output_dir, faker=True)\n",
            "def netease_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):\n",
            "\n",
            "['\\n', 'def', 'netease', 'song', 'download', 'song', 'output', 'dir', '=', 'info', 'only', '=', 'false', 'playlist', 'prefix', '\\n', 'output', 'dir', '=', 'output', 'dir', 'info', 'only', '=', 'info', 'only', '\\n', 'def', 'netease', 'download', 'common', 'title', 'url', 'best', 'output', 'dir', 'info', 'only', '\\n', '<del>', 'songtype', 'ext', 'size', '= ', 'url', 'info', 'url', 'best', '\\n', '<add>', 'songtype', 'ext', 'size', '= ', 'url', 'info', 'url', 'best', 'faker', '=', 'true', '\\n', 'print', 'info', 'site', 'info', 'title', 'songtype', 'size', '\\n', 'if', 'not', 'info', 'only', '\\n', '<del>', 'download', 'urls', 'url', 'best', 'title', 'ext', 'size', 'output', 'dir', '\\n', '<add>', 'download', 'urls', 'url', 'best', 'title', 'ext', 'size', 'output', 'dir', 'faker', '=', 'true', '\\n', 'def', 'netease', 'download', 'url', 'output', 'dir', '= ', 'merge', '= ', 'true', 'info', 'only', '= ', 'false', '**', 'kwargs', '\\n']\n",
            "\n",
            "['add', 'format', 'selection', 'for', 'ac', 'fun']\n",
            "@@ -1,168 +1,213 @@\n",
            " #!/usr/bin/env python\n",
            " \n",
            "-__all__ = ['acfun_download']\n",
            "-\n",
            " from ..common import *\n",
            "+from ..extractor import VideoExtractor\n",
            "+\n",
            "+class AcFun(VideoExtractor):\n",
            "+    name = \"AcFun\"\n",
            "+\n",
            "+    stream_types = [\n",
            "+        {'id': '2160P', 'qualityType': '2160p'},\n",
            "+        {'id': '1080P60', 'qualityType': '1080p60'},\n",
            "+        {'id': '720P60', 'qualityType': '720p60'},\n",
            "+        {'id': '1080P+', 'qualityType': '1080p+'},\n",
            "+        {'id': '1080P', 'qualityType': '1080p'},\n",
            "+        {'id': '720P', 'qualityType': '720p'},\n",
            "+        {'id': '540P', 'qualityType': '540p'},\n",
            "+        {'id': '360P', 'qualityType': '360p'}\n",
            "+    ]    \n",
            "+\n",
            "+    def prepare(self, **kwargs):\n",
            "+        assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', self.url)\n",
            "+\n",
            "+        if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', self.url):\n",
            "+            html = get_content(self.url, headers=fake_headers)\n",
            "+            json_text = match1(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "+            json_data = json.loads(json_text)\n",
            "+            vid = json_data.get('currentVideoInfo').get('id')\n",
            "+            up = json_data.get('user').get('name')\n",
            "+            self.title = json_data.get('title')\n",
            "+            video_list = json_data.get('videoList')\n",
            "+            if len(video_list) > 1:\n",
            "+                self.title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][0]\n",
            "+            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "+\n",
            "+        elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", self.url):\n",
            "+            html = get_content(self.url, headers=fake_headers)\n",
            "+            tag_script = match1(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "+            json_text = tag_script[tag_script.find('{') : tag_script.find('};') + 1]\n",
            "+            json_data = json.loads(json_text)\n",
            "+            self.title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "+            vid = str(json_data['videoId'])\n",
            "+            up = \"acfun\"\n",
            "+            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            " \n",
            "-from .le import letvcloud_download_by_vu\n",
            "-from .qq import qq_download_by_vid\n",
            "-from .sina import sina_download_by_vid\n",
            "-from .tudou import tudou_download_by_iid\n",
            "-from .youku import youku_download_by_vid\n",
            "-\n",
            "-import json\n",
            "-import re\n",
            "-import base64\n",
            "-import time\n",
            "-\n",
            "-def get_srt_json(id):\n",
            "-    url = 'http://danmu.aixifan.com/V2/%s' % id\n",
            "-    return get_content(url)\n",
            "-\n",
            "-def youku_acfun_proxy(vid, sign, ref):\n",
            "-    endpoint = 'http://player.acfun.cn/flash_data?vid={}&ct=85&ev=3&sign={}&time={}'\n",
            "-    url = endpoint.format(vid, sign, str(int(time.time() * 1000)))\n",
            "-    json_data = json.loads(get_content(url, headers=dict(referer=ref)))['data']\n",
            "-    enc_text = base64.b64decode(json_data)\n",
            "-    dec_text = rc4(b'8bdc7e1a', enc_text).decode('utf8')\n",
            "-    youku_json = json.loads(dec_text)\n",
            "-\n",
            "-    yk_streams = {}\n",
            "-    for stream in youku_json['stream']:\n",
            "-        tp = stream['stream_type']\n",
            "-        yk_streams[tp] = [], stream['total_size']\n",
            "-        if stream.get('segs'):\n",
            "-            for seg in stream['segs']:\n",
            "-                yk_streams[tp][0].append(seg['url'])\n",
            "-        else:\n",
            "-            yk_streams[tp] = stream['m3u8'], stream['total_size']\n",
            "-\n",
            "-    return yk_streams\n",
            "-\n",
            "-def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "-    \"\"\"str, str, str, bool, bool ->None\n",
            "-\n",
            "-    Download Acfun video by vid.\n",
            "-\n",
            "-    Call Acfun API, decide which site to use, and pass the job to its\n",
            "-    extractor.\n",
            "-    \"\"\"\n",
            "-\n",
            "-    #first call the main parasing API\n",
            "-    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid, headers=fake_headers))\n",
            "-\n",
            "-    sourceType = info['sourceType']\n",
            "-\n",
            "-    #decide sourceId to know which extractor to use\n",
            "-    if 'sourceId' in info: sourceId = info['sourceId']\n",
            "-    # danmakuId = info['danmakuId']\n",
            "-\n",
            "-    #call extractor decided by sourceId\n",
            "-    if sourceType == 'sina':\n",
            "-        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "-    elif sourceType == 'youku':\n",
            "-        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n",
            "-    elif sourceType == 'tudou':\n",
            "-        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "-    elif sourceType == 'qq':\n",
            "-        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "-    elif sourceType == 'letv':\n",
            "-        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "-    elif sourceType == 'zhuzhan':\n",
            "-        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this\n",
            "-#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player\n",
            "-#old code removed\n",
            "-        url = 'http://www.acfun.cn/v/ac' + vid\n",
            "-        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n",
            "-        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n",
            "-        for t in seq:\n",
            "-            if yk_streams.get(t):\n",
            "-                preferred = yk_streams[t]\n",
            "-                break\n",
            "-#total_size in the json could be incorrect(F.I. 0)\n",
            "-        size = 0\n",
            "-        for url in preferred[0]:\n",
            "-            _, _, seg_size = url_info(url)\n",
            "-            size += seg_size\n",
            "-#fallback to flvhd is not quite possible\n",
            "-        if re.search(r'fid=[0-9A-Z\\-]*.flv', preferred[0][0]):\n",
            "-            ext = 'flv'\n",
            "         else:\n",
            "-            ext = 'mp4'\n",
            "-        print_info(site_info, title, ext, size)\n",
            "-        if not info_only:\n",
            "-            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)\n",
            "-    else:\n",
            "-        raise NotImplementedError(sourceType)\n",
            "-\n",
            "-    if not info_only and not dry_run:\n",
            "-        if not kwargs['caption']:\n",
            "-            print('Skipping danmaku.')\n",
            "-            return\n",
            "-        try:\n",
            "-            title = get_filename(title)\n",
            "-            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n",
            "-            cmt = get_srt_json(vid)\n",
            "-            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:\n",
            "-                x.write(cmt)\n",
            "-        except:\n",
            "-            pass\n",
            "-\n",
            "-def acfun_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "-    assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', url)\n",
            "-\n",
            "-    def getM3u8UrlFromCurrentVideoInfo(currentVideoInfo):\n",
            "-        if 'playInfos' in currentVideoInfo:\n",
            "-            return currentVideoInfo['playInfos'][0]['playUrls'][0]\n",
            "-        elif 'ksPlayJson' in currentVideoInfo:\n",
            "-            ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "+            raise NotImplemented            \n",
            "+\n",
            "+        if 'ksPlayJson' in currentVideoInfo:\n",
            "+            durationMillis = currentVideoInfo['durationMillis']\n",
            "+            ksPlayJson = ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "             representation = ksPlayJson.get('adaptationSet')[0].get('representation')\n",
            "-            reps = []\n",
            "-            for one in representation:\n",
            "-                reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "-            return max(reps)[1]\n",
            "-\n",
            "-\n",
            "-    if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', url):\n",
            "-        html = get_content(url, headers=fake_headers)\n",
            "-        json_text = match1(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "-        json_data = json.loads(json_text)\n",
            "-        vid = json_data.get('currentVideoInfo').get('id')\n",
            "-        up = json_data.get('user').get('name')\n",
            "-        title = json_data.get('title')\n",
            "-        video_list = json_data.get('videoList')\n",
            "-        if len(video_list) > 1:\n",
            "-            title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][0]\n",
            "-        currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "-        m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "-    elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", url):\n",
            "-        html = get_content(url, headers=fake_headers)\n",
            "-        tag_script = match1(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "-        json_text = tag_script[tag_script.find('{') : tag_script.find('};') + 1]\n",
            "-        json_data = json.loads(json_text)\n",
            "-        title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "-        vid = str(json_data['videoId'])\n",
            "-        up = \"acfun\"\n",
            "-\n",
            "-        currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "-        m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "-\n",
            "-    else:\n",
            "-        raise NotImplemented\n",
            "-\n",
            "-    assert title and m3u8_url\n",
            "-    title = unescape_html(title)\n",
            "-    title = escape_file_path(title)\n",
            "-    p_title = r1('active\">([^<]+)', html)\n",
            "-    title = '%s (%s)' % (title, up)\n",
            "-    if p_title:\n",
            "-        title = '%s - %s' % (title, p_title)\n",
            "-\n",
            "-    print_info(site_info, title, 'm3u8', float('inf'))\n",
            "-    if not info_only:\n",
            "-        download_url_ffmpeg(m3u8_url, title, 'mp4', output_dir=output_dir, merge=merge)\n",
            "+            stream_list = representation\n",
            "+\n",
            "+        for stream in stream_list:\n",
            "+            m3u8_url = stream[\"url\"]\n",
            "+            size = durationMillis * stream[\"avgBitrate\"] / 8\n",
            "+            # size = float('inf')\n",
            "+            container = 'mp4'\n",
            "+            stream_id = stream[\"qualityLabel\"]\n",
            "+            quality = stream[\"qualityType\"]\n",
            "+            \n",
            "+            stream_data = dict(src=m3u8_url, size=size, container=container, quality=quality)\n",
            "+            self.streams[stream_id] = stream_data\n",
            "+\n",
            "+        assert self.title and m3u8_url\n",
            "+        self.title = unescape_html(self.title)\n",
            "+        self.title = escape_file_path(self.title)\n",
            "+        p_title = r1('active\">([^<]+)', html)\n",
            "+        self.title = '%s (%s)' % (self.title, up)\n",
            "+        if p_title:\n",
            "+            self.title = '%s - %s' % (self.title, p_title)       \n",
            "+\n",
            "+\n",
            "+    def download(self, **kwargs):\n",
            "+        if 'json_output' in kwargs and kwargs['json_output']:\n",
            "+            json_output.output(self)\n",
            "+        elif 'info_only' in kwargs and kwargs['info_only']:\n",
            "+            if 'stream_id' in kwargs and kwargs['stream_id']:\n",
            "+                # Display the stream\n",
            "+                stream_id = kwargs['stream_id']\n",
            "+                if 'index' not in kwargs:\n",
            "+                    self.p(stream_id)\n",
            "+                else:\n",
            "+                    self.p_i(stream_id)\n",
            "+            else:\n",
            "+                # Display all available streams\n",
            "+                if 'index' not in kwargs:\n",
            "+                    self.p([])\n",
            "+                else:\n",
            "+                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n",
            "+                    self.p_i(stream_id)\n",
            "+\n",
            "+        else:\n",
            "+            if 'stream_id' in kwargs and kwargs['stream_id']:\n",
            "+                # Download the stream\n",
            "+                stream_id = kwargs['stream_id']\n",
            "+            else:\n",
            "+                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n",
            "+\n",
            "+            if 'index' not in kwargs:\n",
            "+                self.p(stream_id)\n",
            "+            else:\n",
            "+                self.p_i(stream_id)\n",
            "+            if stream_id in self.streams:\n",
            "+                url = self.streams[stream_id]['src']\n",
            "+                ext = self.streams[stream_id]['container']\n",
            "+                total_size = self.streams[stream_id]['size']\n",
            "+\n",
            "+\n",
            "+            if ext == 'm3u8' or ext == 'm4a':\n",
            "+                ext = 'mp4'\n",
            "+\n",
            "+            if not url:\n",
            "+                log.wtf('[Failed] Cannot extract video source.')\n",
            "+            # For legacy main()\n",
            "+            headers = {}\n",
            "+            if self.ua is not None:\n",
            "+                headers['User-Agent'] = self.ua\n",
            "+            if self.referer is not None:\n",
            "+                headers['Referer'] = self.referer\n",
            "+\n",
            "+            download_url_ffmpeg(url, self.title, ext, output_dir=kwargs['output_dir'], merge=kwargs['merge'])                           \n",
            "+\n",
            "+            if 'caption' not in kwargs or not kwargs['caption']:\n",
            "+                print('Skipping captions or danmaku.')\n",
            "+                return\n",
            "+\n",
            "+            for lang in self.caption_tracks:\n",
            "+                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n",
            "+                print('Saving %s ... ' % filename, end=\"\", flush=True)\n",
            "+                srt = self.caption_tracks[lang]\n",
            "+                with open(os.path.join(kwargs['output_dir'], filename),\n",
            "+                          'w', encoding='utf-8') as x:\n",
            "+                    x.write(srt)\n",
            "+                print('Done.')\n",
            "+\n",
            "+            if self.danmaku is not None and not dry_run:\n",
            "+                filename = '{}.cmt.xml'.format(get_filename(self.title))\n",
            "+                print('Downloading {} ...\\n'.format(filename))\n",
            "+                with open(os.path.join(kwargs['output_dir'], filename), 'w', encoding='utf8') as fp:\n",
            "+                    fp.write(self.danmaku)\n",
            "+\n",
            "+            if self.lyrics is not None and not dry_run:\n",
            "+                filename = '{}.lrc'.format(get_filename(self.title))\n",
            "+                print('Downloading {} ...\\n'.format(filename))\n",
            "+                with open(os.path.join(kwargs['output_dir'], filename), 'w', encoding='utf8') as fp:\n",
            "+                    fp.write(self.lyrics)\n",
            "+\n",
            "+            # For main_dev()\n",
            "+            #download_urls(urls, self.title, self.streams[stream_id]['container'], self.streams[stream_id]['size'])\n",
            "+        keep_obj = kwargs.get('keep_obj', False)\n",
            "+        if not keep_obj:\n",
            "+            self.__init__()\n",
            "+\n",
            "+\n",
            "+    def acfun_download(self, url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "+        assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', url)\n",
            "+\n",
            "+        def getM3u8UrlFromCurrentVideoInfo(currentVideoInfo):\n",
            "+            if 'playInfos' in currentVideoInfo:\n",
            "+                return currentVideoInfo['playInfos'][0]['playUrls'][0]\n",
            "+            elif 'ksPlayJson' in currentVideoInfo:\n",
            "+                ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "+                representation = ksPlayJson.get('adaptationSet')[0].get('representation')\n",
            "+                reps = []\n",
            "+                for one in representation:\n",
            "+                    reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "+                return max(reps)[1]\n",
            "+\n",
            "+\n",
            "+        if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', url):\n",
            "+            html = get_content(url, headers=fake_headers)\n",
            "+            json_text = match1(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "+            json_data = json.loads(json_text)\n",
            "+            vid = json_data.get('currentVideoInfo').get('id')\n",
            "+            up = json_data.get('user').get('name')\n",
            "+            title = json_data.get('title')\n",
            "+            video_list = json_data.get('videoList')\n",
            "+            if len(video_list) > 1:\n",
            "+                title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][0]\n",
            "+            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "+            m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "+        elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", url):\n",
            "+            html = get_content(url, headers=fake_headers)\n",
            "+            tag_script = match1(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "+            json_text = tag_script[tag_script.find('{') : tag_script.find('};') + 1]\n",
            "+            json_data = json.loads(json_text)\n",
            "+            title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "+            vid = str(json_data['videoId'])\n",
            "+            up = \"acfun\"\n",
            "+\n",
            "+            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "+            m3u8_url = getM3u8UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "+\n",
            "+        else:\n",
            "+            raise NotImplemented\n",
            " \n",
            "+        assert title and m3u8_url\n",
            "+        title = unescape_html(title)\n",
            "+        title = escape_file_path(title)\n",
            "+        p_title = r1('active\">([^<]+)', html)\n",
            "+        title = '%s (%s)' % (title, up)\n",
            "+        if p_title:\n",
            "+            title = '%s - %s' % (title, p_title)\n",
            "+\n",
            "+        print_info(site_info, title, 'm3u8', float('inf'))\n",
            "+        if not info_only:\n",
            "+            download_url_ffmpeg(m3u8_url, title, 'mp4', output_dir=output_dir, merge=merge)\n",
            " \n",
            "+site = AcFun()\n",
            " site_info = \"AcFun.cn\"\n",
            "-download = acfun_download\n",
            "+download = site.download_by_url\n",
            " download_playlist = playlist_not_supported('acfun')\n",
            "\n",
            "\n",
            "#!/usr/bin/env python\n",
            "<del>__all__ = ['acfun_download']\n",
            "<del>\n",
            "from ..common import *\n",
            "<add>from ..extractor import VideoExtractor\n",
            "<add>\n",
            "<add>class AcFun(VideoExtractor):\n",
            "<add>    name = \"AcFun\"\n",
            "<add>\n",
            "<add>    stream_types = [\n",
            "<add>        {'id': '<number>P', 'qualityType': '<number>p'},\n",
            "<add>        {'id': '<number>P<number>', 'qualityType': '<number>p<number>'},\n",
            "<add>        {'id': '<number>P<number>', 'qualityType': '<number>p<number>'},\n",
            "<add>        {'id': '<number>P+', 'qualityType': '<number>p+'},\n",
            "<add>        {'id': '<number>P', 'qualityType': '<number>p'},\n",
            "<add>        {'id': '<number>P', 'qualityType': '<number>p'},\n",
            "<add>        {'id': '<number>P', 'qualityType': '<number>p'},\n",
            "<add>        {'id': '<number>P', 'qualityType': '<number>p'}\n",
            "<add>    ]    \n",
            "<add>\n",
            "<add>    def prepare(self, **kwargs):\n",
            "<add>        assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', self.url)\n",
            "<add>\n",
            "<add>        if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', self.url):\n",
            "<add>            html = get_content(self.url, headers=fake_headers)\n",
            "<add>            json_text = match<number>(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "<add>            json_data = json.loads(json_text)\n",
            "<add>            vid = json_data.get('currentVideoInfo').get('id')\n",
            "<add>            up = json_data.get('user').get('name')\n",
            "<add>            self.title = json_data.get('title')\n",
            "<add>            video_list = json_data.get('videoList')\n",
            "<add>            if len(video_list) > <number>:\n",
            "<add>                self.title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][<number>]\n",
            "<add>            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<add>\n",
            "<add>        elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", self.url):\n",
            "<add>            html = get_content(self.url, headers=fake_headers)\n",
            "<add>            tag_script = match<number>(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "<add>            json_text = tag_script[tag_script.find('{') : tag_script.find('};') + <number>]\n",
            "<add>            json_data = json.loads(json_text)\n",
            "<add>            self.title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "<add>            vid = str(json_data['videoId'])\n",
            "<add>            up = \"acfun\"\n",
            "<add>            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<del>from .le import letvcloud_download_by_vu\n",
            "<del>from .qq import qq_download_by_vid\n",
            "<del>from .sina import sina_download_by_vid\n",
            "<del>from .tudou import tudou_download_by_iid\n",
            "<del>from .youku import youku_download_by_vid\n",
            "<del>\n",
            "<del>import json\n",
            "<del>import re\n",
            "<del>import base<number>\n",
            "<del>import time\n",
            "<del>\n",
            "<del>def get_srt_json(id):\n",
            "<del>    url = 'http://danmu.aixifan.com/V<number>/%s' % id\n",
            "<del>    return get_content(url)\n",
            "<del>\n",
            "<del>def youku_acfun_proxy(vid, sign, ref):\n",
            "<del>    endpoint = 'http://player.acfun.cn/flash_data?vid={}&ct=<number>&ev=<number>&sign={}&time={}'\n",
            "<del>    url = endpoint.format(vid, sign, str(int(time.time() * <number>)))\n",
            "<del>    json_data = json.loads(get_content(url, headers=dict(referer=ref)))['data']\n",
            "<del>    enc_text = base<number>.b<number>decode(json_data)\n",
            "<del>    dec_text = rc<number>(b'<number>bdc<number>e<number>a', enc_text).decode('utf<number>')\n",
            "<del>    youku_json = json.loads(dec_text)\n",
            "<del>\n",
            "<del>    yk_streams = {}\n",
            "<del>    for stream in youku_json['stream']:\n",
            "<del>        tp = stream['stream_type']\n",
            "<del>        yk_streams[tp] = [], stream['total_size']\n",
            "<del>        if stream.get('segs'):\n",
            "<del>            for seg in stream['segs']:\n",
            "<del>                yk_streams[tp][<number>].append(seg['url'])\n",
            "<del>        else:\n",
            "<del>            yk_streams[tp] = stream['m<number>u<number>'], stream['total_size']\n",
            "<del>\n",
            "<del>    return yk_streams\n",
            "<del>\n",
            "<del>def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "<del>    \"\"\"str, str, str, bool, bool ->None\n",
            "<del>\n",
            "<del>    Download Acfun video by vid.\n",
            "<del>\n",
            "<del>    Call Acfun API, decide which site to use, and pass the job to its\n",
            "<del>    extractor.\n",
            "<del>    \"\"\"\n",
            "<del>\n",
            "<del>    #first call the main parasing API\n",
            "<del>    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid, headers=fake_headers))\n",
            "<del>\n",
            "<del>    sourceType = info['sourceType']\n",
            "<del>\n",
            "<del>    #decide sourceId to know which extractor to use\n",
            "<del>    if 'sourceId' in info: sourceId = info['sourceId']\n",
            "<del>    # danmakuId = info['danmakuId']\n",
            "<del>\n",
            "<del>    #call extractor decided by sourceId\n",
            "<del>    if sourceType == 'sina':\n",
            "<del>        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "<del>    elif sourceType == 'youku':\n",
            "<del>        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n",
            "<del>    elif sourceType == 'tudou':\n",
            "<del>        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "<del>    elif sourceType == 'qq':\n",
            "<del>        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "<del>    elif sourceType == 'letv':\n",
            "<del>        letvcloud_download_by_vu(sourceId, '<number>d<number>c<number>', title, output_dir=output_dir, merge=merge, info_only=info_only)\n",
            "<del>    elif sourceType == 'zhuzhan':\n",
            "<del>        #As in Jul.<number>.<number>, Acfun is using embsig to anti hotlink so we need to pass this\n",
            "<del>#In Mar. <number> there is a dedicated ``acfun_proxy'' in youku cloud player\n",
            "<del>#old code removed\n",
            "<del>        url = 'http://www.acfun.cn/v/ac' + vid\n",
            "<del>        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n",
            "<del>        seq = ['mp<number>hd<number>', 'mp<number>hd<number>', 'mp<number>hd', 'flvhd']\n",
            "<del>        for t in seq:\n",
            "<del>            if yk_streams.get(t):\n",
            "<del>                preferred = yk_streams[t]\n",
            "<del>                break\n",
            "<del>#total_size in the json could be incorrect(F.I. <number>)\n",
            "<del>        size = <number>\n",
            "<del>        for url in preferred[<number>]:\n",
            "<del>            _, _, seg_size = url_info(url)\n",
            "<del>            size += seg_size\n",
            "<del>#fallback to flvhd is not quite possible\n",
            "<del>        if re.search(r'fid=[<number>-<number>A-Z\\-]*.flv', preferred[<number>][<number>]):\n",
            "<del>            ext = 'flv'\n",
            "else:\n",
            "<del>            ext = 'mp<number>'\n",
            "<del>        print_info(site_info, title, ext, size)\n",
            "<del>        if not info_only:\n",
            "<del>            download_urls(preferred[<number>], title, ext, size, output_dir=output_dir, merge=merge)\n",
            "<del>    else:\n",
            "<del>        raise NotImplementedError(sourceType)\n",
            "<del>\n",
            "<del>    if not info_only and not dry_run:\n",
            "<del>        if not kwargs['caption']:\n",
            "<del>            print('Skipping danmaku.')\n",
            "<del>            return\n",
            "<del>        try:\n",
            "<del>            title = get_filename(title)\n",
            "<del>            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n",
            "<del>            cmt = get_srt_json(vid)\n",
            "<del>            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-<number>') as x:\n",
            "<del>                x.write(cmt)\n",
            "<del>        except:\n",
            "<del>            pass\n",
            "<del>\n",
            "<del>def acfun_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "<del>    assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', url)\n",
            "<del>\n",
            "<del>    def getM<number>u<number>UrlFromCurrentVideoInfo(currentVideoInfo):\n",
            "<del>        if 'playInfos' in currentVideoInfo:\n",
            "<del>            return currentVideoInfo['playInfos'][<number>]['playUrls'][<number>]\n",
            "<del>        elif 'ksPlayJson' in currentVideoInfo:\n",
            "<del>            ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "<add>            raise NotImplemented            \n",
            "<add>\n",
            "<add>        if 'ksPlayJson' in currentVideoInfo:\n",
            "<add>            durationMillis = currentVideoInfo['durationMillis']\n",
            "<add>            ksPlayJson = ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "representation = ksPlayJson.get('adaptationSet')[<number>].get('representation')\n",
            "<del>            reps = []\n",
            "<del>            for one in representation:\n",
            "<del>                reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "<del>            return max(reps)[<number>]\n",
            "<del>\n",
            "<del>\n",
            "<del>    if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', url):\n",
            "<del>        html = get_content(url, headers=fake_headers)\n",
            "<del>        json_text = match<number>(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "<del>        json_data = json.loads(json_text)\n",
            "<del>        vid = json_data.get('currentVideoInfo').get('id')\n",
            "<del>        up = json_data.get('user').get('name')\n",
            "<del>        title = json_data.get('title')\n",
            "<del>        video_list = json_data.get('videoList')\n",
            "<del>        if len(video_list) > <number>:\n",
            "<del>            title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][<number>]\n",
            "<del>        currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<del>        m<number>u<number>_url = getM<number>u<number>UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "<del>    elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", url):\n",
            "<del>        html = get_content(url, headers=fake_headers)\n",
            "<del>        tag_script = match<number>(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "<del>        json_text = tag_script[tag_script.find('{') : tag_script.find('};') + <number>]\n",
            "<del>        json_data = json.loads(json_text)\n",
            "<del>        title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "<del>        vid = str(json_data['videoId'])\n",
            "<del>        up = \"acfun\"\n",
            "<del>\n",
            "<del>        currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<del>        m<number>u<number>_url = getM<number>u<number>UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "<del>\n",
            "<del>    else:\n",
            "<del>        raise NotImplemented\n",
            "<del>\n",
            "<del>    assert title and m<number>u<number>_url\n",
            "<del>    title = unescape_html(title)\n",
            "<del>    title = escape_file_path(title)\n",
            "<del>    p_title = r<number>('active\">([^<]+)', html)\n",
            "<del>    title = '%s (%s)' % (title, up)\n",
            "<del>    if p_title:\n",
            "<del>        title = '%s - %s' % (title, p_title)\n",
            "<del>\n",
            "<del>    print_info(site_info, title, 'm<number>u<number>', float('inf'))\n",
            "<del>    if not info_only:\n",
            "<del>        download_url_ffmpeg(m<number>u<number>_url, title, 'mp<number>', output_dir=output_dir, merge=merge)\n",
            "<add>            stream_list = representation\n",
            "<add>\n",
            "<add>        for stream in stream_list:\n",
            "<add>            m<number>u<number>_url = stream[\"url\"]\n",
            "<add>            size = durationMillis * stream[\"avgBitrate\"] / <number>\n",
            "<add>            # size = float('inf')\n",
            "<add>            container = 'mp<number>'\n",
            "<add>            stream_id = stream[\"qualityLabel\"]\n",
            "<add>            quality = stream[\"qualityType\"]\n",
            "<add>            \n",
            "<add>            stream_data = dict(src=m<number>u<number>_url, size=size, container=container, quality=quality)\n",
            "<add>            self.streams[stream_id] = stream_data\n",
            "<add>\n",
            "<add>        assert self.title and m<number>u<number>_url\n",
            "<add>        self.title = unescape_html(self.title)\n",
            "<add>        self.title = escape_file_path(self.title)\n",
            "<add>        p_title = r<number>('active\">([^<]+)', html)\n",
            "<add>        self.title = '%s (%s)' % (self.title, up)\n",
            "<add>        if p_title:\n",
            "<add>            self.title = '%s - %s' % (self.title, p_title)       \n",
            "<add>\n",
            "<add>\n",
            "<add>    def download(self, **kwargs):\n",
            "<add>        if 'json_output' in kwargs and kwargs['json_output']:\n",
            "<add>            json_output.output(self)\n",
            "<add>        elif 'info_only' in kwargs and kwargs['info_only']:\n",
            "<add>            if 'stream_id' in kwargs and kwargs['stream_id']:\n",
            "<add>                # Display the stream\n",
            "<add>                stream_id = kwargs['stream_id']\n",
            "<add>                if 'index' not in kwargs:\n",
            "<add>                    self.p(stream_id)\n",
            "<add>                else:\n",
            "<add>                    self.p_i(stream_id)\n",
            "<add>            else:\n",
            "<add>                # Display all available streams\n",
            "<add>                if 'index' not in kwargs:\n",
            "<add>                    self.p([])\n",
            "<add>                else:\n",
            "<add>                    stream_id = self.streams_sorted[<number>]['id'] if 'id' in self.streams_sorted[<number>] else self.streams_sorted[<number>]['itag']\n",
            "<add>                    self.p_i(stream_id)\n",
            "<add>\n",
            "<add>        else:\n",
            "<add>            if 'stream_id' in kwargs and kwargs['stream_id']:\n",
            "<add>                # Download the stream\n",
            "<add>                stream_id = kwargs['stream_id']\n",
            "<add>            else:\n",
            "<add>                stream_id = self.streams_sorted[<number>]['id'] if 'id' in self.streams_sorted[<number>] else self.streams_sorted[<number>]['itag']\n",
            "<add>\n",
            "<add>            if 'index' not in kwargs:\n",
            "<add>                self.p(stream_id)\n",
            "<add>            else:\n",
            "<add>                self.p_i(stream_id)\n",
            "<add>            if stream_id in self.streams:\n",
            "<add>                url = self.streams[stream_id]['src']\n",
            "<add>                ext = self.streams[stream_id]['container']\n",
            "<add>                total_size = self.streams[stream_id]['size']\n",
            "<add>\n",
            "<add>\n",
            "<add>            if ext == 'm<number>u<number>' or ext == 'm<number>a':\n",
            "<add>                ext = 'mp<number>'\n",
            "<add>\n",
            "<add>            if not url:\n",
            "<add>                log.wtf('[Failed] Cannot extract video source.')\n",
            "<add>            # For legacy main()\n",
            "<add>            headers = {}\n",
            "<add>            if self.ua is not None:\n",
            "<add>                headers['User-Agent'] = self.ua\n",
            "<add>            if self.referer is not None:\n",
            "<add>                headers['Referer'] = self.referer\n",
            "<add>\n",
            "<add>            download_url_ffmpeg(url, self.title, ext, output_dir=kwargs['output_dir'], merge=kwargs['merge'])                           \n",
            "<add>\n",
            "<add>            if 'caption' not in kwargs or not kwargs['caption']:\n",
            "<add>                print('Skipping captions or danmaku.')\n",
            "<add>                return\n",
            "<add>\n",
            "<add>            for lang in self.caption_tracks:\n",
            "<add>                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n",
            "<add>                print('Saving %s ... ' % filename, end=\"\", flush=True)\n",
            "<add>                srt = self.caption_tracks[lang]\n",
            "<add>                with open(os.path.join(kwargs['output_dir'], filename),\n",
            "<add>                          'w', encoding='utf-<number>') as x:\n",
            "<add>                    x.write(srt)\n",
            "<add>                print('Done.')\n",
            "<add>\n",
            "<add>            if self.danmaku is not None and not dry_run:\n",
            "<add>                filename = '{}.cmt.xml'.format(get_filename(self.title))\n",
            "<add>                print('Downloading {} ...\\n'.format(filename))\n",
            "<add>                with open(os.path.join(kwargs['output_dir'], filename), 'w', encoding='utf<number>') as fp:\n",
            "<add>                    fp.write(self.danmaku)\n",
            "<add>\n",
            "<add>            if self.lyrics is not None and not dry_run:\n",
            "<add>                filename = '{}.lrc'.format(get_filename(self.title))\n",
            "<add>                print('Downloading {} ...\\n'.format(filename))\n",
            "<add>                with open(os.path.join(kwargs['output_dir'], filename), 'w', encoding='utf<number>') as fp:\n",
            "<add>                    fp.write(self.lyrics)\n",
            "<add>\n",
            "<add>            # For main_dev()\n",
            "<add>            #download_urls(urls, self.title, self.streams[stream_id]['container'], self.streams[stream_id]['size'])\n",
            "<add>        keep_obj = kwargs.get('keep_obj', False)\n",
            "<add>        if not keep_obj:\n",
            "<add>            self.__init__()\n",
            "<add>\n",
            "<add>\n",
            "<add>    def acfun_download(self, url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "<add>        assert re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/(\\D|bangumi)/\\D\\D(\\d+)', url)\n",
            "<add>\n",
            "<add>        def getM<number>u<number>UrlFromCurrentVideoInfo(currentVideoInfo):\n",
            "<add>            if 'playInfos' in currentVideoInfo:\n",
            "<add>                return currentVideoInfo['playInfos'][<number>]['playUrls'][<number>]\n",
            "<add>            elif 'ksPlayJson' in currentVideoInfo:\n",
            "<add>                ksPlayJson = json.loads( currentVideoInfo['ksPlayJson'] )\n",
            "<add>                representation = ksPlayJson.get('adaptationSet')[<number>].get('representation')\n",
            "<add>                reps = []\n",
            "<add>                for one in representation:\n",
            "<add>                    reps.append( (one['width']* one['height'], one['url'], one['backupUrl']) )\n",
            "<add>                return max(reps)[<number>]\n",
            "<add>\n",
            "<add>\n",
            "<add>        if re.match(r'https?://[^\\.]*\\.*acfun\\.[^\\.]+/\\D/\\D\\D(\\d+)', url):\n",
            "<add>            html = get_content(url, headers=fake_headers)\n",
            "<add>            json_text = match<number>(html, r\"(?s)videoInfo\\s*=\\s*(\\{.*?\\});\")\n",
            "<add>            json_data = json.loads(json_text)\n",
            "<add>            vid = json_data.get('currentVideoInfo').get('id')\n",
            "<add>            up = json_data.get('user').get('name')\n",
            "<add>            title = json_data.get('title')\n",
            "<add>            video_list = json_data.get('videoList')\n",
            "<add>            if len(video_list) > <number>:\n",
            "<add>                title += \" - \" + [p.get('title') for p in video_list if p.get('id') == vid][<number>]\n",
            "<add>            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<add>            m<number>u<number>_url = getM<number>u<number>UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "<add>        elif re.match(\"https?://[^\\.]*\\.*acfun\\.[^\\.]+/bangumi/aa(\\d+)\", url):\n",
            "<add>            html = get_content(url, headers=fake_headers)\n",
            "<add>            tag_script = match<number>(html, r'<script>\\s*window\\.pageInfo([^<]+)</script>')\n",
            "<add>            json_text = tag_script[tag_script.find('{') : tag_script.find('};') + <number>]\n",
            "<add>            json_data = json.loads(json_text)\n",
            "<add>            title = json_data['bangumiTitle'] + \" \" + json_data['episodeName'] + \" \" + json_data['title']\n",
            "<add>            vid = str(json_data['videoId'])\n",
            "<add>            up = \"acfun\"\n",
            "<add>\n",
            "<add>            currentVideoInfo = json_data.get('currentVideoInfo')\n",
            "<add>            m<number>u<number>_url = getM<number>u<number>UrlFromCurrentVideoInfo(currentVideoInfo)\n",
            "<add>\n",
            "<add>        else:\n",
            "<add>            raise NotImplemented\n",
            "<add>        assert title and m<number>u<number>_url\n",
            "<add>        title = unescape_html(title)\n",
            "<add>        title = escape_file_path(title)\n",
            "<add>        p_title = r<number>('active\">([^<]+)', html)\n",
            "<add>        title = '%s (%s)' % (title, up)\n",
            "<add>        if p_title:\n",
            "<add>            title = '%s - %s' % (title, p_title)\n",
            "<add>\n",
            "<add>        print_info(site_info, title, 'm<number>u<number>', float('inf'))\n",
            "<add>        if not info_only:\n",
            "<add>            download_url_ffmpeg(m<number>u<number>_url, title, 'mp<number>', output_dir=output_dir, merge=merge)\n",
            "<add>site = AcFun()\n",
            "site_info = \"AcFun.cn\"\n",
            "<del>download = acfun_download\n",
            "<add>download = site.download_by_url\n",
            "download_playlist = playlist_not_supported('acfun')\n",
            "\n",
            "['\\n', '#', '!/', 'usr', '/', 'bin', '/', 'env', 'python', '\\n', '<del>', 'all', '= ', 'acfun', 'download', '\\n', '<del>', '\\n', 'from', 'common', 'import', '*\\n<', 'add', '>', 'from', 'extractor', 'import', 'video', 'extractor', '\\n', '<add>', '\\n', '<add>', 'class', 'ac', 'fun', 'video', 'extractor', '\\n', '<add>', 'name', '= ', 'ac', 'fun', '\\n', '<add>', '\\n', '<add>', 'stream', 'types', '= ', '\\n', '<add>', 'id', '<number>', 'p', 'quality', 'type', '<number>', 'p', '\\n', '<add>', 'id', '<number>', 'p', '<number>', 'quality', 'type', '<number>', 'p', '<number>', '\\n', '<add>', 'id', '<number>', 'p', '<number>', 'quality', 'type', '<number>', 'p', '<number>', '\\n', '<add>', 'id', '<number>', 'p', '+', 'quality', 'type', '<number>', 'p', '+', '\\n', '<add>', 'id', '<number>', 'p', 'quality', 'type', '<number>', 'p', '\\n', '<add>', 'id', '<number>', 'p', 'quality', 'type', '<number>', 'p', '\\n', '<add>', 'id', '<number>', 'p', 'quality', 'type', '<number>', 'p', '\\n', '<add>', 'id', '<number>', 'p', 'quality', 'type', '<number>', 'p', '\\n', '<add>', '\\n', '<add>', '\\n', '<add>', 'def', 'prepare', 'self', '**', 'kwargs', '\\n', '<add>', 'assert', 're', 'match', 'r', 'https', '?', '//', '^', '*', '*', 'acfun', '^', '+/', 'd', '|', 'bangumi', '/', 'd', 'd', 'd', '+', 'self', 'url', '\\n', '<add>', '\\n', '<add>', 'if', 're', 'match', 'r', 'https', '?', '//', '^', '*', '*', 'acfun', '^', '+/', 'd', '/', 'd', 'd', 'd', '+', 'self', 'url', '\\n', '<add>', 'html', '= ', 'get', 'content', 'self', 'url', 'headers', '=', 'fake', 'headers', '\\n', '<add>', 'json', 'text', '= ', 'match', '<number>', 'html', 'r', '?', 's', 'video', 'info', 's', '*=', 's', '*', '*?', '\\n', '<add>', 'json', 'data', '= ', 'json', 'loads', 'json', 'text', '\\n', '<add>', 'vid', '= ', 'json', 'data', 'get', 'current', 'video', 'info', 'get', 'id', '\\n', '<add>', 'up', '= ', 'json', 'data', 'get', 'user', 'get', 'name', '\\n', '<add>', 'self', 'title', '= ', 'json', 'data', 'get', 'title', '\\n', '<add>', 'video', 'list', '= ', 'json', 'data', 'get', 'video', 'list', '\\n', '<add>', 'if', 'len', 'video', 'list', '> <', 'number', '>', '\\n', '<add>', 'self', 'title', '+= ', '- ', '+ ', 'p', 'get', 'title', 'for', 'p', 'in', 'video', 'list', 'if', 'p', 'get', 'id', '== ', 'vid', '<number>', '\\n', '<add>', 'current', 'video', 'info', '= ', 'json', 'data', 'get', 'current', 'video', 'info', '\\n', '<add>', '\\n', '<add>', 'elif', 're', 'match', 'https', '?', '//', '^', '*', '*', 'acfun', '^', '+/', 'bangumi', '/', 'aa', 'd', '+', 'self', 'url', '\\n', '<add>', 'html', '= ', 'get', 'content', 'self', 'url', 'headers', '=', 'fake', 'headers', '\\n', '<add>', 'tag', 'script', '= ', 'match', '<number>', 'html', 'r', '<', 'script', '>', 's', '*', 'window', 'page', 'info', '^<', '+', '</', 'script', '>', '\\n', '<add>', 'json', 'text', '= ', 'tag', 'script', 'tag', 'script', 'find', 'tag', 'script', 'find', '+ <', 'number', '>', '\\n', '<add>', 'json', 'data', '= ', 'json', 'loads', 'json', 'text', '\\n', '<add>', 'self', 'title', '= ', 'json', 'data', 'bangumi', 'title', '+ ', '+ ', 'json', 'data', 'episode', 'name', '+ ', '+ ', 'json', 'data', 'title', '\\n', '<add>', 'vid', '= ', 'str', 'json', 'data', 'video', 'id', '\\n', '<add>', 'up', '= ', 'acfun', '\\n', '<add>', 'current', 'video', 'info', '= ', 'json', 'data', 'get', 'current', 'video', 'info', '\\n', '<del>', 'from', 'le', 'import', 'letvcloud', 'download', 'by', 'vu', '\\n', '<del>', 'from', 'qq', 'import', 'qq', 'download', 'by', 'vid', '\\n', '<del>', 'from', 'sina', 'import', 'sina', 'download', 'by', 'vid', '\\n', '<del>', 'from', 'tudou', 'import', 'tudou', 'download', 'by', 'iid', '\\n', '<del>', 'from', 'youku', 'import', 'youku', 'download', 'by', 'vid', '\\n', '<del>', '\\n', '<del>', 'import', 'json', '\\n', '<del>', 'import', 're', '\\n', '<del>', 'import', 'base', '<number>', '\\n', '<del>', 'import', 'time', '\\n', '<del>', '\\n', '<del>', 'def', 'get', 'srt', 'json', 'id', '\\n', '<del>', 'url', '= ', 'http', '//', 'danmu', 'aixifan', 'com', '/', 'v', '<number>', '/%', 's', '% ', 'id', '\\n', '<del>', 'return', 'get', 'content', 'url', '\\n', '<del>', '\\n', '<del>', 'def', 'youku', 'acfun', 'proxy', 'vid', 'sign', 'ref', '\\n', '<del>', 'endpoint', '= ', 'http', '//', 'player', 'acfun', 'cn', '/', 'flash', 'data', '?', 'vid', '=', '&', 'ct', '=<', 'number', '>&', 'ev', '=<', 'number', '>&', 'sign', '=', '&', 'time', '=', '\\n', '<del>', 'url', '= ', 'endpoint', 'format', 'vid', 'sign', 'str', 'int', 'time', 'time', '* <', 'number', '>', '\\n', '<del>', 'json', 'data', '= ', 'json', 'loads', 'get', 'content', 'url', 'headers', '=', 'dict', 'referer', '=', 'ref', 'data', '\\n', '<del>', 'enc', 'text', '= ', 'base', '<number>', 'b', '<number>', 'decode', 'json', 'data', '\\n', '<del>', 'dec', 'text', '= ', 'rc', '<number>', 'b', '<number>', 'bdc', '<number>', 'e', '<number>', 'a', 'enc', 'text', 'decode', 'utf', '<number>', '\\n', '<del>', 'youku', 'json', '= ', 'json', 'loads', 'dec', 'text', '\\n', '<del>', '\\n', '<del>', 'yk', 'streams', '= ', '\\n', '<del>', 'for', 'stream', 'in', 'youku', 'json', 'stream', '\\n', '<del>', 'tp', '= ', 'stream', 'stream', 'type', '\\n', '<del>', 'yk', 'streams', 'tp', '= ', 'stream', 'total', 'size', '\\n', '<del>', 'if', 'stream', 'get', 'segs', '\\n', '<del>', 'for', 'seg', 'in', 'stream', 'segs', '\\n', '<del>', 'yk', 'streams', 'tp', '<number>', 'append', 'seg', 'url', '\\n', '<del>', 'else', '\\n', '<del>', 'yk', 'streams', 'tp', '= ', 'stream', 'm', '<number>', 'u', '<number>', 'stream', 'total', 'size', '\\n', '<del>', '\\n', '<del>', 'return', 'yk', 'streams', '\\n', '<del>', '\\n', '<del>', 'def', 'acfun', 'download', 'by', 'vid', 'vid', 'title', 'output', 'dir', '=', 'merge', '=', 'true', 'info', 'only', '=', 'false', '**', 'kwargs', '\\n', '<del>', 'str', 'str', 'str', 'bool', 'bool', '->', 'none', '\\n', '<del>', '\\n', '<del>', 'download', 'acfun', 'video', 'by', 'vid', '\\n', '<del>', '\\n', '<del>', 'call', 'acfun', 'api', 'decide', 'which', 'site', 'to', 'use', 'and', 'pass', 'the', 'job', 'to', 'its', '\\n', '<del>', 'extractor', '\\n', '<del>', '\\n', '<del>', '\\n', '<del>', '#', 'first', 'call', 'the', 'main', 'parasing', 'api', '\\n', '<del>', 'info', '= ', 'json', 'loads', 'get', 'content', 'http', '//', 'www', 'acfun', 'cn', '/', 'video', '/', 'get', 'video', 'aspx', '?', 'id', '=', '+ ', 'vid', 'headers', '=', 'fake', 'headers', '\\n', '<del>', '\\n', '<del>', 'source', 'type', '= ', 'info', 'source', 'type', '\\n', '<del>', '\\n', '<del>', '#', 'decide', 'source', 'id', 'to', 'know', 'which', 'extractor', 'to', 'use', '\\n', '<del>', 'if', 'source', 'id', 'in', 'info', 'source', 'id', '= ', 'info', 'source', 'id', '\\n', '<del>', '# ', 'danmaku', 'id', '= ', 'info', 'danmaku', 'id', '\\n', '<del>', '\\n', '<del>', '#', 'call', 'extractor', 'decided', 'by', 'source', 'id', '\\n', '<del>', 'if', 'source', 'type', '== ', 'sina', '\\n', '<del>', 'sina', 'download', 'by', 'vid', 'source', 'id', 'title', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'info', 'only', '=', 'info', 'only', '\\n', '<del>', 'elif', 'source', 'type', '== ', 'youku', '\\n', '<del>', 'youku', 'download', 'by', 'vid', 'source', 'id', 'title', '=', 'title', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'info', 'only', '=', 'info', 'only', '**', 'kwargs', '\\n', '<del>', 'elif', 'source', 'type', '== ', 'tudou', '\\n', '<del>', 'tudou', 'download', 'by', 'iid', 'source', 'id', 'title', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'info', 'only', '=', 'info', 'only', '\\n', '<del>', 'elif', 'source', 'type', '== ', 'qq', '\\n', '<del>', 'qq', 'download', 'by', 'vid', 'source', 'id', 'title', 'true', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'info', 'only', '=', 'info', 'only', '\\n', '<del>', 'elif', 'source', 'type', '== ', 'letv', '\\n', '<del>', 'letvcloud', 'download', 'by', 'vu', 'source', 'id', '<number>', 'd', '<number>', 'c', '<number>', 'title', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'info', 'only', '=', 'info', 'only', '\\n', '<del>', 'elif', 'source', 'type', '== ', 'zhuzhan', '\\n', '<del>', '#', 'as', 'in', 'jul', '<number>', '<number>', 'acfun', 'is', 'using', 'embsig', 'to', 'anti', 'hotlink', 'so', 'we', 'need', 'to', 'pass', 'this', '\\n', '<del>', '#', 'in', 'mar', '<number>', 'there', 'is', 'a', 'dedicated', 'acfun', 'proxy', 'in', 'youku', 'cloud', 'player', '\\n', '<del>', '#', 'old', 'code', 'removed', '\\n', '<del>', 'url', '= ', 'http', '//', 'www', 'acfun', 'cn', '/', 'v', '/', 'ac', '+ ', 'vid', '\\n', '<del>', 'yk', 'streams', '= ', 'youku', 'acfun', 'proxy', 'info', 'source', 'id', 'info', 'encode', 'url', '\\n', '<del>', 'seq', '= ', 'mp', '<number>', 'hd', '<number>', 'mp', '<number>', 'hd', '<number>', 'mp', '<number>', 'hd', 'flvhd', '\\n', '<del>', 'for', 't', 'in', 'seq', '\\n', '<del>', 'if', 'yk', 'streams', 'get', 't', '\\n', '<del>', 'preferred', '= ', 'yk', 'streams', 't', '\\n', '<del>', 'break', '\\n', '<del>', '#', 'total', 'size', 'in', 'the', 'json', 'could', 'be', 'incorrect', 'f', 'i', '<number>', '\\n', '<del>', 'size', '= <', 'number', '>\\n<', 'del', '>        ', 'for', 'url', 'in', 'preferred', '<number>', '\\n', '<del>', 'seg', 'size', '= ', 'url', 'info', 'url', '\\n', '<del>', 'size', '+= ', 'seg', 'size', '\\n', '<del>', '#', 'fallback', 'to', 'flvhd', 'is', 'not', 'quite', 'possible', '\\n', '<del>', 'if', 're', 'search', 'r', 'fid', '=', '<number>', '-<', 'number', '>', 'a', '-', 'Z', '-', '*', 'flv', 'preferred', '<number>', '<number>', '\\n', '<del>', 'ext', '= ', 'flv', '\\n', 'else', '\\n', '<del>', 'ext', '= ', 'mp', '<number>', '\\n', '<del>', 'print', 'info', 'site', 'info', 'title', 'ext', 'size', '\\n', '<del>', 'if', 'not', 'info', 'only', '\\n', '<del>', 'download', 'urls', 'preferred', '<number>', 'title', 'ext', 'size', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', '\\n', '<del>', 'else', '\\n', '<del>', 'raise', 'not', 'implemented', 'error', 'source', 'type', '\\n', '<del>', '\\n', '<del>', 'if', 'not', 'info', 'only', 'and', 'not', 'dry', 'run', '\\n', '<del>', 'if', 'not', 'kwargs', 'caption', '\\n', '<del>', 'print', 'skipping', 'danmaku', '\\n', '<del>', 'return', '\\n', '<del>', 'try', '\\n', '<del>', 'title', '= ', 'get', 'filename', 'title', '\\n', '<del>', 'print', 'downloading', '%', 's', 'n', '% ', 'title', '+ ', 'cmt', 'json', '\\n', '<del>', 'cmt', '= ', 'get', 'srt', 'json', 'vid', '\\n', '<del>', 'with', 'open', 'os', 'path', 'join', 'output', 'dir', 'title', '+ ', 'cmt', 'json', 'w', 'encoding', '=', 'utf', '-<', 'number', '>', 'as', 'x', '\\n', '<del>', 'x', 'write', 'cmt', '\\n', '<del>', 'except', '\\n', '<del>', 'pass', '\\n', '<del>', '\\n', '<del>', 'def', 'acfun', 'download', 'url', 'output', 'dir', '=', 'merge', '=', 'true', 'info', 'only', '=', 'false', '**', 'kwargs', '\\n', '<del>', 'assert', 're', 'match', 'r', 'https', '?', '//', '^', '*', '*', 'acfun', '^', '+/', 'd', '|', 'bangumi', '/', 'd', 'd', 'd', '+', 'url', '\\n', '<del>', '\\n', '<del>', 'def', 'get', 'm', '<number>', 'u', '<number>', 'url', 'from', 'current', 'video', 'info', 'current', 'video', 'info', '\\n', '<del>', 'if', 'play', 'infos', 'in', 'current', 'video', 'info', '\\n', '<del>', 'return', 'current', 'video', 'info', 'play', 'infos', '<number>', 'play', 'urls', '<number>', '\\n', '<del>', 'elif', 'ks', 'play', 'json', 'in', 'current', 'video', 'info', '\\n', '<del>', 'ks', 'play', 'json', '= ', 'json', 'loads', 'current', 'video', 'info', 'ks', 'play', 'json', '\\n', '<add>', 'raise', 'not', 'implemented', '\\n', '<add>', '\\n', '<add>', 'if', 'ks', 'play', 'json', 'in', 'current', 'video', 'info', '\\n', '<add>', 'duration', 'millis', '= ', 'current', 'video', 'info', 'duration', 'millis', '\\n', '<add>', 'ks', 'play', 'json', '= ', 'ks', 'play', 'json', '= ', 'json', 'loads', 'current', 'video', 'info', 'ks', 'play', 'json', '\\n', 'representation', '= ', 'ks', 'play', 'json', 'get', 'adaptation', 'set', '<number>', 'get', 'representation', '\\n', '<del>', 'reps', '= ', '\\n', '<del>', 'for', 'one', 'in', 'representation', '\\n', '<del>', 'reps', 'append', 'one', 'width', '* ', 'one', 'height', 'one', 'url', 'one', 'backup', 'url', '\\n', '<del>', 'return', 'max', 'reps', '<number>', '\\n', '<del>', '\\n', '<del>', '\\n', '<del>', 'if', 're', 'match', 'r', 'https', '?', '//', '^', '*', '*', 'acfun', '^', '+/', 'd', '/', 'd', 'd', 'd', '+', 'url', '\\n', '<del>', 'html', '= ', 'get', 'content', 'url', 'headers', '=', 'fake', 'headers', '\\n', '<del>', 'json', 'text', '= ', 'match', '<number>', 'html', 'r', '?', 's', 'video', 'info', 's', '*=', 's', '*', '*?', '\\n', '<del>', 'json', 'data', '= ', 'json', 'loads', 'json', 'text', '\\n', '<del>', 'vid', '= ', 'json', 'data', 'get', 'current', 'video', 'info', 'get', 'id', '\\n', '<del>', 'up', '= ', 'json', 'data', 'get', 'user', 'get', 'name', '\\n', '<del>', 'title', '= ', 'json', 'data', 'get', 'title', '\\n', '<del>', 'video', 'list', '= ', 'json', 'data', 'get', 'video', 'list', '\\n', '<del>', 'if', 'len', 'video', 'list', '> <', 'number', '>', '\\n', '<del>', 'title', '+= ', '- ', '+ ', 'p', 'get', 'title', 'for', 'p', 'in', 'video', 'list', 'if', 'p', 'get', 'id', '== ', 'vid', '<number>', '\\n', '<del>', 'current', 'video', 'info', '= ', 'json', 'data', 'get', 'current', 'video', 'info', '\\n', '<del>', 'm', '<number>', 'u', '<number>', 'url', '= ', 'get', 'm', '<number>', 'u', '<number>', 'url', 'from', 'current', 'video', 'info', 'current', 'video', 'info', '\\n', '<del>', 'elif', 're', 'match', 'https', '?', '//', '^', '*', '*', 'acfun', '^', '+/', 'bangumi', '/', 'aa', 'd', '+', 'url', '\\n', '<del>', 'html', '= ', 'get', 'content', 'url', 'headers', '=', 'fake', 'headers', '\\n', '<del>', 'tag', 'script', '= ', 'match', '<number>', 'html', 'r', '<', 'script', '>', 's', '*', 'window', 'page', 'info', '^<', '+', '</', 'script', '>', '\\n', '<del>', 'json', 'text', '= ', 'tag', 'script', 'tag', 'script', 'find', 'tag', 'script', 'find', '+ <', 'number', '>', '\\n', '<del>', 'json', 'data', '= ', 'json', 'loads', 'json', 'text', '\\n', '<del>', 'title', '= ', 'json', 'data', 'bangumi', 'title', '+ ', '+ ', 'json', 'data', 'episode', 'name', '+ ', '+ ', 'json', 'data', 'title', '\\n', '<del>', 'vid', '= ', 'str', 'json', 'data', 'video', 'id', '\\n', '<del>', 'up', '= ', 'acfun', '\\n', '<del>', '\\n', '<del>', 'current', 'video', 'info', '= ', 'json', 'data', 'get', 'current', 'video', 'info', '\\n', '<del>', 'm', '<number>', 'u', '<number>', 'url', '= ', 'get', 'm', '<number>', 'u', '<number>', 'url', 'from', 'current', 'video', 'info', 'current', 'video', 'info', '\\n', '<del>', '\\n', '<del>', 'else', '\\n', '<del>', 'raise', 'not', 'implemented', '\\n', '<del>', '\\n', '<del>', 'assert', 'title', 'and', 'm', '<number>', 'u', '<number>', 'url', '\\n', '<del>', 'title', '= ', 'unescape', 'html', 'title', '\\n', '<del>', 'title', '= ', 'escape', 'file', 'path', 'title', '\\n', '<del>', 'p', 'title', '= ', 'r', '<number>', 'active', '>', '^<', '+', 'html', '\\n', '<del>', 'title', '= ', '%', 's', '%', 's', '% ', 'title', 'up', '\\n', '<del>', 'if', 'p', 'title', '\\n', '<del>', 'title', '= ', '%', 's', '- %', 's', '% ', 'title', 'p', 'title', '\\n', '<del>', '\\n', '<del>', 'print', 'info', 'site', 'info', 'title', 'm', '<number>', 'u', '<number>', 'float', 'inf', '\\n', '<del>', 'if', 'not', 'info', 'only', '\\n', '<del>', 'download', 'url', 'ffmpeg', 'm', '<number>', 'u', '<number>', 'url', 'title', 'mp', '<number>', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', '\\n', '<add>', 'stream', 'list', '= ', 'representation', '\\n', '<add>', '\\n', '<add>', 'for', 'stream', 'in', 'stream', 'list', '\\n', '<add>', 'm', '<number>', 'u', '<number>', 'url', '= ', 'stream', 'url', '\\n', '<add>', 'size', '= ', 'duration', 'millis', '* ', 'stream', 'avg', 'bitrate', '/ <', 'number', '>\\n<', 'add', '>            ', '# ', 'size', '= ', 'float', 'inf', '\\n', '<add>', 'container', '= ', 'mp', '<number>', '\\n', '<add>', 'stream', 'id', '= ', 'stream', 'quality', 'label', '\\n', '<add>', 'quality', '= ', 'stream', 'quality', 'type', '\\n', '<add>', '\\n', '<add>', 'stream', 'data', '= ', 'dict', 'src', '=', 'm', '<number>', 'u', '<number>', 'url', 'size', '=', 'size', 'container', '=', 'container', 'quality', '=', 'quality', '\\n', '<add>', 'self', 'streams', 'stream', 'id', '= ', 'stream', 'data', '\\n', '<add>', '\\n', '<add>', 'assert', 'self', 'title', 'and', 'm', '<number>', 'u', '<number>', 'url', '\\n', '<add>', 'self', 'title', '= ', 'unescape', 'html', 'self', 'title', '\\n', '<add>', 'self', 'title', '= ', 'escape', 'file', 'path', 'self', 'title', '\\n', '<add>', 'p', 'title', '= ', 'r', '<number>', 'active', '>', '^<', '+', 'html', '\\n', '<add>', 'self', 'title', '= ', '%', 's', '%', 's', '% ', 'self', 'title', 'up', '\\n', '<add>', 'if', 'p', 'title', '\\n', '<add>', 'self', 'title', '= ', '%', 's', '- %', 's', '% ', 'self', 'title', 'p', 'title', '\\n', '<add>', '\\n', '<add>', '\\n', '<add>', 'def', 'download', 'self', '**', 'kwargs', '\\n', '<add>', 'if', 'json', 'output', 'in', 'kwargs', 'and', 'kwargs', 'json', 'output', '\\n', '<add>', 'json', 'output', 'output', 'self', '\\n', '<add>', 'elif', 'info', 'only', 'in', 'kwargs', 'and', 'kwargs', 'info', 'only', '\\n', '<add>', 'if', 'stream', 'id', 'in', 'kwargs', 'and', 'kwargs', 'stream', 'id', '\\n', '<add>', '# ', 'display', 'the', 'stream', '\\n', '<add>', 'stream', 'id', '= ', 'kwargs', 'stream', 'id', '\\n', '<add>', 'if', 'index', 'not', 'in', 'kwargs', '\\n', '<add>', 'self', 'p', 'stream', 'id', '\\n', '<add>', 'else', '\\n', '<add>', 'self', 'p', 'i', 'stream', 'id', '\\n', '<add>', 'else', '\\n', '<add>', '# ', 'display', 'all', 'available', 'streams', '\\n', '<add>', 'if', 'index', 'not', 'in', 'kwargs', '\\n', '<add>', 'self', 'p', '\\n', '<add>', 'else', '\\n', '<add>', 'stream', 'id', '= ', 'self', 'streams', 'sorted', '<number>', 'id', 'if', 'id', 'in', 'self', 'streams', 'sorted', '<number>', 'else', 'self', 'streams', 'sorted', '<number>', 'itag', '\\n', '<add>', 'self', 'p', 'i', 'stream', 'id', '\\n', '<add>', '\\n', '<add>', 'else', '\\n', '<add>', 'if', 'stream', 'id', 'in', 'kwargs', 'and', 'kwargs', 'stream', 'id', '\\n', '<add>', '# ', 'download', 'the', 'stream', '\\n', '<add>', 'stream', 'id', '= ', 'kwargs', 'stream', 'id', '\\n', '<add>', 'else', '\\n', '<add>', 'stream', 'id', '= ', 'self', 'streams', 'sorted', '<number>', 'id', 'if', 'id', 'in', 'self', 'streams', 'sorted', '<number>', 'else', 'self', 'streams', 'sorted', '<number>', 'itag', '\\n', '<add>', '\\n', '<add>', 'if', 'index', 'not', 'in', 'kwargs', '\\n', '<add>', 'self', 'p', 'stream', 'id', '\\n', '<add>', 'else', '\\n', '<add>', 'self', 'p', 'i', 'stream', 'id', '\\n', '<add>', 'if', 'stream', 'id', 'in', 'self', 'streams', '\\n', '<add>', 'url', '= ', 'self', 'streams', 'stream', 'id', 'src', '\\n', '<add>', 'ext', '= ', 'self', 'streams', 'stream', 'id', 'container', '\\n', '<add>', 'total', 'size', '= ', 'self', 'streams', 'stream', 'id', 'size', '\\n', '<add>', '\\n', '<add>', '\\n', '<add>', 'if', 'ext', '== ', 'm', '<number>', 'u', '<number>', 'or', 'ext', '== ', 'm', '<number>', 'a', '\\n', '<add>', 'ext', '= ', 'mp', '<number>', '\\n', '<add>', '\\n', '<add>', 'if', 'not', 'url', '\\n', '<add>', 'log', 'wtf', 'failed', 'cannot', 'extract', 'video', 'source', '\\n', '<add>', '# ', 'for', 'legacy', 'main', '\\n', '<add>', 'headers', '= ', '\\n', '<add>', 'if', 'self', 'ua', 'is', 'not', 'none', '\\n', '<add>', 'headers', 'user', '-', 'agent', '= ', 'self', 'ua', '\\n', '<add>', 'if', 'self', 'referer', 'is', 'not', 'none', '\\n', '<add>', 'headers', 'referer', '= ', 'self', 'referer', '\\n', '<add>', '\\n', '<add>', 'download', 'url', 'ffmpeg', 'url', 'self', 'title', 'ext', 'output', 'dir', '=', 'kwargs', 'output', 'dir', 'merge', '=', 'kwargs', 'merge', '\\n', '<add>', '\\n', '<add>', 'if', 'caption', 'not', 'in', 'kwargs', 'or', 'not', 'kwargs', 'caption', '\\n', '<add>', 'print', 'skipping', 'captions', 'or', 'danmaku', '\\n', '<add>', 'return', '\\n', '<add>', '\\n', '<add>', 'for', 'lang', 'in', 'self', 'caption', 'tracks', '\\n', '<add>', 'filename', '= ', '%', 's', '%', 's', 'srt', '% ', 'get', 'filename', 'self', 'title', 'lang', '\\n', '<add>', 'print', 'saving', '%', 's', '% ', 'filename', 'end', '=', 'flush', '=', 'true', '\\n', '<add>', 'srt', '= ', 'self', 'caption', 'tracks', 'lang', '\\n', '<add>', 'with', 'open', 'os', 'path', 'join', 'kwargs', 'output', 'dir', 'filename', '\\n', '<add>', 'w', 'encoding', '=', 'utf', '-<', 'number', '>', 'as', 'x', '\\n', '<add>', 'x', 'write', 'srt', '\\n', '<add>', 'print', 'done', '\\n', '<add>', '\\n', '<add>', 'if', 'self', 'danmaku', 'is', 'not', 'none', 'and', 'not', 'dry', 'run', '\\n', '<add>', 'filename', '= ', 'cmt', 'xml', 'format', 'get', 'filename', 'self', 'title', '\\n', '<add>', 'print', 'downloading', 'n', 'format', 'filename', '\\n', '<add>', 'with', 'open', 'os', 'path', 'join', 'kwargs', 'output', 'dir', 'filename', 'w', 'encoding', '=', 'utf', '<number>', 'as', 'fp', '\\n', '<add>', 'fp', 'write', 'self', 'danmaku', '\\n', '<add>', '\\n', '<add>', 'if', 'self', 'lyrics', 'is', 'not', 'none', 'and', 'not', 'dry', 'run', '\\n', '<add>', 'filename', '= ', 'lrc', 'format', 'get', 'filename', 'self', 'title', '\\n', '<add>', 'print', 'downloading', 'n', 'format', 'filename', '\\n', '<add>', 'with', 'open', 'os', 'path', 'join', 'kwargs', 'output', 'dir', 'filename', 'w', 'encoding', '=', 'utf', '<number>', 'as', 'fp', '\\n', '<add>', 'fp', 'write', 'self', 'lyrics', '\\n', '<add>', '\\n', '<add>', '# ', 'for', 'main', 'dev', '\\n', '<add>', '#', 'download', 'urls', 'urls', 'self', 'title', 'self', 'streams', 'stream', 'id', 'container', 'self', 'streams', 'stream', 'id', 'size', '\\n', '<add>', 'keep', 'obj', '= ', 'kwargs', 'get', 'keep', 'obj', 'false', '\\n', '<add>', 'if', 'not', 'keep', 'obj', '\\n', '<add>', 'self', 'init', '\\n', '<add>', '\\n', '<add>', '\\n', '<add>', 'def', 'acfun', 'download', 'self', 'url', 'output', 'dir', '=', 'merge', '=', 'true', 'info', 'only', '=', 'false', '**', 'kwargs', '\\n', '<add>', 'assert', 're', 'match', 'r', 'https', '?', '//', '^', '*', '*', 'acfun', '^', '+/', 'd', '|', 'bangumi', '/', 'd', 'd', 'd', '+', 'url', '\\n', '<add>', '\\n', '<add>', 'def', 'get', 'm', '<number>', 'u', '<number>', 'url', 'from', 'current', 'video', 'info', 'current', 'video', 'info', '\\n', '<add>', 'if', 'play', 'infos', 'in', 'current', 'video', 'info', '\\n', '<add>', 'return', 'current', 'video', 'info', 'play', 'infos', '<number>', 'play', 'urls', '<number>', '\\n', '<add>', 'elif', 'ks', 'play', 'json', 'in', 'current', 'video', 'info', '\\n', '<add>', 'ks', 'play', 'json', '= ', 'json', 'loads', 'current', 'video', 'info', 'ks', 'play', 'json', '\\n', '<add>', 'representation', '= ', 'ks', 'play', 'json', 'get', 'adaptation', 'set', '<number>', 'get', 'representation', '\\n', '<add>', 'reps', '= ', '\\n', '<add>', 'for', 'one', 'in', 'representation', '\\n', '<add>', 'reps', 'append', 'one', 'width', '* ', 'one', 'height', 'one', 'url', 'one', 'backup', 'url', '\\n', '<add>', 'return', 'max', 'reps', '<number>', '\\n', '<add>', '\\n', '<add>', '\\n', '<add>', 'if', 're', 'match', 'r', 'https', '?', '//', '^', '*', '*', 'acfun', '^', '+/', 'd', '/', 'd', 'd', 'd', '+', 'url', '\\n', '<add>', 'html', '= ', 'get', 'content', 'url', 'headers', '=', 'fake', 'headers', '\\n', '<add>', 'json', 'text', '= ', 'match', '<number>', 'html', 'r', '?', 's', 'video', 'info', 's', '*=', 's', '*', '*?', '\\n', '<add>', 'json', 'data', '= ', 'json', 'loads', 'json', 'text', '\\n', '<add>', 'vid', '= ', 'json', 'data', 'get', 'current', 'video', 'info', 'get', 'id', '\\n', '<add>', 'up', '= ', 'json', 'data', 'get', 'user', 'get', 'name', '\\n', '<add>', 'title', '= ', 'json', 'data', 'get', 'title', '\\n', '<add>', 'video', 'list', '= ', 'json', 'data', 'get', 'video', 'list', '\\n', '<add>', 'if', 'len', 'video', 'list', '> <', 'number', '>', '\\n', '<add>', 'title', '+= ', '- ', '+ ', 'p', 'get', 'title', 'for', 'p', 'in', 'video', 'list', 'if', 'p', 'get', 'id', '== ', 'vid', '<number>', '\\n', '<add>', 'current', 'video', 'info', '= ', 'json', 'data', 'get', 'current', 'video', 'info', '\\n', '<add>', 'm', '<number>', 'u', '<number>', 'url', '= ', 'get', 'm', '<number>', 'u', '<number>', 'url', 'from', 'current', 'video', 'info', 'current', 'video', 'info', '\\n', '<add>', 'elif', 're', 'match', 'https', '?', '//', '^', '*', '*', 'acfun', '^', '+/', 'bangumi', '/', 'aa', 'd', '+', 'url', '\\n', '<add>', 'html', '= ', 'get', 'content', 'url', 'headers', '=', 'fake', 'headers', '\\n', '<add>', 'tag', 'script', '= ', 'match', '<number>', 'html', 'r', '<', 'script', '>', 's', '*', 'window', 'page', 'info', '^<', '+', '</', 'script', '>', '\\n', '<add>', 'json', 'text', '= ', 'tag', 'script', 'tag', 'script', 'find', 'tag', 'script', 'find', '+ <', 'number', '>', '\\n', '<add>', 'json', 'data', '= ', 'json', 'loads', 'json', 'text', '\\n', '<add>', 'title', '= ', 'json', 'data', 'bangumi', 'title', '+ ', '+ ', 'json', 'data', 'episode', 'name', '+ ', '+ ', 'json', 'data', 'title', '\\n', '<add>', 'vid', '= ', 'str', 'json', 'data', 'video', 'id', '\\n', '<add>', 'up', '= ', 'acfun', '\\n', '<add>', '\\n', '<add>', 'current', 'video', 'info', '= ', 'json', 'data', 'get', 'current', 'video', 'info', '\\n', '<add>', 'm', '<number>', 'u', '<number>', 'url', '= ', 'get', 'm', '<number>', 'u', '<number>', 'url', 'from', 'current', 'video', 'info', 'current', 'video', 'info', '\\n', '<add>', '\\n', '<add>', 'else', '\\n', '<add>', 'raise', 'not', 'implemented', '\\n', '<add>', 'assert', 'title', 'and', 'm', '<number>', 'u', '<number>', 'url', '\\n', '<add>', 'title', '= ', 'unescape', 'html', 'title', '\\n', '<add>', 'title', '= ', 'escape', 'file', 'path', 'title', '\\n', '<add>', 'p', 'title', '= ', 'r', '<number>', 'active', '>', '^<', '+', 'html', '\\n', '<add>', 'title', '= ', '%', 's', '%', 's', '% ', 'title', 'up', '\\n', '<add>', 'if', 'p', 'title', '\\n', '<add>', 'title', '= ', '%', 's', '- %', 's', '% ', 'title', 'p', 'title', '\\n', '<add>', '\\n', '<add>', 'print', 'info', 'site', 'info', 'title', 'm', '<number>', 'u', '<number>', 'float', 'inf', '\\n', '<add>', 'if', 'not', 'info', 'only', '\\n', '<add>', 'download', 'url', 'ffmpeg', 'm', '<number>', 'u', '<number>', 'url', 'title', 'mp', '<number>', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', '\\n', '<add>', 'site', '= ', 'ac', 'fun', '\\n', 'site', 'info', '= ', 'ac', 'fun', 'cn', '\\n', '<del>', 'download', '= ', 'acfun', 'download', '\\n', '<add>', 'download', '= ', 'site', 'download', 'by', 'url', '\\n', 'download', 'playlist', '= ', 'playlist', 'not', 'supported', 'acfun', '\\n']\n",
            "\n",
            "['fixed', 'tiktok', 'extraction']\n",
            "@@ -15,16 +15,16 @@ def tiktok_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "     uniqueId = videoData['authorInfos'].get('uniqueId')\n",
            "     nickName = videoData['authorInfos'].get('nickName')\n",
            " \n",
            "-    for i, url in enumerate(urls):\n",
            "+    for i, videoUrl in enumerate(urls):\n",
            "         title = '%s [%s]' % (nickName or uniqueId, videoId)\n",
            "         if len(urls) > 1:\n",
            "             title = '%s [%s]' % (title, i)\n",
            " \n",
            "-        mime, ext, size = url_info(url)\n",
            "+        mime, ext, size = url_info(videoUrl, headers={'Referer': url})\n",
            " \n",
            "         print_info(site_info, title, mime, size)\n",
            "         if not info_only:\n",
            "-            download_urls([url], title, ext, size, output_dir=output_dir, merge=merge)\n",
            "+            download_urls([videoUrl], title, ext, size, output_dir=output_dir, merge=merge, headers={'Referer': url})\n",
            " \n",
            " site_info = \"TikTok.com\"\n",
            " download = tiktok_download\n",
            "\n",
            "\n",
            "def tiktok_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
            "uniqueId = videoData['authorInfos'].get('uniqueId')\n",
            "nickName = videoData['authorInfos'].get('nickName')\n",
            "<del>    for i, url in enumerate(urls):\n",
            "<add>    for i, videoUrl in enumerate(urls):\n",
            "title = '%s [%s]' % (nickName or uniqueId, videoId)\n",
            "if len(urls) > <number>:\n",
            "title = '%s [%s]' % (title, i)\n",
            "<del>        mime, ext, size = url_info(url)\n",
            "<add>        mime, ext, size = url_info(videoUrl, headers={'Referer': url})\n",
            "print_info(site_info, title, mime, size)\n",
            "if not info_only:\n",
            "<del>            download_urls([url], title, ext, size, output_dir=output_dir, merge=merge)\n",
            "<add>            download_urls([videoUrl], title, ext, size, output_dir=output_dir, merge=merge, headers={'Referer': url})\n",
            "site_info = \"TikTok.com\"\n",
            "download = tiktok_download\n",
            "\n",
            "['\\n', 'def', 'tiktok', 'download', 'url', 'output', 'dir', '=', 'merge', '=', 'true', 'info', 'only', '=', 'false', '**', 'kwargs', '\\n', 'unique', 'id', '= ', 'video', 'data', 'author', 'infos', 'get', 'unique', 'id', '\\n', 'nick', 'name', '= ', 'video', 'data', 'author', 'infos', 'get', 'nick', 'name', '\\n', '<del>', 'for', 'i', 'url', 'in', 'enumerate', 'urls', '\\n', '<add>', 'for', 'i', 'video', 'url', 'in', 'enumerate', 'urls', '\\n', 'title', '= ', '%', 's', '%', 's', '% ', 'nick', 'name', 'or', 'unique', 'id', 'video', 'id', '\\n', 'if', 'len', 'urls', '> <', 'number', '>', '\\n', 'title', '= ', '%', 's', '%', 's', '% ', 'title', 'i', '\\n', '<del>', 'mime', 'ext', 'size', '= ', 'url', 'info', 'url', '\\n', '<add>', 'mime', 'ext', 'size', '= ', 'url', 'info', 'video', 'url', 'headers', '=', 'referer', 'url', '\\n', 'print', 'info', 'site', 'info', 'title', 'mime', 'size', '\\n', 'if', 'not', 'info', 'only', '\\n', '<del>', 'download', 'urls', 'url', 'title', 'ext', 'size', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', '\\n', '<add>', 'download', 'urls', 'video', 'url', 'title', 'ext', 'size', 'output', 'dir', '=', 'output', 'dir', 'merge', '=', 'merge', 'headers', '=', 'referer', 'url', '\\n', 'site', 'info', '= ', 'tik', 'tok', 'com', '\\n', 'download', '= ', 'tiktok', 'download', '\\n']\n",
            "\n",
            "['fix', 'iqiyi', 'playlist', 'extrator']\n",
            "@@ -119,10 +119,10 @@ class Iqiyi(VideoExtractor):\n",
            "         self.url = url\n",
            " \n",
            "         video_page = get_content(url)\n",
            "-        videos = set(re.findall(r'<a href=\"(http://www\\.iqiyi\\.com/v_[^\"]+)\"', video_page))\n",
            "+        videos = set(re.findall(r'<a href=\"(?=https?:)?(//www\\.iqiyi\\.com/v_[^\"]+)\"', video_page))\n",
            " \n",
            "         for video in videos:\n",
            "-            self.__class__().download_by_url(video, **kwargs)\n",
            "+            self.__class__().download_by_url('https:' + video, **kwargs)\n",
            " \n",
            "     def prepare(self, **kwargs):\n",
            "         assert self.url or self.vid\n",
            "@@ -153,7 +153,7 @@ class Iqiyi(VideoExtractor):\n",
            "             except Exception as e:\n",
            "                 log.i(\"vd: {} is not handled\".format(stream['vd']))\n",
            "                 log.i(\"info is {}\".format(stream))\n",
            "-    \n",
            "+\n",
            " \n",
            "     def download(self, **kwargs):\n",
            "         \"\"\"Override the original one\n",
            "@@ -201,7 +201,7 @@ class Iqiyi(VideoExtractor):\n",
            "             if not urls:\n",
            "                 log.wtf('[Failed] Cannot extract video source.')\n",
            "             # For legacy main()\n",
            "-            \n",
            "+\n",
            "             #Here's the change!!\n",
            "             download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n",
            " \n",
            "@@ -215,7 +215,7 @@ class Iqiyi(VideoExtractor):\n",
            "                 with open(os.path.join(kwargs['output_dir'], filename),\n",
            "                           'w', encoding='utf-8') as x:\n",
            "                     x.write(srt)\n",
            "-                print('Done.')    \n",
            "+                print('Done.')\n",
            " \n",
            " '''\n",
            "         if info[\"code\"] != \"A000000\":\n",
            "\n",
            "\n",
            "class Iqiyi(VideoExtractor):\n",
            "self.url = url\n",
            "video_page = get_content(url)\n",
            "<del>        videos = set(re.findall(r'<a href=\"(http://www\\.iqiyi\\.com/v_[^\"]+)\"', video_page))\n",
            "<add>        videos = set(re.findall(r'<a href=\"(?=https?:)?(//www\\.iqiyi\\.com/v_[^\"]+)\"', video_page))\n",
            "for video in videos:\n",
            "<del>            self.__class__().download_by_url(video, **kwargs)\n",
            "<add>            self.__class__().download_by_url('https:' + video, **kwargs)\n",
            "def prepare(self, **kwargs):\n",
            "assert self.url or self.vid\n",
            "class Iqiyi(VideoExtractor):\n",
            "except Exception as e:\n",
            "log.i(\"vd: {} is not handled\".format(stream['vd']))\n",
            "log.i(\"info is {}\".format(stream))\n",
            "<del>    \n",
            "<add>\n",
            "def download(self, **kwargs):\n",
            "\"\"\"Override the original one\n",
            "class Iqiyi(VideoExtractor):\n",
            "if not urls:\n",
            "log.wtf('[Failed] Cannot extract video source.')\n",
            "# For legacy main()\n",
            "<del>            \n",
            "<add>\n",
            "#Here's the change!!\n",
            "download_url_ffmpeg(urls[<number>], self.title, 'mp<number>', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n",
            "class Iqiyi(VideoExtractor):\n",
            "with open(os.path.join(kwargs['output_dir'], filename),\n",
            "'w', encoding='utf-<number>') as x:\n",
            "x.write(srt)\n",
            "<del>                print('Done.')    \n",
            "<add>                print('Done.')\n",
            "'''\n",
            "if info[\"code\"] != \"A<number>\":\n",
            "\n",
            "['\\n', 'class', 'iqiyi', 'video', 'extractor', '\\n', 'self', 'url', '= ', 'url', '\\n', 'video', 'page', '= ', 'get', 'content', 'url', '\\n', '<del>', 'videos', '= ', 'set', 're', 'findall', 'r', '<', 'a', 'href', '=', 'http', '//', 'www', 'iqiyi', 'com', '/', 'v', '^', '+', 'video', 'page', '\\n', '<add>', 'videos', '= ', 'set', 're', 'findall', 'r', '<', 'a', 'href', '=', '?=', 'https', '?', '?', '//', 'www', 'iqiyi', 'com', '/', 'v', '^', '+', 'video', 'page', '\\n', 'for', 'video', 'in', 'videos', '\\n', '<del>', 'self', 'class', 'download', 'by', 'url', 'video', '**', 'kwargs', '\\n', '<add>', 'self', 'class', 'download', 'by', 'url', 'https', '+ ', 'video', '**', 'kwargs', '\\n', 'def', 'prepare', 'self', '**', 'kwargs', '\\n', 'assert', 'self', 'url', 'or', 'self', 'vid', '\\n', 'class', 'iqiyi', 'video', 'extractor', '\\n', 'except', 'exception', 'as', 'e', '\\n', 'log', 'i', 'vd', 'is', 'not', 'handled', 'format', 'stream', 'vd', '\\n', 'log', 'i', 'info', 'is', 'format', 'stream', '\\n', '<del>', '\\n', '<add>', '\\n', 'def', 'download', 'self', '**', 'kwargs', '\\n', 'override', 'the', 'original', 'one', '\\n', 'class', 'iqiyi', 'video', 'extractor', '\\n', 'if', 'not', 'urls', '\\n', 'log', 'wtf', 'failed', 'cannot', 'extract', 'video', 'source', '\\n', '# ', 'for', 'legacy', 'main', '\\n', '<del>', '\\n', '<add>', '\\n', '#', 'here', 's', 'the', 'change', '!!\\n', 'download', 'url', 'ffmpeg', 'urls', '<number>', 'self', 'title', 'mp', '<number>', 'output', 'dir', '=', 'kwargs', 'output', 'dir', 'merge', '=', 'kwargs', 'merge', 'stream', '=', 'false', '\\n', 'class', 'iqiyi', 'video', 'extractor', '\\n', 'with', 'open', 'os', 'path', 'join', 'kwargs', 'output', 'dir', 'filename', '\\n', 'w', 'encoding', '=', 'utf', '-<', 'number', '>', 'as', 'x', '\\n', 'x', 'write', 'srt', '\\n', '<del>', 'print', 'done', '\\n', '<add>', 'print', 'done', '\\n', '\\n', 'if', 'info', 'code', '!= ', 'a', '<number>', '\\n']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-20b8b38e30d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_repo_commits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"soimort/you-get\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-67bff691ab65>\u001b[0m in \u001b[0;36mparse_repo_commits\u001b[0;34m(repo_name, commit_limit)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_processing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodified_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-67bff691ab65>\u001b[0m in \u001b[0;36mdiff_processing\u001b[0;34m(mf)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mtoken_camel_case_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcase_splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;36m97\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m122\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_initial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_camel_case_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f65faf9"
      },
      "source": [
        "import multiprocessing\n",
        "\n",
        "data = []\n",
        "\n",
        "def f(repo):\n",
        "    df = parse_repo_commits(repo)\n",
        "    df.to_pickle(f\"./repos/{repo.replace('/', '+')}.pkl\")\n",
        "    print(repo, \"Done\")\n",
        "    return df\n",
        "\n",
        "pool = multiprocessing.Pool()\n",
        "outputs = pool.map(f, repodf['repo'][:3])\n",
        "pd.concat(outputs).to_pickle(\"data.pkl\")"
      ],
      "id": "0f65faf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04bfe004"
      },
      "source": [
        "df = pd.read_pickle(\"data.pkl\")\n",
        "df.head(3)"
      ],
      "id": "04bfe004",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbdfa1a5"
      },
      "source": [
        ""
      ],
      "id": "fbdfa1a5",
      "execution_count": null,
      "outputs": []
    }
  ]
}