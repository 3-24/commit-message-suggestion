{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Commit Generation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOsElGqU/4MkGqSEK49bJML"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "72e500d6c3f74667bff3fdb4753e7862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c34ab33d97c54d179d59d1cc1d91c35f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a3c1f06e1ca04a3986dc99b53a08ee68",
              "IPY_MODEL_d5580158df7d4e509f2dfa4e8a846f6d",
              "IPY_MODEL_a3346979b72a40bebaafb89afbcf78e0"
            ]
          }
        },
        "c34ab33d97c54d179d59d1cc1d91c35f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "a3c1f06e1ca04a3986dc99b53a08ee68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7d03eb96405d425b9237f676af21d35e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Validation sanity check:   0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34b22b47aa2f4b16ac527718a4ffe50f"
          }
        },
        "d5580158df7d4e509f2dfa4e8a846f6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_872f5bf91f4d49cd83c2fb53426918b4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d3cb00b57e64d1bb4f48d29f54377ef"
          }
        },
        "a3346979b72a40bebaafb89afbcf78e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3a5c02da5afa4eb28e29080c79ba83db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/2 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3900282cf05b48ecb244a776e34462b6"
          }
        },
        "7d03eb96405d425b9237f676af21d35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34b22b47aa2f4b16ac527718a4ffe50f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "872f5bf91f4d49cd83c2fb53426918b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d3cb00b57e64d1bb4f48d29f54377ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a5c02da5afa4eb28e29080c79ba83db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3900282cf05b48ecb244a776e34462b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNdEvLAmj3cy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b09fcc6-e306-44f6-aca1-783b6cfa851a"
      },
      "source": [
        "# mount drive https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab\n",
        "# login with your google account and type authorization code to mount on your googlbie drive.\n",
        "from google.colab import drive\n",
        "drive._mount('/gdrive')\n",
        "root = '/gdrive/My Drive/CS492I/project'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from easydict import EasyDict\n",
        "\n",
        "args = EasyDict()\n",
        "\n",
        "args.vocab_size = 50000\n",
        "args.embed_dim = 128\n",
        "args.hidden_dim = 256\n",
        "args.batch_size = 8\n",
        "args.trg_max_len = 50\n",
        "args.learning_rate = 0.15\n",
        "args.accum_init = 0.15\n",
        "args.pad_id = 0\n",
        "args.seed = 123\n",
        "args.epochs = 10\n",
        "args.max_grad_norm = 2.0"
      ],
      "metadata": {
        "id": "FRMsQgltslB5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Reference https://github.com/jiminsun/pointer-generator/blob/master/data/vocab.py\n",
        "'''\n",
        "pad_token = '<pad>'\n",
        "unk_token = '<unk>'\n",
        "start_decode = '<start>'\n",
        "stop_decode = '<stop>'\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "  def __init__(self):\n",
        "    self._word_to_id = {}\n",
        "    self._id_to_word = []\n",
        "    self._count = 0\n",
        "\n",
        "  @classmethod\n",
        "  def from_file(cls, filename):\n",
        "    vocab = cls()\n",
        "    with open(filename, 'r') as f:\n",
        "      vocab._word_to_id = json.load(f)\n",
        "    vocab._id_to_word = [w for w, id_ in sorted(vocab._word_to_id, key=vocab._word_to_id.get, reverse=True)]\n",
        "    vocab._count = len(vocab._id_to_word)\n",
        "    return vocab\n",
        "\n",
        "  @classmethod\n",
        "  def from_counter(cls, counter, vocab_size, min_freq=1, specials=[pad_token, unk_token, start_decode, stop_decode]):\n",
        "    vocab = cls()\n",
        "    word_and_freq = sorted(counter.items(), key=lambda tup: tup[0])\n",
        "    word_and_freq.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "    for w in specials:\n",
        "      vocab._word_to_id[w] = vocab._count\n",
        "      vocab._id_to_word.append(w)\n",
        "      vocab._count += 1\n",
        "\n",
        "    for word, freq in word_and_freq:\n",
        "      if freq < min_freq or vocab._count == vocab_size:\n",
        "        break\n",
        "      vocab._word_to_id[word] = vocab._count\n",
        "      vocab._id_to_word.append(word)\n",
        "      vocab._count += 1\n",
        "    \n",
        "    return vocab\n",
        "  \n",
        "  def save(self, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "      json.dump(self._word_to_id)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self._count\n",
        "  \n",
        "  def unk(self):\n",
        "    return self._word_to_id.get(unk_token)\n",
        "  \n",
        "  def pad(self):\n",
        "    return self._word_to_id.get(pad_token)\n",
        "\n",
        "  def word2id(self, word):\n",
        "    unk_id = self._word_to_id.get(word, self.unk())\n",
        "    if word in self._word_to_id:\n",
        "      return self._word_to_id[word]\n",
        "    else:\n",
        "      return unk_id\n",
        "  \n",
        "  def id2word(self, word_id):\n",
        "    if word_id >= self.__len__():\n",
        "      raise ValueError(f\"Id not found in vocab: {word_id}\")\n",
        "    return self_id_to_word[word_id]\n",
        "  \n",
        "  def extend(self, oovs):\n",
        "    return self._id_to_word + list(oovs)\n",
        "  \n",
        "  def tokens2ids(self, tokens):\n",
        "    return [self.word2id(t) for t in tokens]\n",
        "  \n",
        "  def tokens2ids_ext(self, tokens):\n",
        "    ids = []\n",
        "    oovs = []\n",
        "    unk_id = self.unk()\n",
        "    for t in tokens:\n",
        "      t_id = self.word2id(t)\n",
        "      if t_id == unk_id:\n",
        "        if t not in oovs:\n",
        "          oovs.append(t)\n",
        "        ids.append(len(self) + oovs.index(t))\n",
        "      else:\n",
        "        ids.append(t_id)\n",
        "    return ids, oovs"
      ],
      "metadata": {
        "id": "2GfT8GFi2oi7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.optim import Adagrad\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "\"\"\"\n",
        "B : batch size\n",
        "E : embedding size\n",
        "H : encoder hidden state dimension\n",
        "L : sequence length\n",
        "T : target sequence length\n",
        "\"\"\"\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim: source embedding dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.reduce_h = nn.Linear(hidden_dim * 2, hidden_dim, bias=True)\n",
        "        self.reduce_c = nn.Linear(hidden_dim * 2, hidden_dim, bias=True)\n",
        "    \n",
        "    def forward(self, src, src_lens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: source token embeddings    [B x L x E]\n",
        "            src_lens: source text length    [B]\n",
        "        Returns:\n",
        "            enc_hidden: sequence of encoder hidden states                  [B x L x 2H]\n",
        "            (final_h, final_c): Tuple for decoder state initialization     [B x L x H]\n",
        "        \"\"\"\n",
        "\n",
        "        x = pack_padded_sequence(src, src_lens, batch_first=True, enforce_sorted=False)\n",
        "        output, (h, c) = self.lstm(x) # [B x L x 2H], [2 x B x H], [2 x B x H]\n",
        "        enc_hidden, _ = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "        # Concatenate bidirectional lstm states\n",
        "        h = torch.cat((h[0], h[1]), dim=-1)  # [B x 2H]\n",
        "        c = torch.cat((c[0], c[1]), dim=-1)  # [B x 2H]\n",
        "\n",
        "        # Project to decoder hidden state size\n",
        "        final_hidden = torch.relu(self.reduce_h(h))  # [B x H]\n",
        "        final_cell = torch.relu(self.reduce_c(c))  # [B x H]\n",
        "\n",
        "        return enc_hidden, (final_hidden, final_cell)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.v = nn.Linear(hidden_dim * 2, 1, bias=False)                       # v\n",
        "        self.enc_proj = nn.Linear(hidden_dim * 2, hidden_dim * 2, bias=False)   # W_h\n",
        "        self.dec_proj = nn.Linear(hidden_dim, hidden_dim * 2, bias=True)        # W_s, b_attn\n",
        "  \n",
        "\n",
        "    def forward(self, dec_input, enc_hidden, enc_pad_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dec_input: decoder hidden state             [B x H]\n",
        "            enc_hidden: encoder hidden states           [B x L x 2H]\n",
        "            enc_pad_mask: encoder padding masks         [B x L]\n",
        "        Returns:\n",
        "            attn_dist: attention dist'n over src tokens [B x L]\n",
        "        \"\"\"\n",
        "        enc_feature = self.enc_proj(enc_hidden)               # [B X L X 2H]\n",
        "        dec_feature = self.dec_proj(dec_input).unsqueeze(1)   # [B X 1 X 2H]\n",
        "\n",
        "        scores = torch.v(torch.tanh(enc_feature + dec_feature)).squeeze(-1)  # [B X L]\n",
        "\n",
        "        if enc_pad_mask is not None:\n",
        "            scores = scores.float().masked_fill_(\n",
        "                enc_pad_mask,\n",
        "                float('-inf')\n",
        "            ).type_as(scores)  # FP16 support: cast to float and back\n",
        "        \n",
        "        attn_dist = F.softmax(scores, dim=-1) # [B X L]\n",
        "\n",
        "        return attn_dist\n",
        "\n",
        "\n",
        "class AttentionDecoderLayer(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, vocab_size):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTMCell(input_size=input_dim, hidden_size=hidden_dim)\n",
        "    self.attention = Attention(hidden_dim)\n",
        "    self.l1 = nn.Linear(hidden_dim*3, hidden_dim, bias=True)\n",
        "    self.l2 = nn.Linear(hidden_dim, vocab_size, bias=True)\n",
        "  \n",
        "  def forward(self, dec_input, dec_hidden, dec_cell, enc_hidden, enc_pad_mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        dec_input: decoder input embedding at timestep t    [B x E]\n",
        "        prev_h: decoder hidden state from prev timestep     [B x H]\n",
        "        prev_c: decoder cell state from prev timestep       [B x H]\n",
        "        enc_hidden: encoder hidden states                   [B x L x 2H]\n",
        "        enc_pad_mask: encoder masks for attn computation    [B x L]\n",
        "        coverage: coverage vector at timestep t - Eq. (10)  [B x L]\n",
        "    Returns:\n",
        "        vocab_dist: predicted vocab dist'n at timestep t    [B x V]\n",
        "        attn_dist: attention dist'n at timestep t           [B x L]\n",
        "        context_vec: context vector at timestep t           [B x 2H]\n",
        "        hidden: hidden state at timestep t                  [B x H]\n",
        "        cell: cell state at timestep t                      [B x H]\n",
        "    \"\"\"\n",
        "    hidden, cell = self.lstm(dec_input, (dec_hidden, dec_cell))  # [B X H], [B X H]\n",
        "\n",
        "    attn_dist = self.attention(dec_input, enc_hidden, enc_pad_mask).unsqueeze(1)  # [B X 1 X L]\n",
        "\n",
        "    context_vec = torch.bmm(attn_dist, enc_hidden).squeeze(1)  # [B X 2H] <- [B X 1 X 2H] = [B X 1 X L] @ [B X L X 2H]\n",
        "    output = self.l1(torch.cat([hidden, context_vec], dim = -1)) # [B X H]\n",
        "    vocab_dist = F.softmax(self.l2(output), dim=-1)              # [B X V]\n",
        "    return vocab_dist, attn_dist, context_vec, hidden, cell\n",
        "\n",
        "\n",
        "class PointerGenerator(nn.Module):\n",
        "  def __init__(self, src_vocab, trg_vocab):\n",
        "    super().__init__()\n",
        "    embed_dim = args.embed_dim\n",
        "    self.src_embedding = nn.Embedding(len(src_vocab), embed_dim, padding_idx=src_vocab.pad())\n",
        "    self.trg_embedding = nn.Embedding(len(trg_vocab), embed_dim, padding_idx=trg_vocab.pad())\n",
        "\n",
        "\n",
        "    hidden_dim = args.hidden_dim\n",
        "    self.encoder = Encoder(input_dim=embed_dim, hidden_dim=hidden_dim)\n",
        "    self.decoder = AttentionDecoderLayer(input_dim=embed_dim, hidden_dim=hidden_dim, vocab_size=len(trg_vocab))\n",
        "\n",
        "    self.w_h = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "    self.w_s = nn.Linear(hidden_dim, 1, bias=False)\n",
        "    self.w_x = nn.Linear(embed_dim, 1, bias=True)\n",
        "\n",
        "\n",
        "  def forward(self, enc_input, enc_input_ext, enc_pad_mask, enc_len, dec_input, max_oov_len):\n",
        "    \"\"\"\n",
        "    Predict summary using reference summary as decoder inputs (teacher forcing)\n",
        "    Args:\n",
        "        enc_input: source text id sequence                      [B x L]\n",
        "        enc_input_ext: source text id seq w/ extended vocab     [B x L]\n",
        "        enc_pad_mask: source text padding mask. [PAD] -> True   [B x L]\n",
        "        enc_len: source text length                             [B]\n",
        "        dec_input: target text id sequence                      [B x T]\n",
        "        max_oov_len: max number of oovs in src                  [1]\n",
        "    Returns:\n",
        "        final_dists: predicted dist'n using extended vocab      [B x V_x x T]\n",
        "        attn_dists: attn dist'n from each t                     [B x L x T]\n",
        "        coverages: coverage vectors from each t                 [B x L x T]\n",
        "    \"\"\"\n",
        "    enc_emb = self.src_embedding(enc_input)             # [B X L X E]\n",
        "    enc_hidden, (h,c) = self.encoder(enc_emb, enc_len)  # [B X L X 2H], [B X L X H], [B X L X H]\n",
        "    \n",
        "    dec_emb = self.trg_embedding(dec_input) # [B X T X E]\n",
        "\n",
        "    final_dists = []\n",
        "\n",
        "    for t in range(args.trg_max_len):\n",
        "        input_t = dec_emb[:, t, :]\n",
        "        vocab_dist, attn_dist, context_vec, h, c = self.decoder(\n",
        "            dec_input=input_t,\n",
        "            prev_h=h,\n",
        "            prev_c=c,\n",
        "            enc_hidden=enc_hidden,\n",
        "            enc_pad_mask=enc_pad_mask\n",
        "        )\n",
        "        \n",
        "        p_gen = torch.sigmoid(self.w_h(context_vec) + self.w_s(h) + self.w_x(input_t))\n",
        "        weighted_attn_dist = p_gen * vocab_dist + (1.0 - p_gen) * attn_dist\n",
        "        B = vocab_dist.size(0)\n",
        "        extended_vocab_dist = torch.cat([vocab_dist, torch.zeros(B, max_oov_len, device=vocab_dist.device)], dim=-1)\n",
        "\n",
        "        final_dist = extended_vocab_dist.scatter_add(dim=-1, index=enc_input_ext, src=weighted_attn_dist)\n",
        "        final_dists.append(final_dist)\n",
        "    return final_dists\n",
        "\n",
        "\n",
        "class SummarizationModel(pl.LightningModule):\n",
        "    def __init__(self, src_vocab, trg_vocab):\n",
        "        super().__init__()\n",
        "        self.vocab = trg_vocab\n",
        "        self.model = PointerGenerator(src_vocab, trg_vocab)\n",
        "        self.num_step = 0\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        output = self.model.forward(\n",
        "            enc_input=batch.enc_input,\n",
        "            enc_input_ext=batch.enc_input_ext,\n",
        "            enc_pad_mask=batch.enc_pad_mask,\n",
        "            enc_len=batch.enc_len,\n",
        "            dec_input=batch.dec_input,\n",
        "            max_oov_len=batch.max_oov_len)\n",
        "        \n",
        "        dec_target = batch.dec_target\n",
        "        loss = F.nll_loss(torch.log(output), dec_target, ignore_index=args.pad_id, reduction='mean')\n",
        "        self.logger.log_metrics({\"train_loss\": loss}, self.num_step)\n",
        "        self.num_step += 1\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        output = self.model.forward(\n",
        "            enc_input=batch.enc_input,\n",
        "            enc_input_ext=batch.enc_input_ext,\n",
        "            enc_pad_mask=batch.enc_pad_mask,\n",
        "            enc_len=batch.enc_len,\n",
        "            dec_input=batch.dec_input,\n",
        "            max_oov_len=batch.max_oov_len)\n",
        "        \n",
        "        dec_target = batch.dec_target\n",
        "        loss = F.nll_loss(\n",
        "            torch.log(output), dec_target, ignore_index=args.pad_id, reduction='mean')\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=False, prog_bar=False, logger=True)\n",
        "        self.logger.log_metrics({'val_loss': loss}, self.num_step)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        output = self.model.inference(\n",
        "            enc_input=batch.enc_input,\n",
        "            enc_input_ext=batch.enc_input_ext,\n",
        "            enc_pad_mask=batch.enc_pad_mask,\n",
        "            enc_len=batch.enc_len,\n",
        "            src_oovs=batch.src_oovs,\n",
        "            max_oov_len=batch.max_oov_len\n",
        "        )\n",
        "        result = {}\n",
        "        result['target'] = output\n",
        "        result['source'] = [' '.join(w) for w in batch.src_text]\n",
        "        result['gold_target'] = [' '.join(w) for w in batch.tgt_text]\n",
        "        return result\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return Adagrad(self.parameters(), lr=args.learning_rate, initial_accumulator_value=args.accum_init)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y6vs7LpAseSv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "class CommitDataset(Dataset):\n",
        "    def __init__(self, src_vocab: Vocab, trg_vocab: Vocab, file_path):\n",
        "        self.src_vocab = src_vocab\n",
        "        self.trg_vocab = trg_vocab\n",
        "        self.df = pd.read_pickle(file_path)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        item = EasyDict()\n",
        "        row = self.df.iloc[index]\n",
        "        src = json.loads(row[\"commit_messsage\"])\n",
        "        trg = json.loads(row[\"diff\"])\n",
        "        item.src_ids = self.src_vocab.tokens2ids(src)\n",
        "        item.src_ids_ext, oovs = self.src_vocab.tokens2ids_ext(src)\n",
        "        item.trg_ids = self.trg_vocab.tokens2ids(trg)\n",
        "        \n",
        "        return item\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "metadata": {
        "id": "4AZqWt1ksegd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def commit_collate_fn(batchdata):\n",
        "  print(batchdata)\n",
        "  print(len(batchdata))\n",
        "  assert(False)\n",
        "  '''\n",
        "  batch = EasyDict()\n",
        "  batch.enc_input\n",
        "  batch.enc_input_ext\n",
        "  batch.enc_pad_mask\n",
        "  batch.enc_len\n",
        "  batch.dec_input\n",
        "  batch.max_oov_len\n",
        "  '''\n"
      ],
      "metadata": {
        "id": "aya06JSFzNXh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train(root):\n",
        "    pl.seed_everything(args.seed)\n",
        "\n",
        "    src_counter = Counter()\n",
        "    trg_counter = Counter()\n",
        "    train_path = Path(root) / 'train.pkl'\n",
        "    validation_path = Path(root) / 'validation.pkl'\n",
        "    test_path = Path(root) / 'test.pkl'\n",
        "    train_df = pd.read_pickle(train_path)\n",
        "\n",
        "    for msg in train_df[\"diff\"]:\n",
        "        m = json.loads(msg)\n",
        "        src_counter.update(m)\n",
        "\n",
        "    for msg in train_df[\"commit_messsage\"]:\n",
        "        m = json.loads(msg)\n",
        "        trg_counter.update(m)\n",
        "\n",
        "    src_vocab = Vocab.from_counter(\n",
        "        counter=src_counter, \n",
        "        vocab_size=args.vocab_size\n",
        "    )\n",
        "\n",
        "    trg_vocab = Vocab.from_counter(\n",
        "        counter=trg_counter, \n",
        "        vocab_size=args.vocab_size\n",
        "    )\n",
        "\n",
        "    model = SummarizationModel(src_vocab, trg_vocab)\n",
        "\n",
        "    trainer = pl.Trainer(\n",
        "        gpus=torch.cuda.device_count(),\n",
        "        max_epochs=args.epochs,\n",
        "        gradient_clip_val=args.max_grad_norm\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        CommitDataset(src_vocab, trg_vocab, train_path),\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=commit_collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        CommitDataset(src_vocab, trg_vocab, validation_path),\n",
        "        batch_size=args.batch_size,\n",
        "        collate_fn=commit_collate_fn,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "train(root)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658,
          "referenced_widgets": [
            "72e500d6c3f74667bff3fdb4753e7862",
            "c34ab33d97c54d179d59d1cc1d91c35f",
            "a3c1f06e1ca04a3986dc99b53a08ee68",
            "d5580158df7d4e509f2dfa4e8a846f6d",
            "a3346979b72a40bebaafb89afbcf78e0",
            "7d03eb96405d425b9237f676af21d35e",
            "34b22b47aa2f4b16ac527718a4ffe50f",
            "872f5bf91f4d49cd83c2fb53426918b4",
            "6d3cb00b57e64d1bb4f48d29f54377ef",
            "3a5c02da5afa4eb28e29080c79ba83db",
            "3900282cf05b48ecb244a776e34462b6"
          ]
        },
        "id": "cpdfuuRBse0c",
        "outputId": "c56878d7-1059-4a01-aa64-b6ba01e5b8c0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 123\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type             | Params\n",
            "-------------------------------------------\n",
            "0 | model | PointerGenerator | 26.3 M\n",
            "-------------------------------------------\n",
            "26.3 M    Trainable params\n",
            "0         Non-trainable params\n",
            "26.3 M    Total params\n",
            "105.225   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72e500d6c3f74667bff3fdb4753e7862",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Validation sanity check: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'src_ids': [1162, 117, 145], 'src_ids_ext': [1162, 117, 145], 'trg_ids': [1, 1, 1, 23, 502, 36, 26, 1, 1, 1, 1, 1560, 412, 1, 1, 17, 307, 412, 992, 35, 468, 1, 1, 88, 468, 181, 1, 1, 1, 1, 26, 41, 10617, 1, 1, 1, 1, 10617, 1, 1, 163, 10617, 5605, 294, 1183, 5, 5839, 332, 3883, 3194, 1, 1, 228, 163, 10617, 412, 1, 1, 736, 10617, 1, 411, 20628, 1, 1, 736, 362, 10617, 22809, 20628, 5436, 539, 1, 1, 74, 10617, 477, 390, 539, 1, 3194, 1, 1, 455, 10617, 3194, 1, 1, 339, 2667, 1, 1, 757, 227, 1299, 1, 1, 2944, 6889, 13473, 9460, 1, 1, 5007, 535, 351, 2360, 1, 1, 3169, 723, 24, 1, 1, 1640, 5964, 7864, 1, 1, 1, 1, 213, 331, 2667, 38317, 1, 1, 1032, 660, 2667, 414, 1, 1, 1]}, {'src_ids': [16, 1, 40643, 55], 'src_ids_ext': [16, 50000, 40643, 55], 'trg_ids': [1, 1, 1560, 10, 1, 17, 1, 311, 2689, 27, 28, 131, 1, 1, 1560, 10, 1, 1, 1, 1, 7841, 10617, 1608, 508, 2191, 2667, 414, 1, 1, 11, 10617, 77, 32, 711, 1, 42069, 11, 1, 1, 2471, 10617, 1571, 382, 24, 11, 1, 1, 17, 7841, 1, 1, 17, 2471, 1, 1, 2689, 10617, 7841, 45590, 17420, 2667, 414, 1, 1, 311, 2689, 27, 28, 131, 1, 1, 1, 1, 1, 1560, 10, 121, 1, 7841, 10617, 1608, 508, 2191, 2667, 414, 1, 2471, 10617, 1571, 382, 24, 1, 1560, 10, 121, 1, 7841, 45590, 1, 1, 1560, 10, 121, 1, 1, 1, 7841, 10617, 1608, 508, 2191, 2667, 414, 1, 1, 2471, 10617, 1571, 382, 24, 1, 1, 17, 7841, 1, 1, 17, 2471, 1, 1, 17, 303, 1027, 76, 1, 1, 7841, 45590, 17420, 2667, 414, 1, 1, 1, 1, 1, 1560, 10, 1, 229, 1, 7841, 10617, 1608, 508, 2191, 2667, 414, 1, 11, 10617, 77, 32, 711, 1, 42069, 11, 1, 1560, 10, 1, 229, 1, 17, 2471, 1, 17, 303, 1027, 508, 229, 25, 1, 2689, 10617, 7841, 45590, 229, 2667, 4932, 1, 1, 1, 1, 1, 1, 1560, 10, 1, 229, 1, 1, 1, 7841, 10617, 1608, 508, 2191, 2667, 414, 1, 1, 11, 10617, 77, 32, 711, 1, 42069, 11, 1, 1, 2471, 10617, 1571, 382, 24, 11, 1, 1, 17, 7841, 1, 1, 17, 2471, 1, 1, 17, 303, 1027, 508, 229, 25, 1, 1, 2689, 10617, 7841, 45590, 229, 2667, 4932, 17420, 2667, 414, 1]}, {'src_ids': [80, 1273, 548, 42, 261, 1248], 'src_ids_ext': [80, 1273, 548, 42, 261, 1248], 'trg_ids': [1, 1, 1, 1389, 445, 1, 1, 888, 53, 73, 22, 1, 1, 23, 449, 36, 144, 1017, 1, 1, 1, 1, 23, 73, 424, 36, 173, 1, 1, 23, 73, 346, 36, 926, 363, 1, 1, 36, 73, 346, 363, 1306, 1, 1, 36, 73, 232, 1010, 1, 1, 1, 1, 1, 1, 63, 661, 926, 661, 1, 1, 1, 1, 446, 10617, 387, 1, 1, 1, 1, 404, 10617, 1, 1, 9837, 19, 249, 46, 41, 1, 1, 926, 6495, 164, 173, 290, 94, 114, 1, 1, 1, 1, 1, 1, 1120, 10617, 1, 1, 926, 124, 114, 1, 1, 41, 2667, 500, 1, 1, 199, 2667, 1, 1, 126, 363, 420, 101, 420, 594, 2667, 387, 1481, 103, 2667, 387, 1404, 2667, 414, 715, 41, 2667, 126, 1, 1, 3172, 112, 126, 363, 1976, 811, 101, 1, 1, 799, 363, 250, 136, 101, 42, 2667, 73, 232, 1010, 65, 1, 1, 3172, 249, 46, 363, 1302, 103, 22, 211, 2667, 73, 346, 363, 1306, 3141, 4, 2667, 9837, 249, 46, 1, 1, 1963, 363, 1302, 103, 22, 211, 2667, 73, 346, 363, 1306, 3141, 464, 41, 2667, 20231, 4, 2667, 173, 290, 94, 114, 1, 1, 1, 1, 1, 1, 926, 2529, 843, 2916, 1, 1, 41, 2667, 500, 1, 1, 843, 2916, 2667, 52, 1963, 3172, 249, 46, 3172, 112, 126, 1, 1, 1, 1, 1, 1]}, {'src_ids': [1265, 383, 145], 'src_ids_ext': [1265, 383, 145], 'trg_ids': [1, 1, 2314, 10617, 1061, 11148, 4557, 7835, 1, 668, 10617, 1061, 23627, 2314, 1, 163, 10617, 4087, 24, 3169, 86, 279, 1, 1, 36, 373, 1, 1, 1, 373, 4876, 118, 685, 10617, 414, 1, 373, 4876, 118, 2064, 376, 10617, 414, 1, 17073, 36, 373, 1, 17073, 1, 1, 10617, 139, 10617, 206, 373, 1, 1, 1, 1458, 229, 10617, 1, 1]}, {'src_ids': [571, 4681, 821, 737], 'src_ids_ext': [571, 4681, 821, 737], 'trg_ids': [1, 1, 1, 23, 3576, 62, 36, 2739, 350, 1, 1, 1, 1, 1, 1, 5787, 350, 1, 1, 1560, 4043, 90, 90, 1, 1, 133, 14, 1, 90, 1, 1, 1, 1, 2739, 1061, 1, 66, 9673, 90, 1, 1, 1, 1, 1, 1, 5787, 350, 1, 1, 1560, 124, 94, 1004, 640, 1, 1, 124, 14, 4043, 94, 1, 1, 1, 1, 4043, 90, 1061, 7, 94, 66, 66, 9673, 1004, 640, 1, 1, 1, 1, 1, 1, 5787, 350, 1, 1, 1560, 124, 7227, 41, 1, 1, 124, 14, 4043, 7227, 1, 1, 1, 1, 4043, 90, 1061, 7, 7227, 66, 9673, 41, 1, 1, 1, 1, 1, 1, 5787, 350, 1, 1, 1560, 52, 7227, 1057, 7227, 1004, 1057, 2667, 1, 1, 52, 1, 5, 14, 94, 22, 14, 837, 7227, 1, 1, 1, 1, 4043, 90, 1061, 52, 1057, 794, 66, 66, 66, 9673, 7227, 1004, 1057, 1]}, {'src_ids': [263, 507, 19, 77, 23, 25667, 1054], 'src_ids_ext': [263, 507, 19, 77, 23, 25667, 1054], 'trg_ids': [1, 1, 1560, 649, 120, 1, 5, 41, 2354, 8, 169, 533, 1, 39, 28, 27, 6657, 53, 6657, 41, 1, 183, 41, 10617, 2354, 1, 17073, 429, 147, 4761, 4, 66, 9673, 889, 1, 1, 429, 169, 649, 4, 66, 9673, 889, 1, 88, 183, 1, 1, 1, 63, 106, 287, 1159, 797, 1, 39, 183, 27, 714, 131, 177, 217, 4, 649, 1, 39, 183, 27, 131, 1, 17073, 429, 169, 13270, 23, 66, 9673, 120, 167, 781, 889, 1, 1, 429, 169, 649, 23, 66, 9673, 120, 167, 781, 889, 1, 183, 10617, 649, 120, 1, 795, 211, 9, 1915, 586, 342, 1, 369, 52, 207, 120, 183, 1]}, {'src_ids': [171, 589, 40, 16, 145], 'src_ids_ext': [171, 589, 40, 16, 145], 'trg_ids': [1, 1, 508, 52, 269, 269, 1, 1856, 9, 920, 4, 9, 508, 2900, 1, 508, 326, 508, 1, 1, 1, 1, 729, 9, 13578, 480, 5, 37, 374, 12, 86, 9, 24, 15118, 480, 1, 1, 1536, 35, 2892, 159, 477, 390, 539, 5369, 5369, 24, 13578, 1]}, {'src_ids': [263, 1457, 95, 4702], 'src_ids_ext': [263, 1457, 95, 4702], 'trg_ids': [1, 1, 63, 398, 112, 1, 369, 10672, 10617, 10672, 1, 369, 91, 10617, 1743, 31468, 369, 91, 1, 17073, 14, 713, 337, 222, 4, 2099, 39, 1433, 92, 379, 1287, 5, 9, 286, 136, 180, 92, 1, 17073, 379, 9453, 1, 1, 11178, 39, 1433, 92, 379, 1287, 5, 9, 286, 136, 180, 92, 379, 9453, 1, 369, 446, 219, 10617, 387, 1, 9, 1099, 61, 12, 1433, 23, 257, 4, 219, 103, 83, 2199, 1, 63, 398, 112, 1, 297, 92, 2055, 35, 69, 1126, 96, 436, 53, 103, 27, 875, 8, 14, 96, 1706, 531, 1, 369, 79, 438, 10617, 143, 1, 17073, 1, 17073, 369, 1962, 460, 297, 10617, 52, 1, 17073, 1, 1, 369, 1962, 460, 297, 10617, 52, 795, 7, 3346, 192, 1, 369, 354, 297, 10617, 52, 7454, 354, 279, 1, 17073, 1440, 79, 22, 8125, 1641, 82, 1, 17073, 39, 3400, 805, 8125, 1, 1, 39, 3400, 805, 8125, 1440, 79, 22, 8125, 1641, 82, 1, 369, 52, 78, 8125, 300, 1, 369, 219, 513, 297, 1]}]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c0f983021986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-c0f983021986>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         self._call_and_handle_interrupt(\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         )\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \"\"\"\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_bar_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0;31m# enable train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mdl_max_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataloader_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mdl_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# store batch level output per dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_run_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\u001b[0m in \u001b[0;36mon_run_start\u001b[0;34m(self, data_fetcher, dataloader_idx, dl_max_batches, num_dataloaders)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reload_dataloader_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_update_dataloader_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     def advance(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_update_dataloader_iter\u001b[0;34m(data_fetcher, batch_idx)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoaderIterDataFetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# restore iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mdataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mdataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\u001b[0m in \u001b[0;36mprefetching\u001b[0;34m(self, prefetch_batches)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefetch_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/fetching.py\u001b[0m in \u001b[0;36m_fetch_next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_fetch_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_profiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"fetch_next_{self.stage}_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetched\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_fetch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/auto_restart.py\u001b[0m in \u001b[0;36m_capture_metadata_collate\u001b[0;34m(samples, dataset, default_collate)\u001b[0m\n\u001b[1;32m    472\u001b[0m         }\n\u001b[1;32m    473\u001b[0m     \"\"\"\n\u001b[0;32m--> 474\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCaptureIterableDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCaptureMapDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-da13d245fc78>\u001b[0m in \u001b[0;36mcommit_collate_fn\u001b[0;34m(batchdata)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcommit_collate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   '''\n\u001b[1;32m      5\u001b[0m   \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEasyDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8Pxk0mkNse8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}