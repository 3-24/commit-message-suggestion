{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Commit Generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7eaAlyYbX+GRYYGN+mLe7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNdEvLAmj3cy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b37d0e-1971-4421-b3df-6951e2cb9002"
      },
      "source": [
        "# mount drive https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab\n",
        "# login with your google account and type authorization code to mount on your googlbie drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "root = '/gdrive/My Drive/CS492I/project'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whIO4LYymsTf"
      },
      "source": [
        "from easydict import EasyDict\n",
        "from torchtext.legacy.data import Field\n",
        "from torchtext.vocab import vocab\n",
        "import collections\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import torch"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed77rUFymouO"
      },
      "source": [
        "args = EasyDict()\n",
        "\n",
        "args.vocab_size = 50000\n",
        "args.embed_dim = 128\n",
        "args.hidden_dim = 256\n",
        "args.batch_size = 8"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKdZ2pbfzpc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de9393b-65db-4cae-df98-cc989f17ce31"
      },
      "source": [
        "src_counter = collections.Counter()\n",
        "trg_counter = collections.Counter()\n",
        "data_path = Path(root) / 'train.pkl'\n",
        "train_df = pd.read_pickle(data_path)\n",
        "\n",
        "for msg in train_df[\"commit_messsage\"]:\n",
        "  m = json.loads(msg)\n",
        "  trg_counter.update(m)\n",
        "\n",
        "for msg in train_df[\"diff\"]:\n",
        "  m = json.loads(msg)\n",
        "  src_counter.update(m)\n",
        "\n",
        "trg_vocab = vocab(trg_counter)\n",
        "print(len(trg_vocab))\n",
        "src_vocab = vocab(src_counter, min_freq=10)\n",
        "print(len(src_vocab))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46401\n",
            "56350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ5_vSk-skkL"
      },
      "source": [
        "'''\n",
        "Reference https://github.com/jiminsun/pointer-generator/blob/master/data/vocab.py\n",
        "'''\n",
        "pad_token = '<pad>'\n",
        "unk_token = '<unk>'\n",
        "start_decode = '<start>'\n",
        "stop_decode = '<stop>'\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "  def __init__(self):\n",
        "    self._word_to_id = {}\n",
        "    self._id_to_word = []\n",
        "    self._count = 0\n",
        "\n",
        "  @classmethod\n",
        "  def from_file(cls, filename):\n",
        "    vocab = cls()\n",
        "    with open(filename, 'r') as f:\n",
        "      vocab._word_to_id = json.load(f)\n",
        "    vocab._id_to_word = [w for w, id_ in sorted(vocab._word_to_id, key=vocab._word_to_id.get, reverse=True)]\n",
        "    vocab._count = len(vocab._id_to_word)\n",
        "    return vocab\n",
        "\n",
        "  @classmethod\n",
        "  def from_counter(cls, counter, vocab_size, specials, min_freq):\n",
        "    vocab = cls()\n",
        "    word_and_freq = sorted(counter.items(), key=lambda tup: tup[0])\n",
        "    word_and_freq.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "    for w in specials:\n",
        "      vocab._word_to_id[w] = vocab._count\n",
        "      vocab.append(w)\n",
        "      vocab._count += 1\n",
        "\n",
        "    for word, freq in word_and_freq:\n",
        "      if freq < min_freq or vocab._count == vocab.size:\n",
        "        break\n",
        "      vocab._word_to_id[word] = vocab._count\n",
        "      vocab._id_to_word.append(word)\n",
        "      vocab._count += 1\n",
        "    \n",
        "    return vocab\n",
        "  \n",
        "  def save(self, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "      json.dump(self._word_to_id)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self._count\n",
        "  \n",
        "  def unk(self):\n",
        "    return self._word_to_id.get(unk_token)\n",
        "\n",
        "  def word2id(self, word):\n",
        "    unk_id = self._word_to_id.get(word, self.unk())\n",
        "  \n",
        "  def id2word(self, word_id):\n",
        "    if word_id >= self.__len__():\n",
        "      raise ValueError(f\"Id not found in vocab: {word_id}\")\n",
        "  \n",
        "  def extend(self, oovs):\n",
        "    return self._id_to_word + list(oovs)\n",
        "  \n",
        "  def tokens2ids(self, tokens):\n",
        "    return [self.word2id(t) for t in tokens]\n",
        "  \n",
        "  def tokens2ids_ext(self, tokens):\n",
        "    ids = []\n",
        "    oovs = []\n",
        "    unk_id = self.unk()\n",
        "    for t in tokens:\n",
        "      t_id = self.word2id(t)\n",
        "      if t_id == unk_id:\n",
        "        if t not in oovs:\n",
        "          oovs.append(t)\n",
        "        ids.append(len(self) + oovs.index(t))\n",
        "    return ids, oovs"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmQRQ5Kwq0TJ"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Single-layer bidirectional LSTM\n",
        "    B : batch size\n",
        "    E : embedding size\n",
        "    H : encoder hidden state dimension\n",
        "    L : sequence length\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "      super().__init__()\n",
        "      self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "      self.reduce_h = nn.Linear(hidden_dim * 2, hidden_dim, bias=True)\n",
        "      self.reduce_c = nn.Linear(hidden_dim * 2, hidden_dim, bias=True)\n",
        "    \n",
        "    def forward(self, src, src_lens):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: source token embeddings    [B x L x E]\n",
        "            src_lens: source text length    [B]\n",
        "        Returns:\n",
        "            enc_hidden: sequence of encoder hidden states                  [B x L x 2H]\n",
        "            (final_h, final_c): Tuple for decoder state initialization     [B x L x H]\n",
        "        \"\"\"\n",
        "\n",
        "        x = pack_padded_sequence(src, src_lens, batch_first=True, enforce_sorted=False) # Packs a Tensor containing padded sequences of variable length\n",
        "        output, (h, c) = self.lstm(x) # [B x L x 2H], [2 x B x H], [2 x B x H]\n",
        "        enc_hidden, _ = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "        # Concatenate bidirectional lstm states\n",
        "        h = torch.cat((h[0], h[1]), dim=-1)  # [B x 2H]\n",
        "        c = torch.cat((c[0], c[1]), dim=-1)  # [B x 2H]\n",
        "\n",
        "        # Project to decoder hidden state size\n",
        "        final_hidden = torch.relu(self.reduce_h(h))  # [B x H]\n",
        "        final_cell = torch.relu(self.reduce_c(c))  # [B x H]\n",
        "\n",
        "        return enc_hidden, (final_hidden, final_cell)\n",
        "\n",
        "\n",
        "class Attnetion(nn.Module):\n",
        "  def __init__(self, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.v = nn.Linear(hidden_dim * 2, 1, bias=False)                       # v\n",
        "    self.enc_proj = nn.Linear(hidden_dim * 2, hidden_dim * 2, bias=False)   # W_h\n",
        "    self.dec_proj = nn.Linear(hidden_dim, hidden_dim * 2, bias=True)        # W_s, b_attn\n",
        "  \n",
        "\n",
        "  def forward(self, dec_input, enc_hidden, enc_pad_mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        dec_input: decoder hidden state             [B x H]\n",
        "        enc_hidden: encoder hidden states           [B x L x 2H]\n",
        "        enc_pad_mask: encoder padding masks         [B x L]\n",
        "    Returns:\n",
        "        attn_dist: attention dist'n over src tokens [B x L]\n",
        "    \"\"\"\n",
        "    enc_feature = self.enc_proj(enc_hidden)               # [B X L X 2H]\n",
        "    dec_feature = self.dec_proj(dec_input).unsqueeze(1)   # [B X 1 X 2H]\n",
        "\n",
        "    scores = torch.v(torch.tanh(enc_feature + dec_feature)).squeeze(-1)  # [B X L]\n",
        "\n",
        "    # Don't attend over padding; fill '-inf' where enc_pad_mask == True\n",
        "    if enc_pad_mask is not None:\n",
        "        scores = scores.float().masked_fill_(\n",
        "            enc_pad_mask,\n",
        "            float('-inf')\n",
        "        ).type_as(scores)  # FP16 support: cast to float and back\n",
        "    \n",
        "    attn_dist = F.softmax(scores, dim=-1) # [B X L]\n",
        "\n",
        "    return attn_dist\n",
        "\n",
        "\n",
        "class AttentionDecoderLayer(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, vocab_size):\n",
        "    super().__init__()\n",
        "    self.lstm = nn.LSTMCell(input_size=input_dim, hidden_size=hidden_dim)\n",
        "    self.attention = Attention(hidden_dim)\n",
        "    self.l1 = nn.Linear(hidden_dim*3, hidden_dim, bias=True)\n",
        "    self.l2 = nn.Linear(hidden_dim, vocab_size, bias=True)\n",
        "  \n",
        "  def forward(self, dec_input, dec_hidden, dec_cell, enc_hidden, enc_pad_mask):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        dec_input: decoder input embedding at timestep t    [B x E]\n",
        "        prev_h: decoder hidden state from prev timestep     [B x H]\n",
        "        prev_c: decoder cell state from prev timestep       [B x H]\n",
        "        enc_hidden: encoder hidden states                   [B x L x 2H]\n",
        "        enc_pad_mask: encoder masks for attn computation    [B x L]\n",
        "        coverage: coverage vector at timestep t - Eq. (10)  [B x L]\n",
        "    Returns:\n",
        "        vocab_dist: predicted vocab dist'n at timestep t    [B x V]\n",
        "        attn_dist: attention dist'n at timestep t           [B x L]\n",
        "        context_vec: context vector at timestep t           [B x 2H]\n",
        "        hidden: hidden state at timestep t                  [B x H]\n",
        "        cell: cell state at timestep t                      [B x H]\n",
        "    \"\"\"\n",
        "    hidden, cell = self.lstm(dec_input, (dec_hidden, dec_cell))  # [B X H], [B X H]\n",
        "\n",
        "    attn_dist = self.attention(dec_input, enc_hidden, enc_pad_mask).unsqueeze(1)  # [B X 1 X L]\n",
        "\n",
        "    context_vec = torch.bmm(attn_dist, enc_hidden).squeeze(1)  # [B X 2H] <- [B X 1 X 2H] = [B X 1 X L] @ [B X L X 2H]\n",
        "    output = self.l1(torch.cat([hidden, context_vec], dim = -1)) # [B X H]\n",
        "    vocab_dist = F.softmax(self.l2(output), dim=-1)              # [B X V]\n",
        "    return vocab_dist, attn_dist, context_vec, hidden, cell\n",
        "\n",
        "\n",
        "class PointerGenerator(nn.Module):\n",
        "  def __init__(self, src_vocab, trg_vocab):\n",
        "    super().__init__()\n",
        "\n",
        "    embed_dim = args.embed_dim\n",
        "    self.src_embedding = nn.Embedding(len(src_vocab), embed_dim, padding_idx=self.src_vocab.pad())\n",
        "    self.trg_embedding = nn.Embedding(len(trg_vocab), embed_dim, padding_idx=self.trg_vocab.pad())\n",
        "\n",
        "\n",
        "    hideen_dim = args.hidden_dim\n",
        "    self.encoder = Encoder(input_dim=embed_dim, hidden_dim=hidden_dim)\n",
        "    self.decoder = AttentionDecoderLayer(input_dim=embed_dim, hidden_dim=hidden_dim, vocab_size=len(trg_vocab))\n",
        "\n",
        "    self.w_h = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
        "    self.w_s = nn.Linear(hidden_dim, 1, bias=False)\n",
        "    self.w_x = nn.Linear(embed_dim, 1, bias=True)\n",
        "\n",
        "\n",
        "  def forward(self, enc_input, enc_input_ext, enc_pad_mask, enc_len, dec_input, max_oov_len):\n",
        "    enc_emb = self.src_embedding(enc_input)\n",
        "    enc_hidden, (h,c) = self.encoder(enc_emb, enc_len)\n",
        "    \n",
        "    final_dists = []\n",
        "    \n",
        "    dec_emb = self.dec_embedding(dec_input)\n",
        "\n",
        "    for t in range(self)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}